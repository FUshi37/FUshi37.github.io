<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>从零开始的科研生活004</title>
      <link href="/2024/10/20/research%20lifev4/"/>
      <url>/2024/10/20/research%20lifev4/</url>
      
        <content type="html"><![CDATA[<h2 id="day25-2024-10-20"><a href="#day25-2024-10-20" class="headerlink" title="day25 2024.10.20"></a>day25 2024.10.20</h2><h3 id="World-Model-based-Perception-for-Visual-Legged-Locomotion"><a href="#World-Model-based-Perception-for-Visual-Legged-Locomotion" class="headerlink" title="World Model-based Perception for Visual Legged Locomotion"></a>World Model-based Perception for Visual Legged Locomotion</h3><p>基于Dreamer框架构建的针对四足机器人的MBRL方法。方法实现了基于视觉的。</p><p>世界模型的更新频率小于policy决策频率，每k步更新。世界模型通过DreamerV3中RSSM的方法构建，构建思路与TWIST那篇十分相似（因为都是Dreamer）。</p><p><img src="/2024/10/20/research%20lifev4/1729428040891.png" alt="1729428040891"></p><p>循环模型使用GRU网络实现，解码器和编码器使用CNN网络处理深度图像实现，MLP处理本体感受观测。</p><p><img src="/2024/10/20/research%20lifev4/1729428986904.png" alt="1729428986904"></p><p>策略网络根据世界模型学习，经过良好训练的世界模型中的循环状态$h_t$包含了足够的信息用于未来预测，类似于底层的马尔可夫状态$s_t$。基于这一见解，训练了一个将$h_t$作为输入的策略。训练时采用actor-critic，critic可以使用特权信息训练。</p><p>奖励函数设置</p><p><img src="/2024/10/20/research%20lifev4/1729431716051.png" alt="1729431716051"></p><p>PPO + Isaac Gym 有类似于课程学习的过程</p><h2 id="day26-2024-10-21"><a href="#day26-2024-10-21" class="headerlink" title="day26 2024.10.21"></a>day26 2024.10.21</h2><h3 id="Temporal-Difference-Learning-or-Model-Predictive-Control-2022"><a href="#Temporal-Difference-Learning-or-Model-Predictive-Control-2022" class="headerlink" title="Temporal Difference Learning or Model Predictive Control 2022"></a>Temporal Difference Learning or Model Predictive Control 2022</h3><p>MBRL虽然比MFRL采样效率高，但是规划long horizon时需要的时间开销大，且很难获取一个准确的环境模型。TD-MPC，集合了MF和MB的优势，讲MPC和Model free的TD error更新结合，使用最终值函数来估计长期奖励。</p><p>具体而言，算法用MBRL学习用于局部轨迹优化的模型，用MFRL学习预测长期回报（用于全局优化）的价值函数（value function）。</p><p>在作者的MPC的pipeline中，通过输入t时刻的observation$s_t$，经过编码器$h_{\theta}$输出$t$时刻的隐向量$z_t$，将$z_t$和从正态分布采用的action$a_t$输入给环境模型$d_{theta}$，输出下一时刻的隐向量$z_{t+1}$并得到相应奖励$\hat{r}<em>t$。接着又从高斯分布中采样下一时刻的action$a</em>{t+1}$，接着如下图所示，直至预测到H时刻的隐向量$z_{H}$结束。</p><p><img src="/2024/10/20/research%20lifev4/98c2b2a59e54d0fd0eab307c6b7f5182.png" alt="img"></p><p>环境模型$d_{\theta}$的不精确可能会导致累计误差+MPC求得是t到H时刻的局部最优解。</p><p>TD-learning for MPC</p><p>TD-learning解决MPC的上述两个问题，TD-learning通过学习一个state-action value function价值函数$Q_{\theta}(s,a)$帮助MPC找到全局最优解。因为TD-learning是MF方法，所以它只学习跟奖励有关的信息，从而减少了MPC学习模型时的误差。</p><p>作者直接将价值函数加到MPC预测的一条轨迹所获得的回报中来学习价值函数。</p><p><img src="/2024/10/20/research%20lifev4/cfa29fdccc731714be0be6952c663ac5.png" alt="img"></p><p>标红的 Value 是状态-动作价值函数$Q_{\theta}(s,a)$所获得的回报，标红的 Rewards 是MPC规划一条轨迹所获得的奖励。也就是在MPC得到的rewards基础上加上$Q_{\theta}(s,a)$所获得的价值。</p><p><img src="/2024/10/20/research%20lifev4/1729576979789.png" alt="1729576979789"></p><p>Can we instead augment model-based planning with the strengths of model-free learning?</p><p>MPC算法用的是MPPI</p><p>TD-MPC维护了一个额外的policy来guide planning，如算法伪代码中蓝色图所示。当策略很差时，就会自然被排除在top-k之外，当粗恶略比较耗时，就可以很自然的按比例影响估计的回报。为了使采样随机化，对$\pi_{\theta}$的action像DDPG中一样应用线性退火的高斯噪声。</p><p><img src="/2024/10/20/research%20lifev4/1729591586033.png" alt="1729591586033"></p><p>TOLD与terminal value function通过TD-learning一起学习，TOLD仅对环境中预测奖励相关的元素进行建模，而不是建立一个完整的世界模型。在推理过程中，TD-MPC用TOLD进行轨迹优化，使用模型rollouts来估计短期奖励，使用terminal value function来估计长期奖励。支持连续动作空间、任意模态输入和系数奖励。</p><p>TOLD（Task-Oriented Latent Dynamics Model），能够支持图像或state的输入，主要在隐空间中模拟能够影响奖励的环境因素。主要 由五部分组成：</p><p><img src="/2024/10/20/research%20lifev4/1729577718396.png" alt="1729577718396"></p><p><img src="/2024/10/20/research%20lifev4/1729592401382.png" alt="1729592401382"></p><p>TOLD模型完全使用deterministic MLP实现，不需要RNN门控或者是概率模型。</p><p>相较于直接估计$Q$，文章选择学习一个策略$\pi_{\theta}$，该策略通过最小化目标函数来最大化$Q_{\theta}$。</p><p><img src="/2024/10/20/research%20lifev4/1729594442767.png" alt="1729594442767"></p><p>预期直接预测未来的状态或图片像素，文章选择预测隐向量，这也是一个很直观的思路，因为直接预测observation是十分困难的。我们建议用潜在状态一致性损失（如公式 10 所示）来正则化 TOLD，该损失迫使时间 $t + 1$ 的未来潜在状态预测 $z_{t+1} &#x3D; d_θ(z_t, a_t)$ 与对应ground-truth观察 $h_{θ^−}(s_{t+1})$ 的潜在表示相似，从而完全避免了对观察的预测。</p><p><img src="/2024/10/20/research%20lifev4/1729594884840.png" alt="1729594884840"></p><h2 id="day27-2024-10-22"><a href="#day27-2024-10-22" class="headerlink" title="day27 2024.10.22"></a>day27 2024.10.22</h2><h4 id="MPC-MPPI"><a href="#MPC-MPPI" class="headerlink" title="MPC -&gt; MPPI"></a>MPC -&gt; MPPI</h4><p>MPPI（Model Predictive Path Integral）</p><ol><li><p><strong>原理</strong>：MPPI是一种基于路径积分的控制方法，它利用随机采样的轨迹来优化控制输入。通过采样生成多条轨迹，MPPI评估这些轨迹的成本，并通过加权平均来得到控制输入。</p></li><li><p><strong>特点</strong>：</p><p>不依赖于精确的系统模型，而是可以使用系统的黑箱模型。</p><p>可以处理不确定性，通过采样和重采样策略获得更稳定的控制性能。</p><p>通常在高维和复杂的环境中表现良好。</p></li></ol><p>总结来说，MPC更强调通过模型进行优化，而MPPI则是利用随机采样的方法来获取控制策略。两者各有优劣，适用于不同的控制场景。</p><h3 id="RSSM"><a href="#RSSM" class="headerlink" title="RSSM"></a>RSSM</h3><p>RSSM（recurrent state-space model）是在<strong>PlaNet</strong>以及<strong>Dreamer</strong>系列的<strong>model-based强化学习</strong>中采用的，用来估计未知环境状态的模型。他的思想是将循环神经网络(下图(a))与状态空间模型(下图(b))联系在一起重构的模型(下图(c))。在model-based强化学习领域，根据PlaNet文章中所描述以及结合Dreamerv2的代码<a href="https://link.zhihu.com/?target=https://github.com/sai-prasanna/dreamerv2_torch">Dreamerv2的代码</a>，可以知利用循环神经网络输入输出的确定性关系以及状态空间模型输出的不确定性，可以利用由以往观测来推断当前的隐状态（prior），以及用当前的观测来推断当前隐状态（post）的两种方法来估计隐状态，然后用无监督的方法让两种估计的分布尽量接近，也就是用类似与两个分布的KL散度的方法来作为loss。</p><p><img src="/2024/10/20/research%20lifev4/1729606421659.png" alt="1729606421659"></p><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>给定一个mini-batch $X\in \mathbb{R}^{n\times d} $，其中n表示batch的大小，d表示特征维度。</p><p><img src="/2024/10/20/research%20lifev4/v2-2dc1444ca4c96ee722da410e79a4de5e_r.jpg" alt="img"></p><p>第一步引入一组参数$W_{xh}\in\mathbb{R}^{d\times h} $用于和输入$X$做线性运算，其中h表示隐藏层节点数。这样的运算用NN的形式表示如下：</p><p><img src="/2024/10/20/research%20lifev4/v2-d8c403de5dd478c25169d4b2246ed7eb_r.jpg" alt="img"></p><p>第二步如果是MLP计算到这就结束了，但RNN的特点就在于，引入上一步计算得到的隐状态$H_{t-1}\in \mathbb{R}^{n\times h} $，同时引入第二组参数$W_{hh}\in \mathbb{R}^{h\times h} $用于和$H_{t-1}$做线性运算，同样把该运算用NN的形式表示如下：</p><p><img src="/2024/10/20/research%20lifev4/v2-5c94ce95b431448824bab0fd93dd81b8_r.jpg" alt="img"></p><p>然后引入第三组参数$b_h\in \mathbb{R}^{1\times h}$，表示隐藏层的偏置，用于线性相加。</p><p>由此，综合上面的三步运算，线性求和通过激活函数$\phi$，得到时间步 t 时隐藏层的输出表达式</p><p><img src="/2024/10/20/research%20lifev4/1729608701925.png" alt="1729608701925"></p><p>$X_tW_{xh}$和$H_{t-1}W_{hh}$运算后都是$\mathbb{R}^{n\times h}$形状的tensor，与$b_h\in \mathbb{R}^{1\times h}$的形状不一致，不过可以通过pytorch的广播机制扩充运算。因此合并后的计算得到如下的计算图表示。</p><p><img src="/2024/10/20/research%20lifev4/v2-d36939e1077d151dbaca761a140f5b8a_r.jpg" alt="img"></p><p>得到$H_t$后，再计算最终输出$O_t$。对于时间步t，先引入第四组参数$W_{ho}\in \mathbb{R}^{h\times o}$，用于和$H_{t}$做线性计算，其中$o$表示输出的特征数。然后引入$b_o\in \mathbb{R}^{1\times o}$做线性求和，同用触发pytorch的广播机制。</p><p><img src="/2024/10/20/research%20lifev4/1729609915237.png" alt="1729609915237"></p><p><img src="/2024/10/20/research%20lifev4/v2-2c77077d50fd269769fd5d3979533334_r.jpg" alt="img"></p><p><img src="/2024/10/20/research%20lifev4/v2-5a11e1037302a681a7d80257c1943ef4_r.jpg" alt="img"></p><p><img src="/2024/10/20/research%20lifev4/v2-e7680d2524c9dabea5240005d40af430_r.jpg" alt="img"></p><h2 id="day28-2024-10-22"><a href="#day28-2024-10-22" class="headerlink" title="day28 2024.10.22"></a>day28 2024.10.22</h2><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>首先明确，GRU是RNN的一种，提出的Motivation是为了解决传统RNN中在处理实际问题时遇到的长期记忆丢失和反向传播中的梯度消失或爆炸等问题。</p><p>现在考虑一个问题：假设我们的文本序列非常长，有几十个甚至几百个单词，而且文本序列中存在非常远的依赖关系。在训练过程中，循环神经网络需要通过反向传播来更新权重，以最小化预测错误。然而，由于梯度消失的影响，网络在传播梯度时可能会出现问题。</p><p>举个例子，假设我们的模型在预测某个句子中的第一个单词时出现了错误。为了纠正这个错误，梯度将向网络的较早时间步传播，以更新与这个错误相关的权重。但是，由于梯度消失的问题，这些梯度可能会在网络中逐渐减小，导致较早时间步的权重几乎不会被更新，从而无法有效地学习到长期依赖关系。</p><p>另一方面，如果我们在训练过程中遇到了梯度爆炸的问题，梯度可能会变得非常大，导致权重的剧烈更新。这可能导致网络参数值的急剧变化，甚至可能导致数值溢出和数值不稳定性，使训练过程无法收敛。</p><p>为了解决这些问题，GRU模型通过引入门控机制，可以更好地控制信息的流动，并有效地缓解梯度消失和梯度爆炸问题。这使得网络能够更好地捕捉到长期依赖关系，提高模型的性能和泛化能力。</p><p>GRU（Gated Recurrent Unit）是一种循环神经网络（RNN）架构，专门用于处理序列数据。它在标准RNN的基础上引入了门控机制，帮助模型更有效地捕捉长程依赖关系。GRU有两个主要的门：</p><ol><li><strong>更新门</strong>（Update Gate）：控制当前状态的更新程度，它决定了多少先前的信息需要保留。</li><li><strong>重置门</strong>（Reset Gate）：控制将多少先前的状态丢弃，它影响如何结合过去的信息。</li></ol><p>通过这些门，GRU能够在处理序列数据时更好地管理信息流，减少梯度消失的问题，因此在许多任务（如自然语言处理、时间序列预测等）中表现良好。相比于LSTM，GRU的结构更简单，参数更少，通常训练更快。</p><p>对于给定的时间步$t$，假设输入一个mini-batch $X_{t}\in \mathbb{R}^{n\times d}$（其中$n$表示样本数，$d$表示输入数），前一个时间步的隐状态是$H_{t-1}\in \mathbb{R}^{n\times h}$（其中$h$表示隐藏单元数），则重置门$R_t\in \mathbb{R}^{n\times h}$和更新门$Z_t\in \mathbb{R}^{n\times h}$的计算如下所示：</p><p><img src="/2024/10/20/research%20lifev4/1729611492916.png" alt="1729611492916"></p><p>两者计算步骤相同。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>从零开始的科研生活003</title>
      <link href="/2024/10/12/research%20lifev3/"/>
      <url>/2024/10/12/research%20lifev3/</url>
      
        <content type="html"><![CDATA[<h2 id="day17"><a href="#day17" class="headerlink" title="day17"></a>day17</h2><p>回学校了，做了会ppt</p><h2 id="day18-2024-10-13"><a href="#day18-2024-10-13" class="headerlink" title="day18 2024.10.13"></a>day18 2024.10.13</h2><p>做ppt，RoboCup开会</p><h2 id="day19-2024-10-14"><a href="#day19-2024-10-14" class="headerlink" title="day19 2024.10.14"></a>day19 2024.10.14</h2><h3 id="Learning-quadrupedal-locomotion-over-challenging-terrainScience-Robotics-2020SR"><a href="#Learning-quadrupedal-locomotion-over-challenging-terrainScience-Robotics-2020SR" class="headerlink" title="Learning quadrupedal locomotion over challenging terrainScience Robotics 2020SR"></a>Learning quadrupedal locomotion over challenging terrainScience Robotics 2020SR</h3><p>对物理系统进行逼真建模，包括执行器。</p><p>域随机化</p><p>teacher-student，teacher根据特权信息编码成隐式向量进行训练</p><p>时间卷积网络TCN根据本体感觉状态的历史信息推断隐式向量</p><p>课程学习，地形难度随训练过程逐渐提升</p><p><img src="/2024/10/12/research%20lifev3/1728886299186.png" alt="1728886299186"></p><h3 id="Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR"><a href="#Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR" class="headerlink" title="Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR"></a>Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR</h3><p>关节分析模型建模+数据驱动学习执行器网络</p><p><img src="/2024/10/12/research%20lifev3/1728891298239.png" alt="1728891298239"></p><p>1.适用于机器人系统-&gt;建模困难、自由度高的方法   硬件系统特性   无人机sim-to-real</p><p>对象区别，构建仿真系统的方法</p><p>2.系统可以拿到，仿真系统；哪个可实现 哪个不可实现</p><p>3.核心关注算法点，方法；实现手段</p><p>4.仿真嵌入AIGC，生成。认知孪生系统，虚实交互，交互反馈提升仿真系统</p><p>5.六足区别于四足，被动式</p><p>细节、细节差别、细微差别</p><p>揣测工作的源头，溯源而上</p><p>虚实交互</p><p>单机智能直接感知</p><p>虚拟场景发现问题</p><h2 id="day20-2024-10-15"><a href="#day20-2024-10-15" class="headerlink" title="day20 2024.10.15"></a>day20 2024.10.15</h2><p>摸了</p><h2 id="day21-2024-10-16"><a href="#day21-2024-10-16" class="headerlink" title="day21 2024.10.16"></a>day21 2024.10.16</h2><h3 id="Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR-1"><a href="#Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR-1" class="headerlink" title="Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR"></a>Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR</h3><p><img src="/2024/10/12/research%20lifev3/1729064820128.png" alt="1729064820128"></p><p>求解器[41]</p><p>12个关节的执行器模型各自训练，输入关节角度差和关节角速度，端到端输出关节位置，然后通过一个PD控制器实现位置控制。</p><p>TRPO</p><p>关节执行器网络建模激活函数选的是softsign。</p><p>策略网络训练选的是tanh。</p><p>没有用力传感器作为observation的一部分，用joint state history代替。</p><p>课程学习、课程参数调节reward各项系数值。</p><p>训练过程中对observation的velocity做randomization。</p><p><img src="/2024/10/12/research%20lifev3/1729069222649.png" alt="1729069222649"></p><h2 id="day22-2024-10-17"><a href="#day22-2024-10-17" class="headerlink" title="day22 2024.10.17"></a>day22 2024.10.17</h2><p>Model-Based RL -&gt; Dreamer相关论文</p><h3 id="DayDreamer-World-Models-for-Physical-Robot-Learning"><a href="#DayDreamer-World-Models-for-Physical-Robot-Learning" class="headerlink" title="DayDreamer: World Models for Physical Robot Learning"></a>DayDreamer: World Models for Physical Robot Learning</h3><p>文章用Dreamer在物理机器人上在线学习，在实验中，仅用一小时四足机器人就能学会翻到转身和行走，而无需使用仿真。</p><p><img src="/2024/10/12/research%20lifev3/1729234733820.png" alt="1729234733820"></p><p>Dreamer是一个MBRL算法，学习世界模型（即dynamics model）。世界模型试通过RSSM（Recurrent State-space Model）实现的。</p><p>RSSM包含四个部分：<br><img src="/2024/10/12/research%20lifev3/1729235516657.png" alt="1729235516657"></p><p><img src="/2024/10/12/research%20lifev3/1729235125653.png" alt="1729235125653"></p><h2 id="day23-2024-10-18"><a href="#day23-2024-10-18" class="headerlink" title="day23 2024.10.18"></a>day23 2024.10.18</h2><h3 id="2022-A-Survey-on-Model-based-Reinforcement-Learning"><a href="#2022-A-Survey-on-Model-based-Reinforcement-Learning" class="headerlink" title="2022 A Survey on Model-based Reinforcement Learning"></a>2022 A Survey on Model-based Reinforcement Learning</h3><p>MBRL（Model-based Reinforcement Learning）能够显著提高采样效率，在有世界模型后，智能体的训练可能具有一定的想象力，即可以预测不同action产生的结果，因此在训练过程中可以选用恰当的action减小试错成本。</p><p>MBRL的主要任务是学习状态转移函数M（或者叫state transition dynamics）。理论上来说要实现好的训练效果，MFRL最好是直接与真实环境交互采样数据（经验数据）。</p><p>(a)为on-policy强化学习，(b)为off-policy强化学习，(c)为actor-critic，(d)为model-based方法，(e)为off-policy actor-critic方法。</p><p>与actor-critic相比，model-based方法重构了状态转移动力学，尽管critic计算设计状态转移动力学，但MBRL中的学习模型与策略解耦，因此该模型可用于评估其他策略，而critic是与actor绑定的。</p><p>off-policy、actor-critic和MBRL是并行的，可以组合使用（如图e）</p><p><img src="/2024/10/12/research%20lifev3/1729238916444.png" alt="1729238916444"></p><p>对于一个给定的MBRL问题，MDP中&lt;S,A,M,R,$\gamma$&gt;要学习的是状态转移动力学(state transition dynamics)M和奖励函数R。历史轨迹数据以state-action-reward序列出现呈现（如下图）。很显然序列包含了M和R的输入和输出数据。</p><p><img src="/2024/10/12/research%20lifev3/1729240130483.png" alt="1729240130483"></p><p><img src="/2024/10/12/research%20lifev3/1729240141468.png" alt="1729240141468"></p><h4 id="表格法（状态、动作空间小且有限）-R-MAX"><a href="#表格法（状态、动作空间小且有限）-R-MAX" class="headerlink" title="表格法（状态、动作空间小且有限）-&gt; R-MAX"></a>表格法（状态、动作空间小且有限）-&gt; R-MAX</h4><h4 id="Prediction-Loss"><a href="#Prediction-Loss" class="headerlink" title="Prediction Loss"></a>Prediction Loss</h4><p>通过神经网络实现近似函数代替表格。transition model命名为$M_{\theta}$，其中$\theta$是网络权重，真实transition model命名为$M^{*}$。</p><h5 id="单步转移模型（fit-one-step-transition）"><a href="#单步转移模型（fit-one-step-transition）" class="headerlink" title="单步转移模型（fit one-step transition）"></a>单步转移模型（fit one-step transition）</h5><p>当$M_{\theta}$是确定性(deterministic)的时，模型学习目标就是$M_{\theta}$对下一个状态的均方预测误差。为了应对不确定性，$M_{\theta}$可以利用概率转移模型建模（通常被实例化为高斯分布），此时模型的学习目标可以是$M_{\theta}$和$M^{*}$之间的KL散度最小化（有点像监督学习任务）。</p><p>价值评估误差：使用预测模型损失获得模型后，模型能够提供多少帮助？特别是策略$\pi$在模型和真实环境中的表现差异。</p><p>模拟引理（Simulation Lemma）表明与模型误差相比，奖励误差并不严重。</p><p><img src="/2024/10/12/research%20lifev3/1729252209575.png" alt="1729252209575"></p><p><img src="/2024/10/12/research%20lifev3/1729248793657.png" alt="1729248793657"></p><p>模拟引理2（Simulation Lemma II）表明策略价值评估误差是有界的。</p><p><img src="/2024/10/12/research%20lifev3/1729249030491.png" alt="1729249030491"></p><h5 id="多步预测"><a href="#多步预测" class="headerlink" title="多步预测"></a>多步预测</h5><p>由于累积误差是由于使用单步转移模型递归生成state-action而产生的，因此缓解该问题的一种方法是同时预测多个步骤。多步模型 [Asadi 等人，2019] 以当前状态 $s_t$ 和长度为 $h$ 的一系列动作 $(a_t, a_{t+1}, . . . , a_{t+h}) $作为输入，并预测未来的 $h$ 状态。（确定性）多步模型表示如下，同样可以通过监督学习训练</p><p><img src="/2024/10/12/research%20lifev3/1729249665915.png" alt="1729249665915"></p><p>以上方法得到的transtion models均为前向模型。MBRL的反向模型研究并不多，主要研究是以未来状态作为输入来预测当前状态，通常用于生成反向数据。</p><h4 id="Reduced-Error"><a href="#Reduced-Error" class="headerlink" title="Reduced Error"></a>Reduced Error</h4><p>上述基于predicition loss的方法的主要问题是horizon-squared compounding error，这主要是由于学习无约束模型导致的。</p><h5 id="Lipschitz-Continuity-Constraint"><a href="#Lipschitz-Continuity-Constraint" class="headerlink" title="Lipschitz Continuity Constraint"></a>Lipschitz Continuity Constraint</h5><p>为了减少累计误差，一种方法是约束模型。Lipschitz连续性约束。使用Wasserstein距离来衡量两个transition distributions间的相似性</p><p><img src="/2024/10/12/research%20lifev3/1729251606385.png" alt="1729251606385"></p><p>考虑状态空间S上的状态分布$\rho$而不是单个状态s时，可以定义广义转移模型（generalized transition model）如下</p><p><img src="/2024/10/12/research%20lifev3/1729251788059.png" alt="1729251788059"></p><p>n步误差</p><p>将Lipschitz连续性引入概率转移模型，一个概率转移模型M是K-Lipschitz的当且仅当</p><p><img src="/2024/10/12/research%20lifev3/1729252014058.png" alt="1729252014058"></p><p>在Lipschitz连续约束模型下，可以对n步误差进行界定。</p><p><img src="/2024/10/12/research%20lifev3/1729252185814.png" alt="1729252185814"></p><p><img src="/2024/10/12/research%20lifev3/1729252302302.png" alt="1729252302302"></p><p>the compounding error可以得到控制。</p><h5 id="Distribution-Matching"><a href="#Distribution-Matching" class="headerlink" title="Distribution Matching"></a>Distribution Matching</h5><p>学习transition的长期影响，考虑去匹配真实轨迹和学习模型展开的轨迹间的分布。在之前的一个工作中（GAIL）通过对抗学习和模仿学习，采用distribution matching的思想，以对抗的方式模仿专家策略。（最小化$\rho_{\pi_{E}}$(专家策略)和$\rho_{\pi}$）之间的JS散度。</p><p>在模仿学习角度，转移模型$M_{\theta}$以当前state-action输入并预测下一个状态的分布，被视为一种policy（对抗学习）。discriminator用于区分专家state和预测的state。（？）</p><p><img src="/2024/10/12/research%20lifev3/1729255229504.png" alt="1729255229504"></p><p>Simulation Lemma III解决了compounding error的问题。</p><h5 id="Robust-Model-Learning"><a href="#Robust-Model-Learning" class="headerlink" title="Robust Model Learning"></a>Robust Model Learning</h5><p>虽然compounding error减小了，但是策略差异仍然可能很大。策略差异（policy divergence）是指数据收集策略和目标策略之间的差异。为了减小差异，一个方向是使用具有广泛分布的数据收集策略。</p><h5 id="Complex-Environments-Dynamics"><a href="#Complex-Environments-Dynamics" class="headerlink" title="Complex Environments Dynamics"></a>Complex Environments Dynamics</h5><p>environment dynamics实现的主流思路为通过神经网络构建均值和协方差矩阵的高斯分布。该种架构在MuJuCo机器人运动环境中表现不错。</p><p>部分可观测性。对于部分可观测的环境所导致的POMDP，可能会有观测不足以作为推到的充分统计量。POMDP的一个经典解决方案是belief state estimation，观测模型$p(o_t|s_t)$和潜在转移模型latent transition model$p(s_{t+1}|s_t,a_t)$通过最大后验学习，并且可以推断后验分布$p(s_t|o_1,…,o_t)$。latent state distribution$p(s_t|o_1,…,o_t)$可以通过循环神经网络获得。</p><p>表示学习（Representation Learning）。对于诸如图像的高纬状态空间，学习信息丰富的latent state or action将极大地有利于环境模型的构建。</p><p><img src="/2024/10/12/research%20lifev3/1729262995387.png" alt="1729262995387"></p><p>DreamerV2用离散潜在变量替换了PlaNet中提出的高斯潜在变量，提高了性能。</p><p>state-action pair分布在模型训练和rollout阶段不匹配的问题，Shen等人将域自适应（domain adaptation）纳入模型学习任务，鼓励模型学习真实数据和rollout数据间state-action pair的不变表示。</p><h2 id="day24-2024-10-19"><a href="#day24-2024-10-19" class="headerlink" title="day24 2024.10.19"></a>day24 2024.10.19</h2><h3 id="TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer"><a href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer" class="headerlink" title="TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer"></a>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer</h3><p>训练teacher model的时候会生成一个经过DR的轨迹数据集D，以用来训练student model。</p><p><img src="/2024/10/12/research%20lifev3/1729350289161.png" alt="1729350289161"></p><p>所有上述modules的实现通过神经网络实现，网络参数为$\phi$，state通过deterministic componenet $h_t$和遵循categorical distribution的stochastic component联合表征。在每一步，RSSM用$h_t$计算两个stochastic state $z_t$和$\hat{z}$的分布。其中随机后验状态$z_t$是对当前输入观测$x_t$的编码，先验状态$\hat{z}$是对后验状态的预测，无需访问当前输入观测。因此，通过学习预测$z_t$，模型可以学习预测环境的动态。给定后验状态，训练解码器和奖励预测器分别重建当前输入观测值$x_t$和奖励$r_t$。</p><p><img src="/2024/10/12/research%20lifev3/1729350986669.png" alt="1729350986669"></p><p>一旦模型经过训练，就可以通过使用先验$\hat{z}$代替后验$z$，在不获取任何输入观测值的情况下进行推广。这使得模型能够生成形式为${(h_t，\hat{z_t}，a_t，r_t)_{t&#x3D;0}^{t&#x3D;T}}$的无限合成（或想象的）轨迹，其中t是想象的时间范围。</p><p>在知识蒸馏部分student model训练去学习对于teacher model一个从D中采样长度为L的轨迹trajectory $\tau$d的先验分布$p(\tilde{z}_t^{teacher}|\tilde{h}_t^{teacher})$，后验分布$q(\tilde{z}_t^{teacher}|\tilde{h}_t^{teacher},s_t)$和deterministic representations $h_t^{teacher}$</p><p><img src="/2024/10/12/research%20lifev3/1729350451865.png" alt="1729350451865"></p><p><img src="/2024/10/12/research%20lifev3/1729350542280.png" alt="1729350542280"></p><h3 id="PlaNet-2019ICML-Learning-Latent-Dynamics-for-Planning-from-Pixels"><a href="#PlaNet-2019ICML-Learning-Latent-Dynamics-for-Planning-from-Pixels" class="headerlink" title="PlaNet 2019ICML Learning Latent Dynamics for Planning from Pixels."></a>PlaNet 2019ICML Learning Latent Dynamics for Planning from Pixels.</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>从零开始的科研生活002</title>
      <link href="/2024/10/04/research%20lifev2/"/>
      <url>/2024/10/04/research%20lifev2/</url>
      
        <content type="html"><![CDATA[<h2 id="day8"><a href="#day8" class="headerlink" title="day8"></a>day8</h2><p>回家了</p><h2 id="day9"><a href="#day9" class="headerlink" title="day9"></a>day9</h2><p>看了一点ANYMAL的文章</p><h2 id="day10"><a href="#day10" class="headerlink" title="day10"></a>day10</h2><h3 id="ANYmal-Parkour-Learning-Agile-Navigation-for-Quadrupedal-Robots"><a href="#ANYmal-Parkour-Learning-Agile-Navigation-for-Quadrupedal-Robots" class="headerlink" title="ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots"></a>ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots</h3><p>locomotion policy是固定情景下的运动策略，在固定障碍物环境下训练。perception module和navigaion module在不同障碍物布置的情境下训练。</p><p>训练算法用的是PPO，不过对PPO做了一定的改进。hydird actor输出，low-level command用高斯分布，skill selection用分类分布。训练在一个hierarchical set-up中进行的。</p><p><img src="/2024/10/04/research%20lifev2/1728098611335.png" alt="1728098611335"></p><p>文章并没有具体提及sim-to-real的办法，不过确实有对障碍物(场景)的随机化。</p><p>这篇文章值得多看几遍。</p><h3 id="Sim-to-Real-Learnging-Agile-Locomotion-For-Quadruped-Robots"><a href="#Sim-to-Real-Learnging-Agile-Locomotion-For-Quadruped-Robots" class="headerlink" title="Sim-to-Real: Learnging Agile Locomotion For Quadruped Robots"></a>Sim-to-Real: Learnging Agile Locomotion For Quadruped Robots</h3><p>仿真用的是PyBullet，观测空间舍弃了观测值容易漂和变化剧烈的观测值，如关节速度项，这是因为越紧致的观测空间越容易减小gap。</p><p>减小sim-to-real gap的方法主要有两个，第一个是提高仿真的保真度。该方法首先建立了一个精确的URDF模型(对于难以获得精确值的机器人，通过SysID也可以实现这部分工作)。此外还建立了一个更精准的执行器(关节)模型(PD控制增益项不能太大，否在在reality中容易振荡)根据理想直流电机动力学模型为基础对执行器进行建模，同时修改了线性转矩-电流的关系。此外是延迟的解决。对延迟进行建模，对最相邻两个time step的observation进行线性插值(对时间插值)，同时测量实际电机执行的延迟并将其建模到仿真中。</p><p>第二个是学习鲁棒的控制器。有三个方向，第一个是domain randomization；第二个是训练时增加随机扰动，具体为在机器人基体上施加随即方向、随机大小的力；第三个是使用紧致的观测空间，因为高纬度的观测空间可能导致机器人对训练场景(仿真)过拟合。</p><p><img src="/2024/10/04/research%20lifev2/1728120665560.png" alt="1728120665560"></p><h2 id="day11"><a href="#day11" class="headerlink" title="day11"></a>day11</h2><h3 id="RMA-Rapid-Motor-Adaptation-for-Legged-Robots"><a href="#RMA-Rapid-Motor-Adaptation-for-Legged-Robots" class="headerlink" title="RMA: Rapid Motor Adaptation for Legged Robots"></a>RMA: Rapid Motor Adaptation for Legged Robots</h3><p>Adaptation Module通过状态历史state和action来估计隐式向量z，z是privileged knowledge编码后的结果，在reality中因为无法获取privileged knowledge，z通过机器人的历史状态和action来获得。Base Policy通过当前状态state、上一步action和隐式向量z来输出机器狗的action。</p><p>仿真中可以直接拿到priviledged knowledge，所以Adapation Module的训练可以采用监督学习。训练过程中on-policy，即同步和base policy训练。</p><p><img src="/2024/10/04/research%20lifev2/1728192253044.png" alt="1728192253044"></p><h3 id="Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation"><a href="#Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation" class="headerlink" title="Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation"></a>Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation</h3><p>首先在一个相对rough的仿真中训练一个模型，然后在实物上部署并收集数据，最后根据收集到的数据去仿真中更新模型和方法。</p><p>对于较软的物体，仿真建模通常不够精准，Sim2Real2Sim方法就是为了解决这个问题而提出的。</p><p>仿真用的是Gazebo</p><p><img src="/2024/10/04/research%20lifev2/1728216401553.png" alt="1728216401553"></p><h3 id="Real2sim2real-Self-supervised-learning-of-physical-single-step-dynamic-actions-for-planar-robot-casting"><a href="#Real2sim2real-Self-supervised-learning-of-physical-single-step-dynamic-actions-for-planar-robot-casting" class="headerlink" title="Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting"></a>Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting</h3><p>Parameter estimation using Differential Evolution [53,55] with associated code and datasets from 64,350 simulated experiments and 2,076 physical experiments with 3 distinct cables.</p><p><img src="/2024/10/04/research%20lifev2/1728223180124.png" alt="1728223180124"></p><p>首先从reality中收集physical dataset，并选取子集来进行仿真的SysID。</p><p>然后文章从PyBullet、两个版本的NVIDIA Isaac Gym三个仿真做了对比。</p><h2 id="day12"><a href="#day12" class="headerlink" title="day12"></a>day12</h2><h3 id="Robot-Parkour-Learning"><a href="#Robot-Parkour-Learning" class="headerlink" title="Robot Parkour Learning"></a>Robot Parkour Learning</h3><p>文章提出了一个端到端的机器狗跑酷策略网络，每个具体技能的训练分为两阶段。第一阶段为soft dynamics预训练，障碍物可穿越，训练过程采用了课程学习，障碍物难度逐渐增大。同时该阶段的训练用到了privilege visual information。</p><p>第二阶段为fine-tune阶段，同样使用PPO，在预训练后在Hard Dynamics Constraints上再对每个运动技能进行训练。该阶段的训练能够实现sim2real的转换。五个skill的训练过程用到了privilege visual information</p><p>训练完五个技能的policy网络后，通过一个DAgger蒸馏一个vision-based的parkour策略网络。策略参数化为GRU，输入包括recurrent latent state、本体感知、上一步的aciton和经过CNN处理得到的深度图的latent embedding。</p><p>经过蒸馏后，策略具有了一定sim-to-real transfer的能力(terrain friction and mass properties)，不过还需要对visual appearence进行sim-to-real transfer。</p><p><img src="/2024/10/04/research%20lifev2/1728292261234.png" alt="1728292261234"></p><p>毕设申题需要翻译一篇文献，翻译了一下RMA那篇文章。</p><h2 id="day13"><a href="#day13" class="headerlink" title="day13"></a>day13</h2><h3 id="Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real"><a href="#Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real" class="headerlink" title="Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real"></a>Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real</h3><p>文章研究的是双足机器人，文章的主要思想就是蒸馏。策略蒸馏部分可以在看一下。</p><p><img src="/2024/10/04/research%20lifev2/1728378609266.png" alt="1728378609266"></p><p>系统辨识提升建模精度，文章发现膝关节的reflected inertia of motors十分重要，仿真器用的是Mujoco。文章发现没有使用dynamics rondamization策略网络也可以实现sim-to-real transfer。</p><p>文章中提到没有使用data-driven的方法来对执行机构进行建模。</p><p>文章中使用了状态估计的方法，状态估计用来模拟传感器。</p><h2 id="day14"><a href="#day14" class="headerlink" title="day14"></a>day14</h2><h3 id="Sim-to-Real-Transfer-in-Deep-Reinforcement-Learning-for-Robotics-a-Survey"><a href="#Sim-to-Real-Transfer-in-Deep-Reinforcement-Learning-for-Robotics-a-Survey" class="headerlink" title="Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey"></a>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey</h3><p>主要的sim-to-real方法一览</p><p><img src="/2024/10/04/research%20lifev2/1728453552003.png" alt="1728453552003"></p><ol><li>系统辨识：建立物理系统的精确模型，使得仿真更加精准</li><li>域随机化：与其精确建立现实世界的所有参数，不如对仿真进行高度随机化，以覆盖现实世界数据的真是分布</li><li>域自适应：利用源数据来提高学习模型在数据量较少的目标域上的性能。在源域和目标域不同的特征空间寻找统一的特征集。</li><li>带干扰的学习：奖励函数加入随机噪声。</li><li>仿真环境：Gazebo PyBullet MuJoCo IsaacGym</li><li>元强化学习</li><li>模仿学习</li><li>知识蒸馏</li></ol><p><img src="/2024/10/04/research%20lifev2/1728557708842.png" alt="1728557708842"></p><h2 id="day15"><a href="#day15" class="headerlink" title="day15"></a>day15</h2><h3 id="Sim-to-real-Six-legged-robot-control-with-deep-reinforcement-learning-and-curriculum-learning"><a href="#Sim-to-real-Six-legged-robot-control-with-deep-reinforcement-learning-and-curriculum-learning" class="headerlink" title="Sim-to-real: Six-legged robot control  with deep reinforcement learning and curriculum learning."></a>Sim-to-real: Six-legged robot control  with deep reinforcement learning and curriculum learning.</h3><p>文章的主体思路和谢广明的TRO差不多，粗仿真+精确仿真+课程学习</p><p>仿真用的是V-rep，sim-to-real主要是靠精准的逆运动学实现的，所以这并不是一个端到端的方法。</p><p><img src="/2024/10/04/research%20lifev2/1728742522363.png" alt="1728742522363"></p><h2 id="day16"><a href="#day16" class="headerlink" title="day16"></a>day16</h2><h3 id="Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer-2019ICML"><a href="#Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer-2019ICML" class="headerlink" title="Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer 2019ICML"></a>Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer 2019ICML</h3><p>SRL CL RL Distillation</p><p><img src="/2024/10/04/research%20lifev2/1728636890456.png" alt="1728636890456"></p><p><img src="/2024/10/04/research%20lifev2/1728637438867.png" alt="1728637438867"></p><h3 id="TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer-2024ICRA"><a href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer-2024ICRA" class="headerlink" title="TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer 2024ICRA"></a>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer 2024ICRA</h3><p>该篇文章用知识蒸馏的方法去训练了类似于状态估计器的模型，用privileged states去训练teacher world model，再去蒸馏训练student world model。（world model即dynamics model）</p><p>该文章针对vision-based的方法，在训练teacher world model的时候对输入图片进行了domain randomization。student world model同理。</p><p>teacher world model的训练是用的Dreamer算法，一种model-based强化学习算法。</p><p><img src="/2024/10/04/research%20lifev2/1728623533715.png" alt="1728623533715"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>从零开始的科研生活001</title>
      <link href="/2024/09/26/research%20life/"/>
      <url>/2024/09/26/research%20life/</url>
      
        <content type="html"><![CDATA[<p>这是一个从零开始记录的研究生科研生活，不过是从研0开始的(悲)</p><p>昨天老板找我聊毕设的事，忽觉自己在复试完后已经放飞自我。老板给了两个方向，仿生(六足)和双臂机械臂操作。本人对六足更感兴趣但觉得没什么前途，做机械臂操作又感觉没什么意思，遂仍在犹豫中。</p><p>同时老板还给了做一个sim2real调研的任务，ddl定在了10月7号，这真不是让我帮他写毕设申题吗(bushi)。 </p><p>总之研0生活现在就算是开始了，昨晚找了许多篇文章准备开啃。</p><h2 id="day1"><a href="#day1" class="headerlink" title="day1"></a>day1</h2><h3 id="From-Simulation-to-Reality-A-Learning-Framework"><a href="#From-Simulation-to-Reality-A-Learning-Framework" class="headerlink" title="From Simulation to Reality A Learning Framework"></a>From Simulation to Reality A Learning Framework</h3><p>今天首先仔细过了一遍谢广明在2022年发在TRO上的《From Simulation to Reality A Learning Framework》，文章针对仿生鱼提出了Tri-S仿真+A2C强化学习训练的方法，减小了reality gap，实现了sim-to-real跨越。</p><p>文章提出了Tri-S System(Self-Switching-Simulator)是一个结合Surrogate env和CFD env的两阶段方法，Surrogate是data driven的环境仿真方法，在实验中通过给定输入，根据观测到的机器鱼的位置和姿态来做从输入到机器鱼状态s的映射fs。CFD env是基于数值计算的环境仿真方法，其计精度更高但同时计算效率也更低。</p><p><img src="/2024/09/26/research%20life/1727346561621.png" alt="1727346561621"></p><p>训练主体也可以看作两阶段训练，即Surrogate-&gt;CFD，Surrogate用于快速训练，为后续在CFD上的训练提供一个基本成型的策略网络结果。然后通过CFD高精度的仿真性能，再进行训练将显著减少reality gap，实现sim-to-real。</p><p><img src="/2024/09/26/research%20life/1727346792628.png" alt="1727346792628"></p><p>文章在此基础上实现了两个目标任务的训练，第一个是目标轨迹的跟随，第二个是到固定位姿。第一个的奖励函数设置是紧密的，训练过程并不难。但第二个任务由于任务的多目标性(要求距离和角度差同时为0)和机器鱼的非完整约束性质(nonholonomic constraints)，距离差和角度差不能同时保证减小，因此设置紧密的奖励函数是比较复杂的，为此文章采取了稀疏的奖励函数设置，同时为了保证训练过程能够收敛，采用了三阶段课程学习(curriculum learning)的方法。</p><p>该文章解决sim-to-real问题的主体思路是采用更精准的环境仿真器，同时采用一个data-driven的方法进行训练加速。</p><h3 id="TRANSIC-Sim-to-Real-Policy-Transfer-by-Learning-from-Online-Correction"><a href="#TRANSIC-Sim-to-Real-Policy-Transfer-by-Learning-from-Online-Correction" class="headerlink" title="TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction"></a>TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</h3><p>另一篇文章是李飞飞在今年发在CoRL的《TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction》，该文章提出了一个基于干预的学习方案来消除机械臂操作的sim-to-real问题。具体来说TRANSIC首先在仿真中采用teacher-student的方法，通过PPO训练teacher policy，然后通过GMMs(Gaussian Mixture Models)方法来训练student网络。</p><p>teacher可以利用特权信息，而student只能用点云输出。</p><p>随后，这些策略被部署到真实机器人上，由人类操作员监控执行情况。当机器人遇到错误或困难时，操作员会进行必要的干预，通过遥操作提供在线纠正。这些干预和纠正的数据被收集起来，用于训练一个残差策略，该策略能够学习预测并纠正机器人的状态差异。最终，基础策略和残差策略在测试时通过一个集成部署框架结合起来，通过门控机制决定何时应用残差策略，以实现更自然、更流畅的动作执行。TRANSIC方法的一个显著优势是它能够显著提高sim-to-real转移的性能，同时相比于其他仅依赖真实机器人轨迹的方法，它需要的真实世界数据更少。</p><p><img src="/2024/09/26/research%20life/1727351017885.png" alt="1727351017885"></p><p><img src="/2024/09/26/research%20life/1727353328109.png" alt="1727353328109"></p><p>李飞飞在这篇文章中对sim-to-real出现的原因，以及各种解决方案做了比较全面地介绍，对了解sim-to-real地相关问题大有裨益。</p><h4 id="sim-to-real已有的成熟解决方案的领域"><a href="#sim-to-real已有的成熟解决方案的领域" class="headerlink" title="sim-to-real已有的成熟解决方案的领域"></a>sim-to-real已有的成熟解决方案的领域</h4><p>a) locomotion</p><p>b) dexterous in-hand manipulation</p><p>c) simple non-prehensile manipulation</p><h4 id="sim-to-real-gaps"><a href="#sim-to-real-gaps" class="headerlink" title="sim-to-real gaps:"></a>sim-to-real gaps:</h4><p>a) perception gap：传感器观测差异</p><p>b) embodiment mismatch：机器人仿真模型与现实模型不匹配</p><p>c) controller inaccuracy：控制误差</p><p>d) poor physical realism：仿真精度问题，如对接触和碰撞不能做出精准的仿真计算</p><h4 id="traditional-methods"><a href="#traditional-methods" class="headerlink" title="traditional methods:"></a>traditional methods:</h4><p>a) system identification</p><p>b) domain randomization</p><p>c) real-world adaptation</p><p>d) simulator augmentation</p><h3 id="TODO-LIST"><a href="#TODO-LIST" class="headerlink" title="TODO LIST"></a>TODO LIST</h3><ol><li><p>TRANSIC中提到的state-of-the-art simulation</p></li><li><p>sim-to-real已有解决方案领域的文章，尤其是locomotion</p></li><li><p>sim-to-real gap相关文章</p></li><li><p>sim-to-real gap解决方案相关文章</p></li><li><p>残差网络、模仿学习、off line RL</p></li></ol><h2 id="day2"><a href="#day2" class="headerlink" title="day2"></a>day2</h2><h3 id="state-of-the-art-simulation"><a href="#state-of-the-art-simulation" class="headerlink" title="state-of-the-art simulation"></a>state-of-the-art simulation</h3><p>Mujoco：物理引擎，精度计算高于Pybullet，适合机械臂控制任务。DeepMind基于MuJoCo开发了强化学习环境<a href="https://link.zhihu.com/?target=https://github.com/deepmind/dm_control">Control Suite</a> 。OpenAI维护了一个mujoco-py版本专门给gym用。精度最高</p><p>Pybullet：基于Bullet Physics物理引擎，Bullet Physics使用离散元素方法(DEM)的算法来处理碰撞和接触问题。该方法将物体划分为小的离散单元，计算单元间的相互作用。</p><p>robosuite：基于Mujoco，专门用于manipulation。</p><p>Isaac Sim&#x2F;Lab：集成物理引擎PhysX，GPU加速训练。速度最快</p><p>Gazebo：与ROS集成，有多种物理引擎，仿真速度较慢。</p><p>Webots：速度慢、物理引擎精度低。</p><p>虚幻引擎：高保真度。</p><h3 id="domain-adaption"><a href="#domain-adaption" class="headerlink" title="domain adaption"></a>domain adaption</h3><p>domain adaption：source domain -&gt; target domain。核心观点是不同domain拥有某些共同特征，需要来自target domain的数据。</p><p>《Learning Invariant Feature Spaces To Transfer Skills With Reinforcement Learning》，该文章重点在不同morphology机械臂的policy网络的迁移。并非传统意义上的sim-to-real问题。</p><p><img src="/2024/09/26/research%20life/1727426172631.png" alt="1727426172631"></p><p>该文章的主体思路是寻找两个机械臂共通的隐式状态空间，然后用共通的隐式状态空间表征这两个机械臂。</p><p>举个例子来说就是我们知道robot1(2关节机械臂)在做task b时会先伸直机械臂，那么我们也鼓励robot2(3关节机械臂)也去伸直机械臂。更加广义的来说，相当于robot1给robot2提供了一个expert policy供robot2来进行imitation learning，只不过robot1和robot2结构不同，这里用一个mapping把他们对应起来</p><p>对于两个不同的domain a和b，我们有了domain a的知识，然后通过建立a和b的映射，把domain a的知识迁移到domain b上。 具体到这个robot的例子，我们想利用robot1的policy对robot2来做imitation learning，但是他们的状态无法对应，所以要做的就是找一个mapping来把他们的状态对应起来，然后在对应的状态上鼓励相似。</p><h3 id="domain-randomization"><a href="#domain-randomization" class="headerlink" title="domain randomization"></a>domain randomization</h3><p>source domain和target domain间的difference被建模为source domain中的可变量。domain randomization已在vision-based控制器中成功应用，即在低保真渲染仿真环境中的训练，通过光照、纹理即相机位置等来弥补视觉外观差异。</p><p>xuebin peng2018年发表在ICRA上的文章《Sim-to-Real Transfer of Robotic Control with Dynamics Randomization》。文章中提到的概念是dynamics randomization。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DeepRL policies are prone to exploiting idiosyncrasies of the simulator to realize behaviours that are infeasible in the real world.</span><br></pre></td></tr></table></figure><p>文章domain randomization的参数包括机械臂连杆质量，关节阻尼系，物体质量、摩擦系数和阻尼系数，桌子的高度，位置控制器的增益，动作的时间步长，观测噪声共95维。</p><p>推动任务依赖于actuators的物理属性，如质量、摩擦力等，而这些属性在真实世界中是难以获得的，因此文章提出了通过历史信息推断这些物理属性的方法。</p><p>HER(Hindsight Experience Replay)用于解决稀疏奖励的问题。依赖于off-policy方法。</p><p>采用RDPG( a method to train recurrent policies with off-policy data)，dynamics randomization参数输入到值函数中。</p><p>网络由feedword和recurrent两个branch组成，internal memory通过LSTM实现。</p><p><img src="/2024/09/26/research%20life/1727427687767.png" alt="1727427687767"></p><h3 id="System-Identification-SysID"><a href="#System-Identification-SysID" class="headerlink" title="System Identification(SysID)"></a>System Identification(SysID)</h3><p>刘正芸在2017年发表在RSS上的《Preparing for the Unknown: Learning a Universal Policy with Online System Identification》</p><p><img src="/2024/09/26/research%20life/1727439635214.png" alt="1727439635214"></p><p>Once trained, the combined algorithm, UP-OSI, can be executed in an unknown dynamic environment. At each time instance, the dynamic model parameters is first predicted by the learned system identification model. The universal control policy then takes the predicted model parameters along with the current state to compute the the optimal action (Figure 1).</p><p>UP同时接受机械臂的状态和dynamic model parameters，OSI的训练采用监督学习。UP训练过程选用TRPO方法。</p><p>学习过程分两个stage: universial policy learning和online system identification. 在第一个stage，我们对系统参数mu进行采样，训练出一个统一的控制策略pi，使其可以handle这个范围内所有的mu. 这个universial control policy以当前agent的state以及系统参数mu作为输入，输出action. 经过这个stage的训练，控制策略可以处理一定范围内的系统参数：比如如果这里的系统参数指地面摩擦系数，那么学出的行走控制器可以处理不同摩擦系数的地面.</p><p>在第二个stage，我们要做的是online地去识别出系统参数. 作者的想法也很符合直觉，我们给定agent过去一段时间的state action 序列，然后用一个简单的神经网络去拟合对应的系统参数. 当然这里的这些训练数据要尽可能的diverse,防止overfitting的发生.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>现在比较好的仿真无非三个，Isaac Sim(速度最快)，MuJoCo(精度最高)，Pybullet(上手快，性能也不错)</p><p>domain randomization就是对物理变量进行随机化，system identification则是对物理系统的dynamics进行预测。domain adaption则是通过共同的隐式空间进行策略迁移。</p><p>xuebin peng的ICRA有点集成了domain randomization和system identification的意思。</p><h3 id="TODO-LIST-1"><a href="#TODO-LIST-1" class="headerlink" title="TODO LIST"></a>TODO LIST</h3><ol><li><p>real-word adaptation</p></li><li><p>simulator augmentation</p></li><li><p>off lien RL与imitation learning</p></li><li><p>locomotion sim2real &amp; ETH SR</p></li></ol><h2 id="day3"><a href="#day3" class="headerlink" title="day3"></a>day3</h2><h3 id="real-world-adaptation"><a href="#real-world-adaptation" class="headerlink" title="real-world adaptation"></a>real-world adaptation</h3><p>2020年发表在IROS上的《Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks》，采用了PEARL(Probabilistic Embeddings for Actor-Critic RL)方法来进行元强化学习训练。</p><p><img src="/2024/09/26/research%20life/1727500886489.png" alt="1727500886489"></p><p>仿真器采用MuJoCo。所谓的Simulated Environment Design就是随机化机械臂插入任务的相关参数，比如每次reset机械臂的末端位置、方孔尺寸和物块尺寸、机械臂位置控制器步长等参数。</p><p>sim-to-real transfer via meta reinforcement learning，PEARL对于off-policy训练采样效率更高，PEARL学习一些列任务的隐式表达。这这些特性得PEARL具有很快的适应性，在真实环境采样数据代价比较高的请跨下十分有效。然后，在仿真中训练的元RL策略能够在少量试验中适应从训练分布中采样的任务。对于PEARL来说，真实环境只是另一个需要去适应的task。</p><p>文章只用元强化学习做了insertion部分的工作，抓取和控制部分是自己设计的固定算法。</p><h3 id="simulator-augmentation"><a href="#simulator-augmentation" class="headerlink" title="simulator augmentation"></a>simulator augmentation</h3><p>2019年发表在ICRA上的《Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience》，文章中提到了domain randomization + system identification的方法的相关文章。</p><p>simulation randomization方法采用的主要思想是domain randomization，即通过一个参数来对仿真进行参数分布化，然后在该参数的策略下最小化仿真迹和真实迹的差异。问题在于真实迹的获取需要大量的实物试验，而这是不现实的。SimOpt的pipeline如下，在一个仿真分布上训练得到策略网络后，针对该分布参数和策略在仿真和实物上进行采样，计算得到仿真迹的真实迹的差异后对分布参数进行更新。</p><p>policy训练选用PPO算法，分布采用的是高斯分布形式</p><p><img src="/2024/09/26/research%20life/1727509923665.png" alt="1727509923665"></p><p>另一篇是来自于google2020年发表的《Augmenting differentiable simulators with neural networks to close the sim2real gap》，该文章通过一个残差模型来增强仿真。</p><p><img src="/2024/09/26/research%20life/1727513232044.png" alt="1727513232044"></p><h3 id="DeXtreme-Transfer-of-Agile-In-hand-Manipulation-from-Simulation-to-Reality"><a href="#DeXtreme-Transfer-of-Agile-In-hand-Manipulation-from-Simulation-to-Reality" class="headerlink" title="DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality"></a>DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality</h3><p>2023年发表在ICRA上。vision-based policy，仿真采用Isaac Gym。离散时间、部分观测马可夫决策过程，算法采用PPO。ACTOR和CRITIC的观测空间并不同：</p><p><img src="/2024/09/26/research%20life/1727517004236.png" alt="1727517004236"></p><p>domain randomization策略如下图。文章中采用了ADR(Automatic Domain Randomization)，每个参数的randomization范围被建模为均匀分布，所有randomization由ADR完成。randomization的参数包括physics randomization、non-physics randomization。</p><p><img src="/2024/09/26/research%20life/1727517252458.png" alt="1727517252458"></p><p>除此之外，在pose estimation部分也使用了randomization，该部分的randomization设置如下。同时还用到了augmentation</p><p><img src="/2024/09/26/research%20life/1727518399793.png" alt="1727518399793"></p><p><img src="/2024/09/26/research%20life/1727518713303.png" alt="1727518713303"></p><h3 id="Reinforcement-Learning-for-Pivoting-Task"><a href="#Reinforcement-Learning-for-Pivoting-Task" class="headerlink" title="Reinforcement Learning for Pivoting Task"></a>Reinforcement Learning for Pivoting Task</h3><p>domain randomization</p><h3 id="TODO-LIST-2"><a href="#TODO-LIST-2" class="headerlink" title="TODO LIST"></a>TODO LIST</h3><p>locomotion sim-to-real</p><h2 id="day4"><a href="#day4" class="headerlink" title="day4"></a>day4</h2><p>摸了</p><h2 id="day5"><a href="#day5" class="headerlink" title="day5"></a>day5</h2><p>摸了</p><h2 id="day6"><a href="#day6" class="headerlink" title="day6"></a>day6</h2><p>今天和老板又讨论了一下，决定毕设去做六足了。方向大概是一个从粗到精的sim-to-real。先通过强化学习实现在仿真中的步态实现，然后实现sim-to-real，然后部署在实物上。</p><p>毕业设计页数大于等于65页，要有一定的深度。至少20篇相关论文。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">NOTICE</span><br><span class="line"></span><br><span class="line">步态实现 locomotion运动学模型正常交互 dynamics介入</span><br><span class="line">无模型data-driven</span><br><span class="line">CPG参数最小化仿真中学习sim-to-real</span><br><span class="line">从粗到精</span><br><span class="line">强化学习步态仿真-&gt;sim-to-real-&gt;实物</span><br><span class="line">谢广明transformer</span><br><span class="line">强化学习behavior  环境动态交互  强化学习在机器人上</span><br><span class="line">基于强化学习的六足</span><br><span class="line">zwx wyr</span><br><span class="line">强化学习算法相关研究综述：算法、机器人背景创新性&gt;=65页</span><br><span class="line">准备ppt</span><br><span class="line">做一篇期刊英语pratice口语听力</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="day7"><a href="#day7" class="headerlink" title="day7"></a>day7</h2><p>摸了</p><h2 id="day8"><a href="#day8" class="headerlink" title="day8"></a>day8</h2><p>回家了</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Welcome ！！！</title>
      <link href="/2024/09/26/hello-world/"/>
      <url>/2024/09/26/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome!!! 我是杨哲，就读于浙江大学机器人工程（竺可桢学院）专业，目前大四，已保研到浙江大学控制科学与工程学院控制科学与工程专业。我的研究兴趣为仿生智能（多足机器人），目前在研究机械臂操作方向。</p><p>在本科期间我参加了ZJUNlict（浙江大学RoboCup SSL战队），在2024年取得了RoboCup中国赛小型组冠军、RoboCup世界赛小型组亚军。</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
