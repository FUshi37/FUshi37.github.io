<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>从零开始的科研生活004</title><meta name="description"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="day25 2024.10.20World Model-based Perception for Visual Legged Locomotion基于Dreamer框架构建的针对四足机器人的MBRL方法。方法实现了基于视觉的。
世界模型的更新频率小于policy决策频率，每k步更新。世界模型通过DreamerV3中RSSM的方法构建，构建思路与TWIST那篇十分相似（因为都是Dreamer）。

循环模型使用GRU网络实现，解码器和编码器使用CNN网络处理深度图像实现，MLP处理本体感受观测。

策略网络根据世界模型学习，经过良好训练的世界模型中的循环状态$h_t$包含了足够的信息用于未来预测，类似于底层的马尔可夫状态$s_t$。基于这一见解，训练了一个将$h_t$作为输入的策略。训练时采用actor-cr.."><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">FUshi37's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">从零开始的科研生活004</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#day25-2024-10-20"><span class="toc-text">day25 2024.10.20</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#World-Model-based-Perception-for-Visual-Legged-Locomotion"><span class="toc-text">World Model-based Perception for Visual Legged Locomotion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day26-2024-10-21"><span class="toc-text">day26 2024.10.21</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Temporal-Difference-Learning-or-Model-Predictive-Control-2022"><span class="toc-text">Temporal Difference Learning or Model Predictive Control 2022</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day27-2024-10-22"><span class="toc-text">day27 2024.10.22</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MPC-MPPI"><span class="toc-text">MPC -&gt; MPPI</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RSSM"><span class="toc-text">RSSM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN"><span class="toc-text">RNN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day28-2024-10-23"><span class="toc-text">day28 2024.10.23</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GRU"><span class="toc-text">GRU</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day29-2024-10-24"><span class="toc-text">day29 2024.10.24</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Trends-in-the-Control-of-Hexapod-Robots-A-Survey-2021"><span class="toc-text">Trends in the Control of Hexapod Robots: A Survey 2021</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Traditional-Controllers"><span class="toc-text">Traditional Controllers</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%BF%90%E5%8A%A8%E5%AD%A6%E7%9A%84%E6%8E%A7%E5%88%B6%EF%BC%88Kinematic-Based-Control%EF%BC%89"><span class="toc-text">基于运动学的控制（Kinematic-Based Control）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%9A%84%E6%8E%A7%E5%88%B6%EF%BC%88Dynamic-Based-Control%EF%BC%89"><span class="toc-text">基于动力学的控制（Dynamic-Based Control）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6%E8%B7%AF%E5%BE%84%E5%92%8C%E6%AD%A5%E6%80%81%E8%A7%84%E5%88%92%EF%BC%88Real-Time-Path-and-Gait-Planning-Methods%EF%BC%89"><span class="toc-text">实时路径和步态规划（Real-Time Path and Gait Planning Methods）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Bio-Inspired-Controllers"><span class="toc-text">Bio-Inspired Controllers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reinforcement-Learning"><span class="toc-text">Reinforcement Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Discussion"><span class="toc-text">Discussion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Conclusion"><span class="toc-text">Conclusion</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day30-2024-10-25"><span class="toc-text">day30 2024.10.25</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Using-Parameterized-Black-Box-Priors-to-Scale-Up-Model-Based-Policy-Search-for-Robotics-2018ICRA"><span class="toc-text">Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics 2018ICRA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deep-Reinforcement-Learning-for-Multi-contact-Motion-Planning-of-Hexapod-Robots"><span class="toc-text">Deep Reinforcement Learning for Multi-contact Motion Planning of Hexapod Robots</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sim-to-real-research"><span class="toc-text">sim-to-real research</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day31-2024-10-26"><span class="toc-text">day31 2024.10.26</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Adaptive-Gait-Generation-for-Hexapod-Robots-Based-on-Reinforcement-Learning-and-Hierarchical-Framework"><span class="toc-text">Adaptive Gait Generation for Hexapod Robots Based on Reinforcement Learning and Hierarchical Framework</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Diffusion-Models-for-Reinforcement-Learning-A-Survey"><span class="toc-text">Diffusion Models for Reinforcement Learning: A Survey</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">从零开始的科研生活004</h1><time class="has-text-grey" datetime="2024-10-20T08:15:06.253Z">2024-10-20</time><article class="mt-2 post-content"><h2 id="day25-2024-10-20"><a href="#day25-2024-10-20" class="headerlink" title="day25 2024.10.20"></a>day25 2024.10.20</h2><h3 id="World-Model-based-Perception-for-Visual-Legged-Locomotion"><a href="#World-Model-based-Perception-for-Visual-Legged-Locomotion" class="headerlink" title="World Model-based Perception for Visual Legged Locomotion"></a>World Model-based Perception for Visual Legged Locomotion</h3><p>基于Dreamer框架构建的针对四足机器人的MBRL方法。方法实现了基于视觉的。</p>
<p>世界模型的更新频率小于policy决策频率，每k步更新。世界模型通过DreamerV3中RSSM的方法构建，构建思路与TWIST那篇十分相似（因为都是Dreamer）。</p>
<p><img src="/2024/10/20/research%20lifev4/1729428040891.png" alt="1729428040891"></p>
<p>循环模型使用GRU网络实现，解码器和编码器使用CNN网络处理深度图像实现，MLP处理本体感受观测。</p>
<p><img src="/2024/10/20/research%20lifev4/1729428986904.png" alt="1729428986904"></p>
<p>策略网络根据世界模型学习，经过良好训练的世界模型中的循环状态$h_t$包含了足够的信息用于未来预测，类似于底层的马尔可夫状态$s_t$。基于这一见解，训练了一个将$h_t$作为输入的策略。训练时采用actor-critic，critic可以使用特权信息训练。</p>
<p>奖励函数设置</p>
<p><img src="/2024/10/20/research%20lifev4/1729431716051.png" alt="1729431716051"></p>
<p>PPO + Isaac Gym 有类似于课程学习的过程</p>
<h2 id="day26-2024-10-21"><a href="#day26-2024-10-21" class="headerlink" title="day26 2024.10.21"></a>day26 2024.10.21</h2><h3 id="Temporal-Difference-Learning-or-Model-Predictive-Control-2022"><a href="#Temporal-Difference-Learning-or-Model-Predictive-Control-2022" class="headerlink" title="Temporal Difference Learning or Model Predictive Control 2022"></a>Temporal Difference Learning or Model Predictive Control 2022</h3><p>MBRL虽然比MFRL采样效率高，但是规划long horizon时需要的时间开销大，且很难获取一个准确的环境模型。TD-MPC，集合了MF和MB的优势，讲MPC和Model free的TD error更新结合，使用最终值函数来估计长期奖励。</p>
<p>具体而言，算法用MBRL学习用于局部轨迹优化的模型，用MFRL学习预测长期回报（用于全局优化）的价值函数（value function）。</p>
<p>在作者的MPC的pipeline中，通过输入t时刻的observation$s_t$，经过编码器$h_{\theta}$输出$t$时刻的隐向量$z_t$，将$z_t$和从正态分布采用的action$a_t$输入给环境模型$d_{theta}$，输出下一时刻的隐向量$z_{t+1}$并得到相应奖励$\hat{r}<em>t$。接着又从高斯分布中采样下一时刻的action$a</em>{t+1}$，接着如下图所示，直至预测到H时刻的隐向量$z_{H}$结束。</p>
<p><img src="/2024/10/20/research%20lifev4/98c2b2a59e54d0fd0eab307c6b7f5182.png" alt="img"></p>
<p>环境模型$d_{\theta}$的不精确可能会导致累计误差+MPC求得是t到H时刻的局部最优解。</p>
<p>TD-learning for MPC</p>
<p>TD-learning解决MPC的上述两个问题，TD-learning通过学习一个state-action value function价值函数$Q_{\theta}(s,a)$帮助MPC找到全局最优解。因为TD-learning是MF方法，所以它只学习跟奖励有关的信息，从而减少了MPC学习模型时的误差。</p>
<p>作者直接将价值函数加到MPC预测的一条轨迹所获得的回报中来学习价值函数。</p>
<p><img src="/2024/10/20/research%20lifev4/cfa29fdccc731714be0be6952c663ac5.png" alt="img"></p>
<p>标红的 Value 是状态-动作价值函数$Q_{\theta}(s,a)$所获得的回报，标红的 Rewards 是MPC规划一条轨迹所获得的奖励。也就是在MPC得到的rewards基础上加上$Q_{\theta}(s,a)$所获得的价值。</p>
<p><img src="/2024/10/20/research%20lifev4/1729576979789.png" alt="1729576979789"></p>
<p>Can we instead augment model-based planning with the strengths of model-free learning?</p>
<p>MPC算法用的是MPPI</p>
<p>TD-MPC维护了一个额外的policy来guide planning，如算法伪代码中蓝色图所示。当策略很差时，就会自然被排除在top-k之外，当粗恶略比较耗时，就可以很自然的按比例影响估计的回报。为了使采样随机化，对$\pi_{\theta}$的action像DDPG中一样应用线性退火的高斯噪声。</p>
<p><img src="/2024/10/20/research%20lifev4/1729591586033.png" alt="1729591586033"></p>
<p>TOLD与terminal value function通过TD-learning一起学习，TOLD仅对环境中预测奖励相关的元素进行建模，而不是建立一个完整的世界模型。在推理过程中，TD-MPC用TOLD进行轨迹优化，使用模型rollouts来估计短期奖励，使用terminal value function来估计长期奖励。支持连续动作空间、任意模态输入和系数奖励。</p>
<p>TOLD（Task-Oriented Latent Dynamics Model），能够支持图像或state的输入，主要在隐空间中模拟能够影响奖励的环境因素。主要 由五部分组成：</p>
<p><img src="/2024/10/20/research%20lifev4/1729577718396.png" alt="1729577718396"></p>
<p><img src="/2024/10/20/research%20lifev4/1729592401382.png" alt="1729592401382"></p>
<p>TOLD模型完全使用deterministic MLP实现，不需要RNN门控或者是概率模型。</p>
<p>相较于直接估计$Q$，文章选择学习一个策略$\pi_{\theta}$，该策略通过最小化目标函数来最大化$Q_{\theta}$。</p>
<p><img src="/2024/10/20/research%20lifev4/1729594442767.png" alt="1729594442767"></p>
<p>预期直接预测未来的状态或图片像素，文章选择预测隐向量，这也是一个很直观的思路，因为直接预测observation是十分困难的。我们建议用潜在状态一致性损失（如公式 10 所示）来正则化 TOLD，该损失迫使时间 $t + 1$ 的未来潜在状态预测 $z_{t+1} &#x3D; d_θ(z_t, a_t)$ 与对应ground-truth观察 $h_{θ^−}(s_{t+1})$ 的潜在表示相似，从而完全避免了对观察的预测。</p>
<p><img src="/2024/10/20/research%20lifev4/1729594884840.png" alt="1729594884840"></p>
<h2 id="day27-2024-10-22"><a href="#day27-2024-10-22" class="headerlink" title="day27 2024.10.22"></a>day27 2024.10.22</h2><h4 id="MPC-MPPI"><a href="#MPC-MPPI" class="headerlink" title="MPC -&gt; MPPI"></a>MPC -&gt; MPPI</h4><p>MPPI（Model Predictive Path Integral）</p>
<ol>
<li><p><strong>原理</strong>：MPPI是一种基于路径积分的控制方法，它利用随机采样的轨迹来优化控制输入。通过采样生成多条轨迹，MPPI评估这些轨迹的成本，并通过加权平均来得到控制输入。</p>
</li>
<li><p><strong>特点</strong>：</p>
<p>不依赖于精确的系统模型，而是可以使用系统的黑箱模型。</p>
<p>可以处理不确定性，通过采样和重采样策略获得更稳定的控制性能。</p>
<p>通常在高维和复杂的环境中表现良好。</p>
</li>
</ol>
<p>总结来说，MPC更强调通过模型进行优化，而MPPI则是利用随机采样的方法来获取控制策略。两者各有优劣，适用于不同的控制场景。</p>
<h3 id="RSSM"><a href="#RSSM" class="headerlink" title="RSSM"></a>RSSM</h3><p>RSSM（recurrent state-space model）是在<strong>PlaNet</strong>以及<strong>Dreamer</strong>系列的<strong>model-based强化学习</strong>中采用的，用来估计未知环境状态的模型。他的思想是将循环神经网络(下图(a))与状态空间模型(下图(b))联系在一起重构的模型(下图(c))。在model-based强化学习领域，根据PlaNet文章中所描述以及结合Dreamerv2的代码<a href="https://link.zhihu.com/?target=https://github.com/sai-prasanna/dreamerv2_torch">Dreamerv2的代码</a>，可以知利用循环神经网络输入输出的确定性关系以及状态空间模型输出的不确定性，可以利用由以往观测来推断当前的隐状态（prior），以及用当前的观测来推断当前隐状态（post）的两种方法来估计隐状态，然后用无监督的方法让两种估计的分布尽量接近，也就是用类似与两个分布的KL散度的方法来作为loss。</p>
<p><img src="/2024/10/20/research%20lifev4/1729606421659.png" alt="1729606421659"></p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>给定一个mini-batch $X\in \mathbb{R}^{n\times d} $，其中n表示batch的大小，d表示特征维度。</p>
<p><img src="/2024/10/20/research%20lifev4/v2-2dc1444ca4c96ee722da410e79a4de5e_r.jpg" alt="img"></p>
<p>第一步引入一组参数$W_{xh}\in\mathbb{R}^{d\times h} $用于和输入$X$做线性运算，其中h表示隐藏层节点数。这样的运算用NN的形式表示如下：</p>
<p><img src="/2024/10/20/research%20lifev4/v2-d8c403de5dd478c25169d4b2246ed7eb_r.jpg" alt="img"></p>
<p>第二步如果是MLP计算到这就结束了，但RNN的特点就在于，引入上一步计算得到的隐状态$H_{t-1}\in \mathbb{R}^{n\times h} $，同时引入第二组参数$W_{hh}\in \mathbb{R}^{h\times h} $用于和$H_{t-1}$做线性运算，同样把该运算用NN的形式表示如下：</p>
<p><img src="/2024/10/20/research%20lifev4/v2-5c94ce95b431448824bab0fd93dd81b8_r.jpg" alt="img"></p>
<p>然后引入第三组参数$b_h\in \mathbb{R}^{1\times h}$，表示隐藏层的偏置，用于线性相加。</p>
<p>由此，综合上面的三步运算，线性求和通过激活函数$\phi$，得到时间步 t 时隐藏层的输出表达式</p>
<p><img src="/2024/10/20/research%20lifev4/1729608701925.png" alt="1729608701925"></p>
<p>$X_tW_{xh}$和$H_{t-1}W_{hh}$运算后都是$\mathbb{R}^{n\times h}$形状的tensor，与$b_h\in \mathbb{R}^{1\times h}$的形状不一致，不过可以通过pytorch的广播机制扩充运算。因此合并后的计算得到如下的计算图表示。</p>
<p><img src="/2024/10/20/research%20lifev4/v2-d36939e1077d151dbaca761a140f5b8a_r.jpg" alt="img"></p>
<p>得到$H_t$后，再计算最终输出$O_t$。对于时间步t，先引入第四组参数$W_{ho}\in \mathbb{R}^{h\times o}$，用于和$H_{t}$做线性计算，其中$o$表示输出的特征数。然后引入$b_o\in \mathbb{R}^{1\times o}$做线性求和，同用触发pytorch的广播机制。</p>
<p><img src="/2024/10/20/research%20lifev4/1729609915237.png" alt="1729609915237"></p>
<p><img src="/2024/10/20/research%20lifev4/v2-2c77077d50fd269769fd5d3979533334_r.jpg" alt="img"></p>
<p><img src="/2024/10/20/research%20lifev4/v2-5a11e1037302a681a7d80257c1943ef4_r.jpg" alt="img"></p>
<p><img src="/2024/10/20/research%20lifev4/v2-e7680d2524c9dabea5240005d40af430_r.jpg" alt="img"></p>
<h2 id="day28-2024-10-23"><a href="#day28-2024-10-23" class="headerlink" title="day28 2024.10.23"></a>day28 2024.10.23</h2><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>首先明确，GRU是RNN的一种，提出的Motivation是为了解决传统RNN中在处理实际问题时遇到的长期记忆丢失和反向传播中的梯度消失或爆炸等问题。</p>
<p>现在考虑一个问题：假设我们的文本序列非常长，有几十个甚至几百个单词，而且文本序列中存在非常远的依赖关系。在训练过程中，循环神经网络需要通过反向传播来更新权重，以最小化预测错误。然而，由于梯度消失的影响，网络在传播梯度时可能会出现问题。</p>
<p>举个例子，假设我们的模型在预测某个句子中的第一个单词时出现了错误。为了纠正这个错误，梯度将向网络的较早时间步传播，以更新与这个错误相关的权重。但是，由于梯度消失的问题，这些梯度可能会在网络中逐渐减小，导致较早时间步的权重几乎不会被更新，从而无法有效地学习到长期依赖关系。</p>
<p>另一方面，如果我们在训练过程中遇到了梯度爆炸的问题，梯度可能会变得非常大，导致权重的剧烈更新。这可能导致网络参数值的急剧变化，甚至可能导致数值溢出和数值不稳定性，使训练过程无法收敛。</p>
<p>为了解决这些问题，GRU模型通过引入门控机制，可以更好地控制信息的流动，并有效地缓解梯度消失和梯度爆炸问题。这使得网络能够更好地捕捉到长期依赖关系，提高模型的性能和泛化能力。</p>
<p>GRU（Gated Recurrent Unit）是一种循环神经网络（RNN）架构，专门用于处理序列数据。它在标准RNN的基础上引入了门控机制，帮助模型更有效地捕捉长程依赖关系。GRU有两个主要的门：</p>
<ol>
<li><strong>更新门</strong>（Update Gate）：控制当前状态的更新程度，它决定了多少先前的信息需要保留。</li>
<li><strong>重置门</strong>（Reset Gate）：控制将多少先前的状态丢弃，它影响如何结合过去的信息。</li>
</ol>
<p>通过这些门，GRU能够在处理序列数据时更好地管理信息流，减少梯度消失的问题，因此在许多任务（如自然语言处理、时间序列预测等）中表现良好。相比于LSTM，GRU的结构更简单，参数更少，通常训练更快。</p>
<p>对于给定的时间步$t$，假设输入一个mini-batch $X_{t}\in \mathbb{R}^{n\times d}$（其中$n$表示样本数，$d$表示输入数），前一个时间步的隐状态是$H_{t-1}\in \mathbb{R}^{n\times h}$（其中$h$表示隐藏单元数），则重置门$R_t\in \mathbb{R}^{n\times h}$和更新门$Z_t\in \mathbb{R}^{n\times h}$的计算如下所示：</p>
<p><img src="/2024/10/20/research%20lifev4/1729611492916.png" alt="1729611492916"></p>
<p>两者计算步骤相同。$\sigma$为sigmoid激活函数，充当遗忘门控信号。</p>
<p><img src="/2024/10/20/research%20lifev4/v2-3f9ce05492126d04892f815d9a029fcc_720w.webp" alt="img"></p>
<p>候选隐状态，我们得到重置门$R_t$和更新门$Z_t$的计算结果，但对于要更新的隐状态是如何计算的的呢？在GRU中，通常先通过$R_t$计算得到一个候选隐状态$\tilde{H}_{t}$，然后用$Z_t$来决定更新的程度。</p>
<p><img src="/2024/10/20/research%20lifev4/v2-84140f764dff6f7502f917876f25b46c_r.jpg" alt="img"></p>
<p>候选隐状态的详细计算过程如下：</p>
<p>将充值信号$R_t$与$H_{t-1}$做按元素乘积（哈达玛积），即$R_t\odot H_{t-1}$，然后与$W_{hh}\in \mathbb{R}^{h\times h}$做矩阵乘法，得到一个大小为$\mathbb{R}^{n\times h}$的矩阵，n是batch的大小；然后用$W_{xr}\in \mathbb{R}^{d\times h}$和输入$X_t$做矩阵乘法得到一个大小为$\mathbb{R}^{n\times h}$的矩阵。同时用$b_h\in \mathbb{R}^{d\times h}$与前面两个得到的矩阵相加（pytorch广播机制）。最后得到在时间步t的候选隐状态$\tilde{H}_{t}\in \mathbb{R}^{n\times h}$，如下所示：</p>
<p><img src="/2024/10/20/research%20lifev4/1729673988038.png" alt="1729673988038"></p>
<p>tanh用来确保候选隐状态矩阵$\tilde{H}_t$中element-wise值映射到区间（1，-1）内。</p>
<p>隐状态更新，有了重置门$R_t$，更新门$Z_t$的计算结果，以及一个候选隐状态$\tilde{H}<em>{t}$，接下来进行更新操作。通过上面运算得到一个候选隐状态$\tilde{H}</em>{t}$，现在要结合更新门$Z_t$来最终确定时间步t的隐状态在多大程度上由$H_{t-1}$和新得到的$\tilde{H}_{t}$决定。因为将$Z_t$的每个元素都限定在了（0，1）区间，因此可以做一个凸组合（如DDPG中的软更新），对于当前时间步的隐状态$H_t$：</p>
<p><img src="/2024/10/20/research%20lifev4/1729675014956.png" alt="1729675014956">从该式可以看出，当 $Z_t$ 趋于0时，新的隐状态 $H_t$ 几乎来自于 $\tilde{H}_t$ 。不过需要思考一下这样设计的理由。假如所有时间步的更新门 $Z_t$ 都接近1，则无论是多长的序列数据，距离当前时刻最久远的隐状态都能很好的保留到后续的计算中。To this end，我们得到一个完成GRU单元计算流，如下图所示（简化起见，省略了待学习参数矩阵乘法的标注）。</p>
<p><img src="/2024/10/20/research%20lifev4/v2-9621d01ebdf6b9b806fd96357cd9669d_r.jpg" alt="img"></p>
<p>与RNN相同的是，得到了当前时间步的隐状态，我们要将其映射到下游任务需要的特征维度，因此引入矩阵 $W_{hq}∈\mathbb{R}^{h×q}$ 和向量 $b_q$ 用于最后的线性层映射，其中输出的特征维度为 q 。以上计算可由（4）表示</p>
<p><img src="/2024/10/20/research%20lifev4/1729675460408.png" alt="1729675460408"></p>
<p>重置门有助于提取序列中的短期依赖关系，更新门有助于获取序列数据的长期依赖关系。</p>
<h2 id="day29-2024-10-24"><a href="#day29-2024-10-24" class="headerlink" title="day29 2024.10.24"></a>day29 2024.10.24</h2><h3 id="Trends-in-the-Control-of-Hexapod-Robots-A-Survey-2021"><a href="#Trends-in-the-Control-of-Hexapod-Robots-A-Survey-2021" class="headerlink" title="Trends in the Control of Hexapod Robots: A Survey 2021"></a>Trends in the Control of Hexapod Robots: A Survey 2021</h3><h4 id="Traditional-Controllers"><a href="#Traditional-Controllers" class="headerlink" title="Traditional Controllers"></a>Traditional Controllers</h4><p>传统控制器将六足机器人视为连接六个机械臂的刚体，并单独分析每个腿的驱动和控制[7]。运动通常通过运动学和动力学模型完全描述，机器人被视为开链机构（open-chained mechanisms）。</p>
<p>常见步态三足步态（tripod），波动步态（ripple）和异时步态（metachronal）[9]。</p>
<p>由于控制架构设计中采用的方法众多，收集的信息被归类为基于运动学的控制、基于动力学的控制以及实时路径和步态规划。</p>
<h5 id="基于运动学的控制（Kinematic-Based-Control）"><a href="#基于运动学的控制（Kinematic-Based-Control）" class="headerlink" title="基于运动学的控制（Kinematic-Based Control）"></a>基于运动学的控制（Kinematic-Based Control）</h5><p>该方法依赖于根据肢体所需运动计算关节所需的角度位置或其执行器的扭矩。</p>
<p>例如，尽管欧拉-拉格朗日动力学模型被用来研究机器人的相互作用，但六足机器人的驱动是通过逆运动学和关节当前角度位置作为反馈来控制的 [5]。</p>
<p>考虑到不同的应用，Zu 等人 [11] 使用机器人的运动学模型计算了其质心 (CM) 的位置，以调整其转向半径以避开障碍物。然而，环境会影响机器人的稳定性，为了产生自适应运动，需要获得对周围环境的感知。</p>
<p>生成自适应步态最常观察到的方法之一是评估足部的接触力，以检测地面的粗糙度，例如在 [11,12] 中。Irawan 和 Nonami [13] 也检测了足部的接触力，如果它们超过某个阈值，腿部的驱动则由线性弹簧模型控制，以增加支撑阶段的冲击吸收。</p>
<p>同样地，参考文献[1+] 使用接触检测来调整六足机器人的运动。这项研究的新颖之处在于其搜索算法，该算法旨在当肢体在摆动阶段无法检测到接触力时确定新的着力点。</p>
<p>考虑到运动学约束，脚首先施加向下的轨迹，如果传感器未检测到接触力，则执行多个搜索轨迹，直到关节极限被达到。此外，本研究的另一个区别是实施了静态稳定裕度 (SSM) 来评估步态的稳定性。该方法评估机器人质心 (CM) 地面投影到支撑多边形 (SP) 边缘的距离。如果 CM 在 SP 内，则机器人处于静态稳定状态。SSM 广泛用于六足机器人的稳定性控制，主要用于评估步态阶段转换期间躯干是否直立。在 [1] 中，SSM的使用旨在确保机器人在穿过台阶和坡道时产生稳定的步态，并且加工工具保持与身体居中。</p>
<p>同样地，参考文献 [16] 使用 SSM 来调整六足机器人在最大坡度为 30 度的斜坡上行走时的姿态。除了旨在降低 Noros-Il 执行器能耗外，考虑到对操纵对象的预期运动，Ding 和 Yang [17] 也使用 SSM 来评估为承载负荷而生成的四足步态的稳定性。</p>
<p>赵等 [4]关注克服障碍的能力，并在每个步态周期计算SSM，以确定用于克服物体的轨迹是否稳定。此外，刘等 [6]利用六足机器人的关节冗余，测试了当其中一条腿出现故障时生成稳定步态的能力，并使用SSM获得机器人最稳定的配置。</p>
<p>考虑到不同的方法，参考文献 [18] 采用了纵向稳定裕度 (LSM)，它类似于 SSM，但它计算了考虑运动方向的最小距离。该方法被用来在六足机器人跨越有几个禁区的崎岖地形行走时产生稳定的运动，并且需要调整其足部以避开这些禁区。</p>
<p>除了在非结构化环境中导航和克服障碍的能力，一些研究关注了攀爬表面的能力。Henrey 等人 [19] 在六足机器人 Abigaille-III 的每只脚上都包含了干粘合机制，并评估了攀爬垂直表面所需的粘附力，以确定关节所需的扭矩。</p>
<p>文献[2o]研究了在狭窄空间（例如烟囱）中攀爬的能力，该能力利用对机器人刚度和足部与墙壁之间变形程度的估计来控制关节的驱动。进行的实验还使用惯性测量单元（IMU）来检测和控制身体在上升运动过程中的倾斜。</p>
<p>采用不同的策略，[21] 的作者在 T-RHex 的腿上实现了受蟑螂微型毛发启发的微型脊椎，这使得它能够攀爬不同的表面，例如砖墙或斜度为 135 度的胶合板。</p>
<p><img src="/2024/10/20/research%20lifev4/1729742291268.png" alt="1729742291268"></p>
<p><img src="/2024/10/20/research%20lifev4/1729742307633.png" alt="1729742307633"></p>
<h5 id="基于动力学的控制（Dynamic-Based-Control）"><a href="#基于动力学的控制（Dynamic-Based-Control）" class="headerlink" title="基于动力学的控制（Dynamic-Based Control）"></a>基于动力学的控制（Dynamic-Based Control）</h5><p>尽管一些先前描述的论文采用力矩测量来检测站立阶段和腿部轨迹的变化 [11-1+]，但对足部位置偏差或关节角位置的控制仅依赖于运动学变量（例如，位置、速度和加速度）。相反，机器人的动态公式允许通过分析六足机器人上施加的外力和力矩引起的运动偏差，来全面了解其与环境的交互作用。表2总结了所有包含此类控制的研究。</p>
<p><img src="/2024/10/20/research%20lifev4/1729743495593.png" alt="1729743495593"></p>
<p>尽管 Khudher、Powell 和 Abbod [22] 使用这种公式为六足机器人生成基于所需足部加速度的扭矩控制器，该机器人旨在帮助人道主义排雷，但文献 [23] 则侧重于动态稳定步态的生成。</p>
<p>本研究提出了六足机器人的<strong>完整动力学模型</strong>，并测试了动态步态稳定裕度 (DGSM)，该裕度通过产生的角动量和 SP 的边缘来评估步态的稳定性。在这种情况下，如果总角动量与使六足机器人翻越 SP 边缘所需的最小角动量之间的平衡为正，则六足机器人是稳定的。然而，这些模型最重要的应用之一是评估脚与地面之间的接触力，如[2+]所示，这对于六足机器人在地形和粗糙度不同的环境中导航时调整其行为至关重要。</p>
<p>Soyguder 和 Alli [25] 模拟了一个弹簧加载倒立摆 (SLIP) 模型，用于六足动物运动的动态稳定性控制。与动物的行为类似，该系统估计了执行器的扭矩值，以确保每个肢体的虚拟动力学模型在支撑阶段表现为线性弹簧，在考虑摆动阶段刚度增加的同时，吸收了足部着地造成的冲击。</p>
<p>为了调整 HITCR-II 在户外环境中的姿态，刘等人 [26] 设计了一种基于足部力补偿模型的虚拟悬挂动力学模型，用于控制机体在轻微崎岖地形上的高度、俯仰角和偏航角，并设计了一种基于站立时非相邻肢体形成的多边形的调整方法，用于跨越更崎岖地面的行走。</p>
<p>在摆动阶段，通过比较获得的足部位置和Z轴上平坦地形上的预期立足点，识别了土壤的不规则性。参考文献[9]设计了一种顺应性控制器，根据测量接触力和预期接触力之间的误差来调整肢体的位置。</p>
<p>由于六足机器人必须产生稳定的步态，才能用两条腿偏心承载物体并在非结构化地面上行走，因此实施了卡尔曼滤波器来预测零力矩点 (ZMP) 的位置，并检查其是否在支撑多边形 (SP) 内。与SSM和LSM相比，该方法的优势在于，与假设准静态情况不同，它考虑了机器人运动对保持躯干直立能力的影响，这与DGSM类似。文献[27]也使用卡尔曼滤波器来估计六足机器人在地面垂直方向行走时的姿态。该算法通过 IMU 提供的数据、接触力和每个关节的位置来估计和校正身体姿态，即使在发生足部滑移的情况下，也获得了比机器人各自的动力学模型更好的结果。</p>
<p>与[25]中提出的方法不同，Bjelonic等人[28]在Weaver上实现了阻抗控制器，以提高运动的稳定性和能量效率。该文章还讨论了通过视觉惯性里程计检测障碍物以及通过IMU和每个关节的扭矩来计算地面粗糙度，从而从周围环境中收集数据。</p>
<p>此外，在[29]中，对Weaver进行了复杂环境导航的研究。该研究使用卡尔曼滤波器估计密闭空间的地板和天花板，以便Weaver能够跨越它们。六足机器人成功地调整了其身体的高度，以越过或穿过障碍物。</p>
<p>通过本体感觉信息分析周围环境的交互作用在 [30] 中也有讨论，该文献采用欧拉-拉格朗日方法，通过接触力和执行器产生的扭矩来确定关节期望位置和实际位置之间的偏差。</p>
<p>使用机器人的动力学模型而不是运动学模型的另一个优势是能够评估执行器的能耗。Jin、Chen 和 Li [31] 通过欧拉-拉格朗日公式和关节的扭矩分布，最小化了六足机器人不同负载下的能耗。因此，该模型可以根据其必须携带的额外质量调整其步态参数。</p>
<h5 id="实时路径和步态规划（Real-Time-Path-and-Gait-Planning-Methods）"><a href="#实时路径和步态规划（Real-Time-Path-and-Gait-Planning-Methods）" class="headerlink" title="实时路径和步态规划（Real-Time Path and Gait Planning Methods）"></a>实时路径和步态规划（Real-Time Path and Gait Planning Methods）</h5><p>之前各小节中提出的研究主要讨论了适应或生成特定行走模式的能力。然而，在复杂的环境中，使用复杂而精确的机械模型来定义六足机器人的适当行为可能非常耗时。本小节介绍了一些讨论此问题的文章，但没有提及整体控制架构是基于运动学的还是基于动力学的。</p>
<p>一些收集的文章讨论了使用计算机视觉算法来计算安全轨迹。例如，参考文献 [32] 使用快速扩展随机树 (RRT) 算法来估计未知户外环境中的安全路径。同样，参考文献 [33] 研究了 RRT 方法在具有不同类型障碍物的模拟环境中的路径规划。</p>
<p>Deepa 等人 [34] 还提出了一种大规模直接单目 SLAM 方法来映射六足机器人的周围环境并规划其路径，预测威胁区域，但这种方法没有在运动规划算法中进行测试。</p>
<p>除了生成安全路线外，六足机器人还需要知道如何适应其行为。[35]中提出的方法旨在通过人工神经网络 (ANN) 和模糊逻辑来减少六足机器人控制器规划其步态所需的时间。模糊逻辑负责在未知环境中确定肢体的正确驱动方式，而人工神经网络则决定六足机器人穿越已知环境时最合适的运动模式。这种方法的优势在于，机器人不需要评估地形，因为它从经验中知道最合适的步态。</p>
<p>考虑到另一个问题，Tennakoon [36] 引入了一个支持向量机来检测六足机器人行走于脆性表面时，其接触力是否会导致地形坍塌。利用这些信息，机器人调整了其质心和足部的位置，以避免不安全的区域。</p>
<h4 id="Bio-Inspired-Controllers"><a href="#Bio-Inspired-Controllers" class="headerlink" title="Bio-Inspired Controllers"></a>Bio-Inspired Controllers</h4><p>受生物启发的控制架构旨在模仿通过实现人工神经网络 [37] 生成运动的过程。其实现旨在为六足机器人的行为提供对环境的最佳适应 [6,38]。对于哺乳动物，兴奋性和抑制性运动指令从中脑运动区通过脊髓下降，以激活或抑制每个肢体的中央模式发生器 (CPG)，从而引起屈肌和伸肌运动神经元的节律性兴奋 [39]。利用相同的生物学原理，这些系统具有一个更高层的控制中心，类似于动物的大脑，它向负责腿部驱动（即 CPG 网络）的 ANN 发送运动指令。该 ANN 通常包含每个腿部一个神经振荡器（即两个相互抑制的神经元）来生物模拟肌肉的伸展和收缩 [+0]。</p>
<p><img src="/2024/10/20/research%20lifev4/1729750457586.png" alt="1729750457586"></p>
<p><img src="/2024/10/20/research%20lifev4/1729750470204.png" alt="1729750470204"></p>
<p>为了生成对称步态，仿生架构设计中最常见的方法是使用一个包含六个耦合振荡器的CPG网络，以有节奏地激活肢体的摆动和支撑阶段，以及一个较低的控制层，将耦合振荡器的输出转换为关节的角位置，如[42]中所述。</p>
<p>通过调整神经振荡器的参数，可以获得所需的步态模式。然而，并非所有分析的出版物都使用相同类型的振荡器。[42,47] 的作者由于其输出信号的稳定性（不受外部干扰影响）而使用了非线性修正的 Van der Pool (VDP) 振荡器，而参考文献 [1,++] 则建议使用 Hopf 振荡器，因为它们稳定且简单，考虑到不需要仿生模拟昆虫肢体的驱动，因为机器人腿已经是这些生物系统的简化版本。</p>
<p>Grzelczyk, Stanczyk 和 Awrejcewicz [49] 研究了 VDP、Hopf、Rayleigh 和粘滑振荡器在 CPG 模型中的实现，并得出结论，由于其简单性和低能耗，后者是最合适的。另一方面，参考文献 [55,56] 使用脉冲神经元而不是振荡器来设计 CPG 架构，以提高计算效率，并且由于可以使用时间事件作为激活函数。</p>
<p>尽管采用不同的方法，所有这些出版物在生成三足、波浪和异时步态以及安全地在它们之间转换方面都提供了类似的结果。</p>
<p>使用仿生架构的主要兴趣是通过在系统发生扰动时调整振荡器或尖峰神经元的参数和输出信号来实现感觉反馈，从而产生自适应运动 [55,58]。最简单的方法是将IMU与具有三个Matsuoka振荡器的CPG模型结合起来，以控制六足机器人的姿态 [40]。在本研究中，使用安装在机器人质心 (CM) 上的 IMU 提供的数据来调整振荡器的输出，调整足部高度并确保身体保持水平。</p>
<p>然而，正如其他研究中观察到的，地形感知和分类对于自适应运动的生成至关重要。Liu 等人 [4] 使用来自 IMU、足部力传感器、超声波传感器和扫描激光测距仪的数据作为 CPG 网络的输入，来分析地形的崎岖程度。然而，这种方法很大程度上受六足机器人的姿态影响；</p>
<p>[52] 中提出了一种不同的策略，使用径向基函数人工神经网络对每个关节产生的扭矩进行地面在线分类。获得的值调整了 CPG 层的六个 VDP 振荡器的参数。</p>
<p>六足机器人分别在细砾石、粗砾石和光滑表面上进行了测试；Yu、Gao 和 Deng [3s] 利用反射神经元和敏感神经元来模仿动物的反射行为。这些神经元的输入是每个肢体的接触力。如果系统检测到由于与障碍物碰撞而导致的早期地面接触，则启动支撑阶段。相反，当反射神经元在摆动阶段后未检测到任何接触力时，肢体执行多个摆动轨迹以寻找新的立足点；</p>
<p>反射神经元的实现也在 [5i] 中进行了讨论，其中它们被用于基于 Hopf 的 CPG 网络来生成自适应的蟹类运动。在这项研究中，六足机器人每个足尖都配备了红外传感器，用于检测地面并执行与 [38] 中类似的行为；</p>
<p>尽管[46]中也讨论了反射机制的生成，但本研究在每条腿中使用了基于储层的循环神经网络 (RNN) 来实现这些行为。RNN 通过处理来自关节控制的感官反馈和来自力接触传感器的數據来预测肢体的状态，并调整由 CPG 模型发送的运动指令。该控制架构已在 AMOS-II 中实现，并在复杂环境中进行了测试，例如跨越间隙、在具有可变拓扑的地面上行走以及攀爬表面，在适应性方面取得了良好的效果。</p>
<p>利用陀螺仪评估六足机器人的姿态，王等人 [57] 研究了肢体运动适应性以调整身体姿态并执行平地和斜坡之间的过渡运动。该机器人能够爬上倾斜度高达 16 度的斜坡；</p>
<p>与其他研究不同，AmphiHex-II 利用其可变刚度的肢体来调整其在爬坡和楼梯、游泳和在非结构化地面上行走时的姿势 [51]。与评估对称步态产生的研究类似，对基于 Hopf 的 CPG 模型的分析包括调整其参数以生成三足模式，这突出了这种解决方案在复杂环境中导航的简单性。</p>
<p>这些控制架构还测试了调整生成步态以克服障碍的能力。尽管实验是在室内规则地面上进行的，但参考文献[48]讨论了在CPG控制器中集成在线路径规划。本研究使用了一个小脑步态运动检测神经网络来处理视觉传感器提供的数据，并确定机器人轨迹的适当转弯半径。然而，由于视觉传感器放置在六足机器人的前方，并且该神经网络仅处理在每个时间点获取的数据，因此系统没有关于先前避开的障碍物的信息，导致机器人当转弯半径未完美调整时与它们发生碰撞。</p>
<p>Zhong 等人 [53] 也提出了一种利用计算机视觉的避障方法，但这种情况下，CPG 的输出被调整以越过或跨越物体。虽然其目标是空间探索，但在 [54], 一个 SNN 被用来仿生 LAURON V 的肢体驱动，并研究了其轨迹适应以克服障碍和越过障碍。</p>
<h4 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h4><p>Lele 等人[7] 在 SNN 中实现了 RL 以生成稳定的三足步态。通过这种方法，六足机器人学会了协调其腿部，无需预先编程的步态序列。使用陀螺仪和摄像头作为 SNN 的输入数据，系统的奖励和网络权重的调整基于机器人的平衡。由于目标是向前行走，因此相机提供了对所生成运动的视觉确认。与其他学习方法相比，这种方法在训练期间花费的时间更少，其中 70% 的情况收敛到三足步态。然而，该领域最近的兴趣带来了允许六足机器人自学如何应对周围环境的可能性 [38]：</p>
<p>障碍物规避：参考文献 [60] 将模糊逻辑与 Q 学习相结合，生成用于障碍物规避的实时控制。模糊逻辑用于将放置在六足上的声纳提供的数据组织和分组为一组有限状态，从而简化了算法的学习过程。该方法收敛速度快，具有最优策略，能够改变六足的方向以避开不同的障碍物；</p>
<p>自适应运动：在[61]中，蒙特卡罗方法被用来通过放置在每个脚尖上的力传感器检测步态阶段之间的转变。这些数据与SSM一起被用来确定需要驱动哪条腿以确保机器人的稳定性。在这种情况下，算法在每个回合结束时评估其结果，并且不能保证智能体访问所有状态，这可以提供一个贪婪策略。自适应步态的生成也在[62]中讨论。该方法包含一个具有两层的 CPG 模型。其中一层负责肢体间的协调，以产生三足步态、波浪步态或交替步态，而另一层则必须通过正确驱动膝关节和踝关节来调整每个肢体的行为。因此，为了避免手动调整第二层振荡器，我们实施了深度确定性策略梯度算法。该算法使用机器人的位置和速度以及关节的扭矩、角度位置和速度作为观测值，以获得振荡器的正确参数（例如，振幅和相位）。该算法的奖励函数惩罚高能耗，但奖励高航向速度值。该方法在 1400 个回合后收敛到一个解，并且机器人能够成功地调整其运动以适应不同摩擦系数的不同表面。</p>
<p>损伤恢复：Verma 等人 [63] 提出了一种基于近端策略优化的方法，使用监督学习神经网络进行损伤自诊断来实现损伤恢复。该算法可以在六足机器人失去一到两条腿的情况下找到一种步态策略。相反，Chatzilygeroudis 和 Mouret [64] 认为基于模型的策略搜索算法在机器人控制方面更有效，并设计了一种无重置的试错算法来恢复内部损伤。在这项研究中，六足机器人可以在不到一分钟的时间内自主学习最佳行走策略，即使一个或两个肢体出现故障，尽管存在计算问题。两种方法的优点都是不需要智能体在训练期间每次情节结束后都返回到初始位置。在 [65] 中，还关注了损伤恢复问题，作者提出了一种基于地图的多策略算法。该方法存储并映射所有可能的策略，以选择提供最大预期奖励的策略。尽管讨论了自恢复能力，这项研究仅测试了在爬楼梯环境中的运动生成，其中模型的一些特征，例如某些脚趾的尺寸，被改变以诱发一些损伤。</p>
<p><img src="/2024/10/20/research%20lifev4/1729757138731.png" alt="1729757138731"></p>
<h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><p><img src="/2024/10/20/research%20lifev4/1729758228038.png" alt="1729758228038"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758412237.png" alt="1729758412237"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758523766.png" alt="1729758523766"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758552230.png" alt="1729758552230"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758571002.png" alt="1729758571002"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758642106.png" alt="1729758642106"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758656188.png" alt="1729758656188"></p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p><img src="/2024/10/20/research%20lifev4/1729758727063.png" alt="1729758727063"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758767539.png" alt="1729758767539"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758790796.png" alt="1729758790796"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758812544.png" alt="1729758812544"></p>
<p><img src="/2024/10/20/research%20lifev4/1729758828760.png" alt="1729758828760"></p>
<h2 id="day30-2024-10-25"><a href="#day30-2024-10-25" class="headerlink" title="day30 2024.10.25"></a>day30 2024.10.25</h2><h3 id="Using-Parameterized-Black-Box-Priors-to-Scale-Up-Model-Based-Policy-Search-for-Robotics-2018ICRA"><a href="#Using-Parameterized-Black-Box-Priors-to-Scale-Up-Model-Based-Policy-Search-for-Robotics-2018ICRA" class="headerlink" title="Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics 2018ICRA"></a>Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics 2018ICRA</h3><p>Black-DROPS，利用参数化的黑盒先验来扩展到高维系统以及对先验信息的较大误差具有鲁棒性。</p>
<p>Black-DROPS 为 PILCO 带来了两项主要优势：(1) 可以使用任何奖励函数或策略参数化（包括非可微策略，例如有限状态机），以及 (2) 它是一种高度并行的算法，可以利用多核计算机的优势。</p>
<p>使用先验知识加速策略搜索。通过在模型上使用先验 [10]，[11]，[12]，[13]，[33] 可以减少基于模型的策略搜索中的交互时间；即，从对动力学的初始猜测开始，然后学习残差模型。具有先验 [10] 的 PILCO 和 PI-REM [12] 紧密相关，因为它们都使用 PILCO 的策略搜索过程。带有先验的 PILCO 使用模拟数据来创建高斯过程先验，而 PI-REM 使用解析方程来构建先验模型。带有先验的 PILCO 的主要限制是，它隐式地要求任务在 PILCO 的先验模型中得到解决（为了获得原始论文 [10] 中显示的加速效果）。GP-ILQG [11] 也像 PI-REM 一样学习残差模型，然后使用 ILQG [34] 的修改版本来找到给定模型不确定性的策略。然而，GP-ILQG 需要先验模型可微分。</p>
<p>我们希望得到一个模型$\hat{F}$，它能够尽可能准确地近似我们系统未知的动力学$F$，前提是给定一个初始猜测$M$。我们依靠高斯过程 (GP) 来实现这一点，因为它们已成功应用于许多基于模型的强化学习方法[7]、[8]、[41]、[42]、[5]、[40]、[6]。高斯过程 (GP) 是多元高斯分布到无限维随机过程的扩展，其中任意有限维组合都将服从高斯分布 [43]。</p>
<p><strong>模型学习方法</strong></p>
<p>我们的模型学习方法，我们称之为 GP-MI（高斯过程模型识别），它结合了非参数模型学习和参数模型识别，与 [16] 中的方法有关，但两者之间存在一些关键差异。首先，[16] 中的模型学习过程依赖于机械手方程，并且不能轻松地用于不直接符合该方程的机器人（例如，我们实验中的六足机器人或具有复杂动力学的软体机器人），而 GP-MI 对先验模型没有施加任何结构，除了提供一些可调参数（连续或离散）。此外，[16] 中的方法与逆动力学模型相关联，在一般情况下不能与正向模型一起使用（对于长期正向预测是必要的）；相反，GP-MI 可以与逆或正向动力学模型一起使用，并且通常可以与任何黑盒可调先验模型一起使用。</p>
<h3 id="Deep-Reinforcement-Learning-for-Multi-contact-Motion-Planning-of-Hexapod-Robots"><a href="#Deep-Reinforcement-Learning-for-Multi-contact-Motion-Planning-of-Hexapod-Robots" class="headerlink" title="Deep Reinforcement Learning for Multi-contact Motion Planning of Hexapod Robots"></a>Deep Reinforcement Learning for Multi-contact Motion Planning of Hexapod Robots</h3><p>文章将RL与Multi-contact motion planning与Transition feasibility结合起来，实现六足机器人走梅花桩的任务。</p>
<p>可以看一下Multi-contact motion planning和Transition feasibility的相关概念和文献。</p>
<h3 id="sim-to-real-research"><a href="#sim-to-real-research" class="headerlink" title="sim-to-real research"></a>sim-to-real research</h3><h2 id="day31-2024-10-26"><a href="#day31-2024-10-26" class="headerlink" title="day31 2024.10.26"></a>day31 2024.10.26</h2><h3 id="Adaptive-Gait-Generation-for-Hexapod-Robots-Based-on-Reinforcement-Learning-and-Hierarchical-Framework"><a href="#Adaptive-Gait-Generation-for-Hexapod-Robots-Based-on-Reinforcement-Learning-and-Hierarchical-Framework" class="headerlink" title="Adaptive Gait Generation for Hexapod Robots Based on Reinforcement Learning and Hierarchical Framework"></a>Adaptive Gait Generation for Hexapod Robots Based on Reinforcement Learning and Hierarchical Framework</h3><h3 id="Diffusion-Models-for-Reinforcement-Learning-A-Survey"><a href="#Diffusion-Models-for-Reinforcement-Learning-A-Survey" class="headerlink" title="Diffusion Models for Reinforcement Learning: A Survey"></a>Diffusion Models for Reinforcement Learning: A Survey</h3></article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/10/24/sim-to-real%20research/" title="sim-to-real研究综述和毕设研究计划"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: sim-to-real研究综述和毕设研究计划</span></a><a class="button is-default" href="/2024/10/12/research%20lifev3/" title="从零开始的科研生活003"><span class="has-text-weight-semibold">Next: 从零开始的科研生活003</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/FUshi37"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> FUshi37 2024</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>