<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>从零开始的科研生活002</title><meta name="description"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="day8回家了
day9看了一点ANYMAL的文章
day10ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robotslocomotion policy是固定情景下的运动策略，在固定障碍物环境下训练。perception module和navigaion module在不同障碍物布置的情境下训练。
训练算法用的是PPO，不过对PPO做了一定的改进。hydird actor输出，low-level command用高斯分布，skill selection用分类分布。训练在一个hierarchical set-up中进行的。

文章并没有具体提及sim-to-real的办法，不过确实有对障碍物(场景)的随机化。
这篇文章值得多看几遍。
Si.."><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">FUshi37's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">从零开始的科研生活002</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#day8"><span class="toc-text">day8</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day9"><span class="toc-text">day9</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day10"><span class="toc-text">day10</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ANYmal-Parkour-Learning-Agile-Navigation-for-Quadrupedal-Robots"><span class="toc-text">ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sim-to-Real-Learnging-Agile-Locomotion-For-Quadruped-Robots"><span class="toc-text">Sim-to-Real: Learnging Agile Locomotion For Quadruped Robots</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day11"><span class="toc-text">day11</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RMA-Rapid-Motor-Adaptation-for-Legged-Robots"><span class="toc-text">RMA: Rapid Motor Adaptation for Legged Robots</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation"><span class="toc-text">Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Real2sim2real-Self-supervised-learning-of-physical-single-step-dynamic-actions-for-planar-robot-casting"><span class="toc-text">Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day12"><span class="toc-text">day12</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Robot-Parkour-Learning"><span class="toc-text">Robot Parkour Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day13"><span class="toc-text">day13</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real"><span class="toc-text">Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day14"><span class="toc-text">day14</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sim-to-Real-Transfer-in-Deep-Reinforcement-Learning-for-Robotics-a-Survey"><span class="toc-text">Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day15"><span class="toc-text">day15</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sim-to-real-Six-legged-robot-control-with-deep-reinforcement-learning-and-curriculum-learning"><span class="toc-text">Sim-to-real: Six-legged robot control  with deep reinforcement learning and curriculum learning.</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day16"><span class="toc-text">day16</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer-2019ICML"><span class="toc-text">Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer 2019ICML</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer-2024ICRA"><span class="toc-text">TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer 2024ICRA</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">从零开始的科研生活002</h1><time class="has-text-grey" datetime="2024-10-04T02:11:19.138Z">2024-10-04</time><article class="mt-2 post-content"><h2 id="day8"><a href="#day8" class="headerlink" title="day8"></a>day8</h2><p>回家了</p>
<h2 id="day9"><a href="#day9" class="headerlink" title="day9"></a>day9</h2><p>看了一点ANYMAL的文章</p>
<h2 id="day10"><a href="#day10" class="headerlink" title="day10"></a>day10</h2><h3 id="ANYmal-Parkour-Learning-Agile-Navigation-for-Quadrupedal-Robots"><a href="#ANYmal-Parkour-Learning-Agile-Navigation-for-Quadrupedal-Robots" class="headerlink" title="ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots"></a>ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots</h3><p>locomotion policy是固定情景下的运动策略，在固定障碍物环境下训练。perception module和navigaion module在不同障碍物布置的情境下训练。</p>
<p>训练算法用的是PPO，不过对PPO做了一定的改进。hydird actor输出，low-level command用高斯分布，skill selection用分类分布。训练在一个hierarchical set-up中进行的。</p>
<p><img src="/2024/10/04/research%20lifev2/1728098611335.png" alt="1728098611335"></p>
<p>文章并没有具体提及sim-to-real的办法，不过确实有对障碍物(场景)的随机化。</p>
<p>这篇文章值得多看几遍。</p>
<h3 id="Sim-to-Real-Learnging-Agile-Locomotion-For-Quadruped-Robots"><a href="#Sim-to-Real-Learnging-Agile-Locomotion-For-Quadruped-Robots" class="headerlink" title="Sim-to-Real: Learnging Agile Locomotion For Quadruped Robots"></a>Sim-to-Real: Learnging Agile Locomotion For Quadruped Robots</h3><p>仿真用的是PyBullet，观测空间舍弃了观测值容易漂和变化剧烈的观测值，如关节速度项，这是因为越紧致的观测空间越容易减小gap。</p>
<p>减小sim-to-real gap的方法主要有两个，第一个是提高仿真的保真度。该方法首先建立了一个精确的URDF模型(对于难以获得精确值的机器人，通过SysID也可以实现这部分工作)。此外还建立了一个更精准的执行器(关节)模型(PD控制增益项不能太大，否在在reality中容易振荡)根据理想直流电机动力学模型为基础对执行器进行建模，同时修改了线性转矩-电流的关系。此外是延迟的解决。对延迟进行建模，对最相邻两个time step的observation进行线性插值(对时间插值)，同时测量实际电机执行的延迟并将其建模到仿真中。</p>
<p>第二个是学习鲁棒的控制器。有三个方向，第一个是domain randomization；第二个是训练时增加随机扰动，具体为在机器人基体上施加随即方向、随机大小的力；第三个是使用紧致的观测空间，因为高纬度的观测空间可能导致机器人对训练场景(仿真)过拟合。</p>
<p><img src="/2024/10/04/research%20lifev2/1728120665560.png" alt="1728120665560"></p>
<h2 id="day11"><a href="#day11" class="headerlink" title="day11"></a>day11</h2><h3 id="RMA-Rapid-Motor-Adaptation-for-Legged-Robots"><a href="#RMA-Rapid-Motor-Adaptation-for-Legged-Robots" class="headerlink" title="RMA: Rapid Motor Adaptation for Legged Robots"></a>RMA: Rapid Motor Adaptation for Legged Robots</h3><p>Adaptation Module通过状态历史state和action来估计隐式向量z，z是privileged knowledge编码后的结果，在reality中因为无法获取privileged knowledge，z通过机器人的历史状态和action来获得。Base Policy通过当前状态state、上一步action和隐式向量z来输出机器狗的action。</p>
<p>仿真中可以直接拿到priviledged knowledge，所以Adapation Module的训练可以采用监督学习。训练过程中on-policy，即同步和base policy训练。</p>
<p><img src="/2024/10/04/research%20lifev2/1728192253044.png" alt="1728192253044"></p>
<h3 id="Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation"><a href="#Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation" class="headerlink" title="Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation"></a>Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation</h3><p>首先在一个相对rough的仿真中训练一个模型，然后在实物上部署并收集数据，最后根据收集到的数据去仿真中更新模型和方法。</p>
<p>对于较软的物体，仿真建模通常不够精准，Sim2Real2Sim方法就是为了解决这个问题而提出的。</p>
<p>仿真用的是Gazebo</p>
<p><img src="/2024/10/04/research%20lifev2/1728216401553.png" alt="1728216401553"></p>
<h3 id="Real2sim2real-Self-supervised-learning-of-physical-single-step-dynamic-actions-for-planar-robot-casting"><a href="#Real2sim2real-Self-supervised-learning-of-physical-single-step-dynamic-actions-for-planar-robot-casting" class="headerlink" title="Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting"></a>Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting</h3><p>Parameter estimation using Differential Evolution [53,55] with associated code and datasets from 64,350 simulated experiments and 2,076 physical experiments with 3 distinct cables.</p>
<p><img src="/2024/10/04/research%20lifev2/1728223180124.png" alt="1728223180124"></p>
<p>首先从reality中收集physical dataset，并选取子集来进行仿真的SysID。</p>
<p>然后文章从PyBullet、两个版本的NVIDIA Isaac Gym三个仿真做了对比。</p>
<h2 id="day12"><a href="#day12" class="headerlink" title="day12"></a>day12</h2><h3 id="Robot-Parkour-Learning"><a href="#Robot-Parkour-Learning" class="headerlink" title="Robot Parkour Learning"></a>Robot Parkour Learning</h3><p>文章提出了一个端到端的机器狗跑酷策略网络，每个具体技能的训练分为两阶段。第一阶段为soft dynamics预训练，障碍物可穿越，训练过程采用了课程学习，障碍物难度逐渐增大。同时该阶段的训练用到了privilege visual information。</p>
<p>第二阶段为fine-tune阶段，同样使用PPO，在预训练后在Hard Dynamics Constraints上再对每个运动技能进行训练。该阶段的训练能够实现sim2real的转换。五个skill的训练过程用到了privilege visual information</p>
<p>训练完五个技能的policy网络后，通过一个DAgger蒸馏一个vision-based的parkour策略网络。策略参数化为GRU，输入包括recurrent latent state、本体感知、上一步的aciton和经过CNN处理得到的深度图的latent embedding。</p>
<p>经过蒸馏后，策略具有了一定sim-to-real transfer的能力(terrain friction and mass properties)，不过还需要对visual appearence进行sim-to-real transfer。</p>
<p><img src="/2024/10/04/research%20lifev2/1728292261234.png" alt="1728292261234"></p>
<p>毕设申题需要翻译一篇文献，翻译了一下RMA那篇文章。</p>
<h2 id="day13"><a href="#day13" class="headerlink" title="day13"></a>day13</h2><h3 id="Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real"><a href="#Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real" class="headerlink" title="Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real"></a>Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real</h3><p>文章研究的是双足机器人，文章的主要思想就是蒸馏。策略蒸馏部分可以在看一下。</p>
<p><img src="/2024/10/04/research%20lifev2/1728378609266.png" alt="1728378609266"></p>
<p>系统辨识提升建模精度，文章发现膝关节的reflected inertia of motors十分重要，仿真器用的是Mujoco。文章发现没有使用dynamics rondamization策略网络也可以实现sim-to-real transfer。</p>
<p>文章中提到没有使用data-driven的方法来对执行机构进行建模。</p>
<p>文章中使用了状态估计的方法，状态估计用来模拟传感器。</p>
<h2 id="day14"><a href="#day14" class="headerlink" title="day14"></a>day14</h2><h3 id="Sim-to-Real-Transfer-in-Deep-Reinforcement-Learning-for-Robotics-a-Survey"><a href="#Sim-to-Real-Transfer-in-Deep-Reinforcement-Learning-for-Robotics-a-Survey" class="headerlink" title="Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey"></a>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey</h3><p>主要的sim-to-real方法一览</p>
<p><img src="/2024/10/04/research%20lifev2/1728453552003.png" alt="1728453552003"></p>
<ol>
<li>系统辨识：建立物理系统的精确模型，使得仿真更加精准</li>
<li>域随机化：与其精确建立现实世界的所有参数，不如对仿真进行高度随机化，以覆盖现实世界数据的真是分布</li>
<li>域自适应：利用源数据来提高学习模型在数据量较少的目标域上的性能。在源域和目标域不同的特征空间寻找统一的特征集。</li>
<li>带干扰的学习：奖励函数加入随机噪声。</li>
<li>仿真环境：Gazebo PyBullet MuJoCo IsaacGym</li>
<li>元强化学习</li>
<li>模仿学习</li>
<li>知识蒸馏</li>
</ol>
<p><img src="/2024/10/04/research%20lifev2/1728557708842.png" alt="1728557708842"></p>
<h2 id="day15"><a href="#day15" class="headerlink" title="day15"></a>day15</h2><h3 id="Sim-to-real-Six-legged-robot-control-with-deep-reinforcement-learning-and-curriculum-learning"><a href="#Sim-to-real-Six-legged-robot-control-with-deep-reinforcement-learning-and-curriculum-learning" class="headerlink" title="Sim-to-real: Six-legged robot control  with deep reinforcement learning and curriculum learning."></a>Sim-to-real: Six-legged robot control  with deep reinforcement learning and curriculum learning.</h3><p>文章的主体思路和谢广明的TRO差不多，粗仿真+精确仿真+课程学习</p>
<p>仿真用的是V-rep，sim-to-real主要是靠精准的逆运动学实现的，所以这并不是一个端到端的方法。</p>
<p><img src="/2024/10/04/research%20lifev2/1728742522363.png" alt="1728742522363"></p>
<h2 id="day16"><a href="#day16" class="headerlink" title="day16"></a>day16</h2><h3 id="Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer-2019ICML"><a href="#Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer-2019ICML" class="headerlink" title="Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer 2019ICML"></a>Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer 2019ICML</h3><p>SRL CL RL Distillation</p>
<p><img src="/2024/10/04/research%20lifev2/1728636890456.png" alt="1728636890456"></p>
<p><img src="/2024/10/04/research%20lifev2/1728637438867.png" alt="1728637438867"></p>
<h3 id="TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer-2024ICRA"><a href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer-2024ICRA" class="headerlink" title="TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer 2024ICRA"></a>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer 2024ICRA</h3><p>该篇文章用知识蒸馏的方法去训练了类似于状态估计器的模型，用privileged states去训练teacher world model，再去蒸馏训练student world model。（world model即dynamics model）</p>
<p>该文章针对vision-based的方法，在训练teacher world model的时候对输入图片进行了domain randomization。student world model同理。</p>
<p>teacher world model的训练是用的Dreamer算法，一种model-based强化学习算法。</p>
<p><img src="/2024/10/04/research%20lifev2/1728623533715.png" alt="1728623533715"></p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/10/12/research%20lifev3/" title="从零开始的科研生活003"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: 从零开始的科研生活003</span></a><a class="button is-default" href="/2024/09/26/research%20life/" title="从零开始的科研生活001"><span class="has-text-weight-semibold">Next: 从零开始的科研生活001</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/FUshi37"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> FUshi37 2024</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>