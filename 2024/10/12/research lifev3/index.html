<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>从零开始的科研生活003</title><meta name="description"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="day17回学校了，做了会ppt
day18 2024.10.13做ppt，RoboCup开会
day19 2024.10.14Learning quadrupedal locomotion over challenging terrainScience Robotics 2020SR对物理系统进行逼真建模，包括执行器。
域随机化
teacher-student，teacher根据特权信息编码成隐式向量进行训练
时间卷积网络TCN根据本体感觉状态的历史信息推断隐式向量
课程学习，地形难度随训练过程逐渐提升

Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR关节分析模型建模+数据驱动学习执行器网络

1.适用于机器人系统-&amp;gt;.."><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">FUshi37's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">从零开始的科研生活003</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#day17"><span class="toc-text">day17</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day18-2024-10-13"><span class="toc-text">day18 2024.10.13</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day19-2024-10-14"><span class="toc-text">day19 2024.10.14</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-quadrupedal-locomotion-over-challenging-terrainScience-Robotics-2020SR"><span class="toc-text">Learning quadrupedal locomotion over challenging terrainScience Robotics 2020SR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR"><span class="toc-text">Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day20-2024-10-15"><span class="toc-text">day20 2024.10.15</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day21-2024-10-16"><span class="toc-text">day21 2024.10.16</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR-1"><span class="toc-text">Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day22-2024-10-17"><span class="toc-text">day22 2024.10.17</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DayDreamer-World-Models-for-Physical-Robot-Learning"><span class="toc-text">DayDreamer: World Models for Physical Robot Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day23-2024-10-18"><span class="toc-text">day23 2024.10.18</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2022-A-Survey-on-Model-based-Reinforcement-Learning"><span class="toc-text">2022 A Survey on Model-based Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E6%B3%95%EF%BC%88%E7%8A%B6%E6%80%81%E3%80%81%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4%E5%B0%8F%E4%B8%94%E6%9C%89%E9%99%90%EF%BC%89-R-MAX"><span class="toc-text">表格法（状态、动作空间小且有限）-&gt; R-MAX</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Prediction-Loss"><span class="toc-text">Prediction Loss</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8D%95%E6%AD%A5%E8%BD%AC%E7%A7%BB%E6%A8%A1%E5%9E%8B%EF%BC%88fit-one-step-transition%EF%BC%89"><span class="toc-text">单步转移模型（fit one-step transition）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E6%AD%A5%E9%A2%84%E6%B5%8B"><span class="toc-text">多步预测</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reduced-Error"><span class="toc-text">Reduced Error</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Lipschitz-Continuity-Constraint"><span class="toc-text">Lipschitz Continuity Constraint</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Distribution-Matching"><span class="toc-text">Distribution Matching</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Robust-Model-Learning"><span class="toc-text">Robust Model Learning</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Complex-Environments-Dynamics"><span class="toc-text">Complex Environments Dynamics</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#day24-2024-10-19"><span class="toc-text">day24 2024.10.19</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer"><span class="toc-text">TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PlaNet-2019ICML-Learning-Latent-Dynamics-for-Planning-from-Pixels"><span class="toc-text">PlaNet 2019ICML Learning Latent Dynamics for Planning from Pixels.</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">从零开始的科研生活003</h1><time class="has-text-grey" datetime="2024-10-12T07:10:29.108Z">2024-10-12</time><article class="mt-2 post-content"><h2 id="day17"><a href="#day17" class="headerlink" title="day17"></a>day17</h2><p>回学校了，做了会ppt</p>
<h2 id="day18-2024-10-13"><a href="#day18-2024-10-13" class="headerlink" title="day18 2024.10.13"></a>day18 2024.10.13</h2><p>做ppt，RoboCup开会</p>
<h2 id="day19-2024-10-14"><a href="#day19-2024-10-14" class="headerlink" title="day19 2024.10.14"></a>day19 2024.10.14</h2><h3 id="Learning-quadrupedal-locomotion-over-challenging-terrainScience-Robotics-2020SR"><a href="#Learning-quadrupedal-locomotion-over-challenging-terrainScience-Robotics-2020SR" class="headerlink" title="Learning quadrupedal locomotion over challenging terrainScience Robotics 2020SR"></a>Learning quadrupedal locomotion over challenging terrainScience Robotics 2020SR</h3><p>对物理系统进行逼真建模，包括执行器。</p>
<p>域随机化</p>
<p>teacher-student，teacher根据特权信息编码成隐式向量进行训练</p>
<p>时间卷积网络TCN根据本体感觉状态的历史信息推断隐式向量</p>
<p>课程学习，地形难度随训练过程逐渐提升</p>
<p><img src="/2024/10/12/research%20lifev3/1728886299186.png" alt="1728886299186"></p>
<h3 id="Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR"><a href="#Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR" class="headerlink" title="Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR"></a>Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR</h3><p>关节分析模型建模+数据驱动学习执行器网络</p>
<p><img src="/2024/10/12/research%20lifev3/1728891298239.png" alt="1728891298239"></p>
<p>1.适用于机器人系统-&gt;建模困难、自由度高的方法   硬件系统特性   无人机sim-to-real</p>
<p>对象区别，构建仿真系统的方法</p>
<p>2.系统可以拿到，仿真系统；哪个可实现 哪个不可实现</p>
<p>3.核心关注算法点，方法；实现手段</p>
<p>4.仿真嵌入AIGC，生成。	认知孪生系统，虚实交互，交互反馈提升仿真系统</p>
<p>5.六足区别于四足，被动式</p>
<p>细节、细节差别、细微差别</p>
<p>揣测工作的源头，溯源而上</p>
<p>虚实交互</p>
<p>单机智能直接感知</p>
<p>虚拟场景发现问题</p>
<h2 id="day20-2024-10-15"><a href="#day20-2024-10-15" class="headerlink" title="day20 2024.10.15"></a>day20 2024.10.15</h2><p>摸了</p>
<h2 id="day21-2024-10-16"><a href="#day21-2024-10-16" class="headerlink" title="day21 2024.10.16"></a>day21 2024.10.16</h2><h3 id="Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR-1"><a href="#Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR-1" class="headerlink" title="Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR"></a>Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR</h3><p><img src="/2024/10/12/research%20lifev3/1729064820128.png" alt="1729064820128"></p>
<p>求解器[41]</p>
<p>12个关节的执行器模型各自训练，输入关节角度差和关节角速度，端到端输出关节位置，然后通过一个PD控制器实现位置控制。</p>
<p>TRPO</p>
<p>关节执行器网络建模激活函数选的是softsign。</p>
<p>策略网络训练选的是tanh。</p>
<p>没有用力传感器作为observation的一部分，用joint state history代替。</p>
<p>课程学习、课程参数调节reward各项系数值。</p>
<p>训练过程中对observation的velocity做randomization。</p>
<p><img src="/2024/10/12/research%20lifev3/1729069222649.png" alt="1729069222649"></p>
<h2 id="day22-2024-10-17"><a href="#day22-2024-10-17" class="headerlink" title="day22 2024.10.17"></a>day22 2024.10.17</h2><p>Model-Based RL -&gt; Dreamer相关论文</p>
<h3 id="DayDreamer-World-Models-for-Physical-Robot-Learning"><a href="#DayDreamer-World-Models-for-Physical-Robot-Learning" class="headerlink" title="DayDreamer: World Models for Physical Robot Learning"></a>DayDreamer: World Models for Physical Robot Learning</h3><p>文章用Dreamer在物理机器人上在线学习，在实验中，仅用一小时四足机器人就能学会翻到转身和行走，而无需使用仿真。</p>
<p><img src="/2024/10/12/research%20lifev3/1729234733820.png" alt="1729234733820"></p>
<p>Dreamer是一个MBRL算法，学习世界模型（即dynamics model）。世界模型试通过RSSM（Recurrent State-space Model）实现的。</p>
<p>RSSM包含四个部分：<br><img src="/2024/10/12/research%20lifev3/1729235516657.png" alt="1729235516657"></p>
<p><img src="/2024/10/12/research%20lifev3/1729235125653.png" alt="1729235125653"></p>
<h2 id="day23-2024-10-18"><a href="#day23-2024-10-18" class="headerlink" title="day23 2024.10.18"></a>day23 2024.10.18</h2><h3 id="2022-A-Survey-on-Model-based-Reinforcement-Learning"><a href="#2022-A-Survey-on-Model-based-Reinforcement-Learning" class="headerlink" title="2022 A Survey on Model-based Reinforcement Learning"></a>2022 A Survey on Model-based Reinforcement Learning</h3><p>MBRL（Model-based Reinforcement Learning）能够显著提高采样效率，在有世界模型后，智能体的训练可能具有一定的想象力，即可以预测不同action产生的结果，因此在训练过程中可以选用恰当的action减小试错成本。</p>
<p>MBRL的主要任务是学习状态转移函数M（或者叫state transition dynamics）。理论上来说要实现好的训练效果，MFRL最好是直接与真实环境交互采样数据（经验数据）。</p>
<p>(a)为on-policy强化学习，(b)为off-policy强化学习，(c)为actor-critic，(d)为model-based方法，(e)为off-policy actor-critic方法。</p>
<p>与actor-critic相比，model-based方法重构了状态转移动力学，尽管critic计算设计状态转移动力学，但MBRL中的学习模型与策略解耦，因此该模型可用于评估其他策略，而critic是与actor绑定的。</p>
<p>off-policy、actor-critic和MBRL是并行的，可以组合使用（如图e）</p>
<p><img src="/2024/10/12/research%20lifev3/1729238916444.png" alt="1729238916444"></p>
<p>对于一个给定的MBRL问题，MDP中&lt;S,A,M,R,$\gamma$&gt;要学习的是状态转移动力学(state transition dynamics)M和奖励函数R。历史轨迹数据以state-action-reward序列出现呈现（如下图）。很显然序列包含了M和R的输入和输出数据。</p>
<p><img src="/2024/10/12/research%20lifev3/1729240130483.png" alt="1729240130483"></p>
<p><img src="/2024/10/12/research%20lifev3/1729240141468.png" alt="1729240141468"></p>
<h4 id="表格法（状态、动作空间小且有限）-R-MAX"><a href="#表格法（状态、动作空间小且有限）-R-MAX" class="headerlink" title="表格法（状态、动作空间小且有限）-&gt; R-MAX"></a>表格法（状态、动作空间小且有限）-&gt; R-MAX</h4><h4 id="Prediction-Loss"><a href="#Prediction-Loss" class="headerlink" title="Prediction Loss"></a>Prediction Loss</h4><p>通过神经网络实现近似函数代替表格。transition model命名为$M_{\theta}$，其中$\theta$是网络权重，真实transition model命名为$M^{*}$。</p>
<h5 id="单步转移模型（fit-one-step-transition）"><a href="#单步转移模型（fit-one-step-transition）" class="headerlink" title="单步转移模型（fit one-step transition）"></a>单步转移模型（fit one-step transition）</h5><p>当$M_{\theta}$是确定性(deterministic)的时，模型学习目标就是$M_{\theta}$对下一个状态的均方预测误差。为了应对不确定性，$M_{\theta}$可以利用概率转移模型建模（通常被实例化为高斯分布），此时模型的学习目标可以是$M_{\theta}$和$M^{*}$之间的KL散度最小化（有点像监督学习任务）。</p>
<p>价值评估误差：使用预测模型损失获得模型后，模型能够提供多少帮助？特别是策略$\pi$在模型和真实环境中的表现差异。</p>
<p>模拟引理（Simulation Lemma）表明与模型误差相比，奖励误差并不严重。</p>
<p><img src="/2024/10/12/research%20lifev3/1729252209575.png" alt="1729252209575"></p>
<p><img src="/2024/10/12/research%20lifev3/1729248793657.png" alt="1729248793657"></p>
<p>模拟引理2（Simulation Lemma II）表明策略价值评估误差是有界的。</p>
<p><img src="/2024/10/12/research%20lifev3/1729249030491.png" alt="1729249030491"></p>
<h5 id="多步预测"><a href="#多步预测" class="headerlink" title="多步预测"></a>多步预测</h5><p>由于累积误差是由于使用单步转移模型递归生成state-action而产生的，因此缓解该问题的一种方法是同时预测多个步骤。多步模型 [Asadi 等人，2019] 以当前状态 $s_t$ 和长度为 $h$ 的一系列动作 $(a_t, a_{t+1}, . . . , a_{t+h}) $作为输入，并预测未来的 $h$ 状态。（确定性）多步模型表示如下，同样可以通过监督学习训练</p>
<p><img src="/2024/10/12/research%20lifev3/1729249665915.png" alt="1729249665915"></p>
<p>以上方法得到的transtion models均为前向模型。MBRL的反向模型研究并不多，主要研究是以未来状态作为输入来预测当前状态，通常用于生成反向数据。</p>
<h4 id="Reduced-Error"><a href="#Reduced-Error" class="headerlink" title="Reduced Error"></a>Reduced Error</h4><p>上述基于predicition loss的方法的主要问题是horizon-squared compounding error，这主要是由于学习无约束模型导致的。</p>
<h5 id="Lipschitz-Continuity-Constraint"><a href="#Lipschitz-Continuity-Constraint" class="headerlink" title="Lipschitz Continuity Constraint"></a>Lipschitz Continuity Constraint</h5><p>为了减少累计误差，一种方法是约束模型。Lipschitz连续性约束。使用Wasserstein距离来衡量两个transition distributions间的相似性</p>
<p><img src="/2024/10/12/research%20lifev3/1729251606385.png" alt="1729251606385"></p>
<p>考虑状态空间S上的状态分布$\rho$而不是单个状态s时，可以定义广义转移模型（generalized transition model）如下</p>
<p><img src="/2024/10/12/research%20lifev3/1729251788059.png" alt="1729251788059"></p>
<p>n步误差</p>
<p>将Lipschitz连续性引入概率转移模型，一个概率转移模型M是K-Lipschitz的当且仅当</p>
<p><img src="/2024/10/12/research%20lifev3/1729252014058.png" alt="1729252014058"></p>
<p>在Lipschitz连续约束模型下，可以对n步误差进行界定。</p>
<p><img src="/2024/10/12/research%20lifev3/1729252185814.png" alt="1729252185814"></p>
<p><img src="/2024/10/12/research%20lifev3/1729252302302.png" alt="1729252302302"></p>
<p>the compounding error可以得到控制。</p>
<h5 id="Distribution-Matching"><a href="#Distribution-Matching" class="headerlink" title="Distribution Matching"></a>Distribution Matching</h5><p>学习transition的长期影响，考虑去匹配真实轨迹和学习模型展开的轨迹间的分布。在之前的一个工作中（GAIL）通过对抗学习和模仿学习，采用distribution matching的思想，以对抗的方式模仿专家策略。（最小化$\rho_{\pi_{E}}$(专家策略)和$\rho_{\pi}$）之间的JS散度。</p>
<p>在模仿学习角度，转移模型$M_{\theta}$以当前state-action输入并预测下一个状态的分布，被视为一种policy（对抗学习）。discriminator用于区分专家state和预测的state。（？）</p>
<p><img src="/2024/10/12/research%20lifev3/1729255229504.png" alt="1729255229504"></p>
<p>Simulation Lemma III解决了compounding error的问题。</p>
<h5 id="Robust-Model-Learning"><a href="#Robust-Model-Learning" class="headerlink" title="Robust Model Learning"></a>Robust Model Learning</h5><p>虽然compounding error减小了，但是策略差异仍然可能很大。策略差异（policy divergence）是指数据收集策略和目标策略之间的差异。为了减小差异，一个方向是使用具有广泛分布的数据收集策略。</p>
<h5 id="Complex-Environments-Dynamics"><a href="#Complex-Environments-Dynamics" class="headerlink" title="Complex Environments Dynamics"></a>Complex Environments Dynamics</h5><p>environment dynamics实现的主流思路为通过神经网络构建均值和协方差矩阵的高斯分布。该种架构在MuJuCo机器人运动环境中表现不错。</p>
<p>部分可观测性。对于部分可观测的环境所导致的POMDP，可能会有观测不足以作为推到的充分统计量。POMDP的一个经典解决方案是belief state estimation，观测模型$p(o_t|s_t)$和潜在转移模型latent transition model$p(s_{t+1}|s_t,a_t)$通过最大后验学习，并且可以推断后验分布$p(s_t|o_1,…,o_t)$。latent state distribution$p(s_t|o_1,…,o_t)$可以通过循环神经网络获得。</p>
<p>表示学习（Representation Learning）。对于诸如图像的高纬状态空间，学习信息丰富的latent state or action将极大地有利于环境模型的构建。</p>
<p><img src="/2024/10/12/research%20lifev3/1729262995387.png" alt="1729262995387"></p>
<p>DreamerV2用离散潜在变量替换了PlaNet中提出的高斯潜在变量，提高了性能。</p>
<p>state-action pair分布在模型训练和rollout阶段不匹配的问题，Shen等人将域自适应（domain adaptation）纳入模型学习任务，鼓励模型学习真实数据和rollout数据间state-action pair的不变表示。</p>
<h2 id="day24-2024-10-19"><a href="#day24-2024-10-19" class="headerlink" title="day24 2024.10.19"></a>day24 2024.10.19</h2><h3 id="TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer"><a href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer" class="headerlink" title="TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer"></a>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer</h3><p>训练teacher model的时候会生成一个经过DR的轨迹数据集D，以用来训练student model。</p>
<p><img src="/2024/10/12/research%20lifev3/1729350289161.png" alt="1729350289161"></p>
<p>所有上述modules的实现通过神经网络实现，网络参数为$\phi$，state通过deterministic componenet $h_t$和遵循categorical distribution的stochastic component联合表征。在每一步，RSSM用$h_t$计算两个stochastic state $z_t$和$\hat{z}$的分布。其中随机后验状态$z_t$是对当前输入观测$x_t$的编码，先验状态$\hat{z}$是对后验状态的预测，无需访问当前输入观测。因此，通过学习预测$z_t$，模型可以学习预测环境的动态。给定后验状态，训练解码器和奖励预测器分别重建当前输入观测值$x_t$和奖励$r_t$。</p>
<p><img src="/2024/10/12/research%20lifev3/1729350986669.png" alt="1729350986669"></p>
<p>一旦模型经过训练，就可以通过使用先验$\hat{z}$代替后验$z$，在不获取任何输入观测值的情况下进行推广。这使得模型能够生成形式为${(h_t，\hat{z_t}，a_t，r_t)_{t&#x3D;0}^{t&#x3D;T}}$的无限合成（或想象的）轨迹，其中t是想象的时间范围。</p>
<p>在知识蒸馏部分student model训练去学习对于teacher model一个从D中采样长度为L的轨迹trajectory $\tau$d的先验分布$p(\tilde{z}_t^{teacher}|\tilde{h}_t^{teacher})$，后验分布$q(\tilde{z}_t^{teacher}|\tilde{h}_t^{teacher},s_t)$和deterministic representations $h_t^{teacher}$</p>
<p><img src="/2024/10/12/research%20lifev3/1729350451865.png" alt="1729350451865"></p>
<p><img src="/2024/10/12/research%20lifev3/1729350542280.png" alt="1729350542280"></p>
<h3 id="PlaNet-2019ICML-Learning-Latent-Dynamics-for-Planning-from-Pixels"><a href="#PlaNet-2019ICML-Learning-Latent-Dynamics-for-Planning-from-Pixels" class="headerlink" title="PlaNet 2019ICML Learning Latent Dynamics for Planning from Pixels."></a>PlaNet 2019ICML Learning Latent Dynamics for Planning from Pixels.</h3></article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/10/20/research%20lifev4/" title="从零开始的科研生活004"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: 从零开始的科研生活004</span></a><a class="button is-default" href="/2024/10/04/research%20lifev2/" title="从零开始的科研生活002"><span class="has-text-weight-semibold">Next: 从零开始的科研生活002</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/FUshi37"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> FUshi37 2024</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>