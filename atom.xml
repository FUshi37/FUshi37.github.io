<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-10-25T11:36:52.728Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>sim-to-real研究综述和毕设研究计划</title>
    <link href="http://example.com/2024/10/24/sim-to-real%20research/"/>
    <id>http://example.com/2024/10/24/sim-to-real%20research/</id>
    <published>2024-10-24T13:37:50.843Z</published>
    <updated>2024-10-25T11:36:52.728Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sim-to-Real背景"><a href="#Sim-to-Real背景" class="headerlink" title="Sim-to-Real背景"></a>Sim-to-Real背景</h2><h2 id="Sim-to-Real方法"><a href="#Sim-to-Real方法" class="headerlink" title="Sim-to-Real方法"></a>Sim-to-Real方法</h2><p>sim-to-real的研究方法可以分为。。。几个方面，</p><h3 id="DR（Domain-Randomization）"><a href="#DR（Domain-Randomization）" class="headerlink" title="DR（Domain Randomization）"></a>DR（Domain Randomization）</h3><h5 id="2018-ICRA-Sim-to-Real-Transfer-of-Robotic-Control-with-Dynamics-Randomization"><a href="#2018-ICRA-Sim-to-Real-Transfer-of-Robotic-Control-with-Dynamics-Randomization" class="headerlink" title="2018 ICRA Sim-to-Real Transfer of Robotic Control with Dynamics Randomization"></a>2018 ICRA Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</h5><h5 id="2023-ICRA-DeXtreme-Transfer-of-Agile-In-hand-Manipulation-from-Simulation-to-Reality"><a href="#2023-ICRA-DeXtreme-Transfer-of-Agile-In-hand-Manipulation-from-Simulation-to-Reality" class="headerlink" title="2023 ICRA DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality"></a>2023 ICRA DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality</h5><h3 id="SysID（System-Identification）"><a href="#SysID（System-Identification）" class="headerlink" title="SysID（System Identification）"></a>SysID（System Identification）</h3><h5 id="2017-RSS-Preparing-for-the-Unknown-Learning-a-Universal-Policy-with-Online-System-Identification"><a href="#2017-RSS-Preparing-for-the-Unknown-Learning-a-Universal-Policy-with-Online-System-Identification" class="headerlink" title="2017 RSS   Preparing for the Unknown: Learning a Universal Policy with Online System Identification"></a>2017 RSS   Preparing for the Unknown: Learning a Universal Policy with Online System Identification</h5><h5 id="2021-RSS-RMA-Rapid-Motor-Adaptation-for-Legged-Robots"><a href="#2021-RSS-RMA-Rapid-Motor-Adaptation-for-Legged-Robots" class="headerlink" title="2021 RSS   RMA: Rapid Motor Adaptation for Legged Robots"></a>2021 RSS   RMA: Rapid Motor Adaptation for Legged Robots</h5><h5 id="2022-ICRA-Real2sim2real-Self-supervised-Learning-of-Physical-Single-step-Dynamic-Actions-for-Planar-Robot-Casting"><a href="#2022-ICRA-Real2sim2real-Self-supervised-Learning-of-Physical-Single-step-Dynamic-Actions-for-Planar-Robot-Casting" class="headerlink" title="2022 ICRA Real2sim2real: Self-supervised Learning of Physical Single-step Dynamic Actions for Planar Robot Casting"></a>2022 ICRA Real2sim2real: Self-supervised Learning of Physical Single-step Dynamic Actions for Planar Robot Casting</h5><h3 id="DA（Domain-Adaptation）"><a href="#DA（Domain-Adaptation）" class="headerlink" title="DA（Domain Adaptation）"></a>DA（Domain Adaptation）</h3><h5 id="2023-CoRL-Learning-Invariant-Feature-Spaces-To-Transfer-Skills-With-Reinforcement-Learning"><a href="#2023-CoRL-Learning-Invariant-Feature-Spaces-To-Transfer-Skills-With-Reinforcement-Learning" class="headerlink" title="2023 CoRL Learning Invariant Feature Spaces To Transfer Skills With Reinforcement Learning"></a>2023 CoRL Learning Invariant Feature Spaces To Transfer Skills With Reinforcement Learning</h5><h3 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h3><h5 id="2019-ICML-Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer"><a href="#2019-ICML-Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer" class="headerlink" title="2019 ICML Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer"></a>2019 ICML Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer</h5><h5 id="2020-PMLR-Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real"><a href="#2020-PMLR-Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real" class="headerlink" title="2020 PMLR Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real"></a>2020 PMLR Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real</h5><h5 id="2023-CoRL-Robot-Parkour-Learning"><a href="#2023-CoRL-Robot-Parkour-Learning" class="headerlink" title="2023 CoRL Robot Parkour Learning"></a>2023 CoRL Robot Parkour Learning</h5><h5 id="2024-ICRA-TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer"><a href="#2024-ICRA-TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer" class="headerlink" title="2024 ICRA TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer"></a>2024 ICRA TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer</h5><h3 id="Simulation-Augmentation"><a href="#Simulation-Augmentation" class="headerlink" title="Simulation Augmentation"></a>Simulation Augmentation</h3><h5 id="2018-Sim-to-Real-Learning-Agile-Locomotion-For-Quadruped-Robots"><a href="#2018-Sim-to-Real-Learning-Agile-Locomotion-For-Quadruped-Robots" class="headerlink" title="2018 Sim-to-Real: Learning Agile Locomotion For Quadruped Robots"></a>2018 Sim-to-Real: Learning Agile Locomotion For Quadruped Robots</h5><h5 id="2019-ICRA-Closing-the-Sim-to-Real-Loop-Adapting-Simulation-Randomization-with-Real-World-Experience"><a href="#2019-ICRA-Closing-the-Sim-to-Real-Loop-Adapting-Simulation-Randomization-with-Real-World-Experience" class="headerlink" title="2019 ICRA Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience"></a>2019 ICRA Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</h5><h5 id="2020-IRC-Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation"><a href="#2020-IRC-Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation" class="headerlink" title="2020 IRC Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation"></a>2020 IRC Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation</h5><h5 id="2022-TRO-From-Simulation-to-Reality-A-Learning-Framework"><a href="#2022-TRO-From-Simulation-to-Reality-A-Learning-Framework" class="headerlink" title="2022 TRO From Simulation to Reality A Learning Framework"></a>2022 TRO From Simulation to Reality A Learning Framework</h5><p>对于足式机器人比较有启发性的方案应该为2020发表在Science Robotics上的Learning Quadrupedal Locomotion over Challenging Terrain及其先前2019年发表Science Robotics上的工作Learning Agile and Dynamic Motor Skills for Legged Robots，和2021年发表在RSS上的RMA: Rapid Motor Adaptation for Legged Robots。</p><h2 id="六足机器人相关研究"><a href="#六足机器人相关研究" class="headerlink" title="六足机器人相关研究"></a>六足机器人相关研究</h2><p>2021年有一篇关于六足机器人的文献综述Trends in the Control of Hexapod Robots: A Survey，文章就针对六足机器人的控制方法主要分为传统控制、生物启发式控制和强化学习三个方向，我的毕设方向是生物启发式+强化学习的方法，研究的重点可能更偏向于sim-to-real transfer。</p><h2 id="课题研究方向"><a href="#课题研究方向" class="headerlink" title="课题研究方向"></a>课题研究方向</h2><h3 id="Model-Based-Reinforcement-Learning"><a href="#Model-Based-Reinforcement-Learning" class="headerlink" title="Model-Based Reinforcement Learning"></a>Model-Based Reinforcement Learning</h3><p>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer这篇文章里就用到了MBRL来学习world model。文章中的Dreamer是一个</p><h3 id="爬墙"><a href="#爬墙" class="headerlink" title="爬墙"></a>爬墙</h3><h3 id="分层控制结构-多运动技能-知识蒸馏（？）"><a href="#分层控制结构-多运动技能-知识蒸馏（？）" class="headerlink" title="分层控制结构+多运动技能+知识蒸馏（？）"></a>分层控制结构+多运动技能+知识蒸馏（？）</h3><h2 id="课题研究的具体计划"><a href="#课题研究的具体计划" class="headerlink" title="课题研究的具体计划"></a>课题研究的具体计划</h2><h3 id="硬件设置"><a href="#硬件设置" class="headerlink" title="硬件设置"></a>硬件设置</h3><p>现在六足机器人上有的传感器：IMU和舵机</p><p>为了适应复杂地形行走，加一个激光雷达-&gt;高程图</p><h3 id="仿真设置"><a href="#仿真设置" class="headerlink" title="仿真设置"></a>仿真设置</h3><p>仿真选用PyBullet或Isaac Sim</p><h3 id="研究方法"><a href="#研究方法" class="headerlink" title="研究方法"></a>研究方法</h3><h4 id="生物启发式步态生成"><a href="#生物启发式步态生成" class="headerlink" title="生物启发式步态生成"></a>生物启发式步态生成</h4><p>用CPG做步态生成器（内模生成器），依旧采用双层CPG结构设计，耦合方法待定。振荡器初步定位Hpof振荡器（可以看六足机器人综述里的[49]、[55]、[56]以及提到的相关论文）</p><h4 id="强化学习算法"><a href="#强化学习算法" class="headerlink" title="强化学习算法"></a>强化学习算法</h4><p>PPO，ACTOR和CRITIC单独实现，CRITIC可以利用特权信息。</p><p>采用类似RMA的方案或者采用类似于ETH中的方案。</p><p>课程学习+随即地形生成器。</p><h4 id="sim-to-real"><a href="#sim-to-real" class="headerlink" title="sim-to-real"></a>sim-to-real</h4><p>关节执行器建模，data-driven。</p><p>延迟建模（？）</p><p>域随机化（机器人质量、关节阻尼、惯量、电机力矩、控制频率、延迟、摩擦力、IMU数值、施加在机器人身上的力）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Sim-to-Real背景&quot;&gt;&lt;a href=&quot;#Sim-to-Real背景&quot; class=&quot;headerlink&quot; title=&quot;Sim-to-Real背景&quot;&gt;&lt;/a&gt;Sim-to-Real背景&lt;/h2&gt;&lt;h2 id=&quot;Sim-to-Real方法&quot;&gt;&lt;a hre</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>从零开始的科研生活004</title>
    <link href="http://example.com/2024/10/20/research%20lifev4/"/>
    <id>http://example.com/2024/10/20/research%20lifev4/</id>
    <published>2024-10-20T08:15:06.253Z</published>
    <updated>2024-10-26T16:05:21.273Z</updated>
    
    <content type="html"><![CDATA[<h2 id="day25-2024-10-20"><a href="#day25-2024-10-20" class="headerlink" title="day25 2024.10.20"></a>day25 2024.10.20</h2><h3 id="World-Model-based-Perception-for-Visual-Legged-Locomotion"><a href="#World-Model-based-Perception-for-Visual-Legged-Locomotion" class="headerlink" title="World Model-based Perception for Visual Legged Locomotion"></a>World Model-based Perception for Visual Legged Locomotion</h3><p>基于Dreamer框架构建的针对四足机器人的MBRL方法。方法实现了基于视觉的。</p><p>世界模型的更新频率小于policy决策频率，每k步更新。世界模型通过DreamerV3中RSSM的方法构建，构建思路与TWIST那篇十分相似（因为都是Dreamer）。</p><p><img src="/2024/10/20/research%20lifev4/1729428040891.png" alt="1729428040891"></p><p>循环模型使用GRU网络实现，解码器和编码器使用CNN网络处理深度图像实现，MLP处理本体感受观测。</p><p><img src="/2024/10/20/research%20lifev4/1729428986904.png" alt="1729428986904"></p><p>策略网络根据世界模型学习，经过良好训练的世界模型中的循环状态$h_t$包含了足够的信息用于未来预测，类似于底层的马尔可夫状态$s_t$。基于这一见解，训练了一个将$h_t$作为输入的策略。训练时采用actor-critic，critic可以使用特权信息训练。</p><p>奖励函数设置</p><p><img src="/2024/10/20/research%20lifev4/1729431716051.png" alt="1729431716051"></p><p>PPO + Isaac Gym 有类似于课程学习的过程</p><h2 id="day26-2024-10-21"><a href="#day26-2024-10-21" class="headerlink" title="day26 2024.10.21"></a>day26 2024.10.21</h2><h3 id="Temporal-Difference-Learning-or-Model-Predictive-Control-2022"><a href="#Temporal-Difference-Learning-or-Model-Predictive-Control-2022" class="headerlink" title="Temporal Difference Learning or Model Predictive Control 2022"></a>Temporal Difference Learning or Model Predictive Control 2022</h3><p>MBRL虽然比MFRL采样效率高，但是规划long horizon时需要的时间开销大，且很难获取一个准确的环境模型。TD-MPC，集合了MF和MB的优势，讲MPC和Model free的TD error更新结合，使用最终值函数来估计长期奖励。</p><p>具体而言，算法用MBRL学习用于局部轨迹优化的模型，用MFRL学习预测长期回报（用于全局优化）的价值函数（value function）。</p><p>在作者的MPC的pipeline中，通过输入t时刻的observation$s_t$，经过编码器$h_{\theta}$输出$t$时刻的隐向量$z_t$，将$z_t$和从正态分布采用的action$a_t$输入给环境模型$d_{theta}$，输出下一时刻的隐向量$z_{t+1}$并得到相应奖励$\hat{r}<em>t$。接着又从高斯分布中采样下一时刻的action$a</em>{t+1}$，接着如下图所示，直至预测到H时刻的隐向量$z_{H}$结束。</p><p><img src="/2024/10/20/research%20lifev4/98c2b2a59e54d0fd0eab307c6b7f5182.png" alt="img"></p><p>环境模型$d_{\theta}$的不精确可能会导致累计误差+MPC求得是t到H时刻的局部最优解。</p><p>TD-learning for MPC</p><p>TD-learning解决MPC的上述两个问题，TD-learning通过学习一个state-action value function价值函数$Q_{\theta}(s,a)$帮助MPC找到全局最优解。因为TD-learning是MF方法，所以它只学习跟奖励有关的信息，从而减少了MPC学习模型时的误差。</p><p>作者直接将价值函数加到MPC预测的一条轨迹所获得的回报中来学习价值函数。</p><p><img src="/2024/10/20/research%20lifev4/cfa29fdccc731714be0be6952c663ac5.png" alt="img"></p><p>标红的 Value 是状态-动作价值函数$Q_{\theta}(s,a)$所获得的回报，标红的 Rewards 是MPC规划一条轨迹所获得的奖励。也就是在MPC得到的rewards基础上加上$Q_{\theta}(s,a)$所获得的价值。</p><p><img src="/2024/10/20/research%20lifev4/1729576979789.png" alt="1729576979789"></p><p>Can we instead augment model-based planning with the strengths of model-free learning?</p><p>MPC算法用的是MPPI</p><p>TD-MPC维护了一个额外的policy来guide planning，如算法伪代码中蓝色图所示。当策略很差时，就会自然被排除在top-k之外，当粗恶略比较耗时，就可以很自然的按比例影响估计的回报。为了使采样随机化，对$\pi_{\theta}$的action像DDPG中一样应用线性退火的高斯噪声。</p><p><img src="/2024/10/20/research%20lifev4/1729591586033.png" alt="1729591586033"></p><p>TOLD与terminal value function通过TD-learning一起学习，TOLD仅对环境中预测奖励相关的元素进行建模，而不是建立一个完整的世界模型。在推理过程中，TD-MPC用TOLD进行轨迹优化，使用模型rollouts来估计短期奖励，使用terminal value function来估计长期奖励。支持连续动作空间、任意模态输入和系数奖励。</p><p>TOLD（Task-Oriented Latent Dynamics Model），能够支持图像或state的输入，主要在隐空间中模拟能够影响奖励的环境因素。主要 由五部分组成：</p><p><img src="/2024/10/20/research%20lifev4/1729577718396.png" alt="1729577718396"></p><p><img src="/2024/10/20/research%20lifev4/1729592401382.png" alt="1729592401382"></p><p>TOLD模型完全使用deterministic MLP实现，不需要RNN门控或者是概率模型。</p><p>相较于直接估计$Q$，文章选择学习一个策略$\pi_{\theta}$，该策略通过最小化目标函数来最大化$Q_{\theta}$。</p><p><img src="/2024/10/20/research%20lifev4/1729594442767.png" alt="1729594442767"></p><p>预期直接预测未来的状态或图片像素，文章选择预测隐向量，这也是一个很直观的思路，因为直接预测observation是十分困难的。我们建议用潜在状态一致性损失（如公式 10 所示）来正则化 TOLD，该损失迫使时间 $t + 1$ 的未来潜在状态预测 $z_{t+1} &#x3D; d_θ(z_t, a_t)$ 与对应ground-truth观察 $h_{θ^−}(s_{t+1})$ 的潜在表示相似，从而完全避免了对观察的预测。</p><p><img src="/2024/10/20/research%20lifev4/1729594884840.png" alt="1729594884840"></p><h2 id="day27-2024-10-22"><a href="#day27-2024-10-22" class="headerlink" title="day27 2024.10.22"></a>day27 2024.10.22</h2><h4 id="MPC-MPPI"><a href="#MPC-MPPI" class="headerlink" title="MPC -&gt; MPPI"></a>MPC -&gt; MPPI</h4><p>MPPI（Model Predictive Path Integral）</p><ol><li><p><strong>原理</strong>：MPPI是一种基于路径积分的控制方法，它利用随机采样的轨迹来优化控制输入。通过采样生成多条轨迹，MPPI评估这些轨迹的成本，并通过加权平均来得到控制输入。</p></li><li><p><strong>特点</strong>：</p><p>不依赖于精确的系统模型，而是可以使用系统的黑箱模型。</p><p>可以处理不确定性，通过采样和重采样策略获得更稳定的控制性能。</p><p>通常在高维和复杂的环境中表现良好。</p></li></ol><p>总结来说，MPC更强调通过模型进行优化，而MPPI则是利用随机采样的方法来获取控制策略。两者各有优劣，适用于不同的控制场景。</p><h3 id="RSSM"><a href="#RSSM" class="headerlink" title="RSSM"></a>RSSM</h3><p>RSSM（recurrent state-space model）是在<strong>PlaNet</strong>以及<strong>Dreamer</strong>系列的<strong>model-based强化学习</strong>中采用的，用来估计未知环境状态的模型。他的思想是将循环神经网络(下图(a))与状态空间模型(下图(b))联系在一起重构的模型(下图(c))。在model-based强化学习领域，根据PlaNet文章中所描述以及结合Dreamerv2的代码<a href="https://link.zhihu.com/?target=https://github.com/sai-prasanna/dreamerv2_torch">Dreamerv2的代码</a>，可以知利用循环神经网络输入输出的确定性关系以及状态空间模型输出的不确定性，可以利用由以往观测来推断当前的隐状态（prior），以及用当前的观测来推断当前隐状态（post）的两种方法来估计隐状态，然后用无监督的方法让两种估计的分布尽量接近，也就是用类似与两个分布的KL散度的方法来作为loss。</p><p><img src="/2024/10/20/research%20lifev4/1729606421659.png" alt="1729606421659"></p><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>给定一个mini-batch $X\in \mathbb{R}^{n\times d} $，其中n表示batch的大小，d表示特征维度。</p><p><img src="/2024/10/20/research%20lifev4/v2-2dc1444ca4c96ee722da410e79a4de5e_r.jpg" alt="img"></p><p>第一步引入一组参数$W_{xh}\in\mathbb{R}^{d\times h} $用于和输入$X$做线性运算，其中h表示隐藏层节点数。这样的运算用NN的形式表示如下：</p><p><img src="/2024/10/20/research%20lifev4/v2-d8c403de5dd478c25169d4b2246ed7eb_r.jpg" alt="img"></p><p>第二步如果是MLP计算到这就结束了，但RNN的特点就在于，引入上一步计算得到的隐状态$H_{t-1}\in \mathbb{R}^{n\times h} $，同时引入第二组参数$W_{hh}\in \mathbb{R}^{h\times h} $用于和$H_{t-1}$做线性运算，同样把该运算用NN的形式表示如下：</p><p><img src="/2024/10/20/research%20lifev4/v2-5c94ce95b431448824bab0fd93dd81b8_r.jpg" alt="img"></p><p>然后引入第三组参数$b_h\in \mathbb{R}^{1\times h}$，表示隐藏层的偏置，用于线性相加。</p><p>由此，综合上面的三步运算，线性求和通过激活函数$\phi$，得到时间步 t 时隐藏层的输出表达式</p><p><img src="/2024/10/20/research%20lifev4/1729608701925.png" alt="1729608701925"></p><p>$X_tW_{xh}$和$H_{t-1}W_{hh}$运算后都是$\mathbb{R}^{n\times h}$形状的tensor，与$b_h\in \mathbb{R}^{1\times h}$的形状不一致，不过可以通过pytorch的广播机制扩充运算。因此合并后的计算得到如下的计算图表示。</p><p><img src="/2024/10/20/research%20lifev4/v2-d36939e1077d151dbaca761a140f5b8a_r.jpg" alt="img"></p><p>得到$H_t$后，再计算最终输出$O_t$。对于时间步t，先引入第四组参数$W_{ho}\in \mathbb{R}^{h\times o}$，用于和$H_{t}$做线性计算，其中$o$表示输出的特征数。然后引入$b_o\in \mathbb{R}^{1\times o}$做线性求和，同用触发pytorch的广播机制。</p><p><img src="/2024/10/20/research%20lifev4/1729609915237.png" alt="1729609915237"></p><p><img src="/2024/10/20/research%20lifev4/v2-2c77077d50fd269769fd5d3979533334_r.jpg" alt="img"></p><p><img src="/2024/10/20/research%20lifev4/v2-5a11e1037302a681a7d80257c1943ef4_r.jpg" alt="img"></p><p><img src="/2024/10/20/research%20lifev4/v2-e7680d2524c9dabea5240005d40af430_r.jpg" alt="img"></p><h2 id="day28-2024-10-23"><a href="#day28-2024-10-23" class="headerlink" title="day28 2024.10.23"></a>day28 2024.10.23</h2><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>首先明确，GRU是RNN的一种，提出的Motivation是为了解决传统RNN中在处理实际问题时遇到的长期记忆丢失和反向传播中的梯度消失或爆炸等问题。</p><p>现在考虑一个问题：假设我们的文本序列非常长，有几十个甚至几百个单词，而且文本序列中存在非常远的依赖关系。在训练过程中，循环神经网络需要通过反向传播来更新权重，以最小化预测错误。然而，由于梯度消失的影响，网络在传播梯度时可能会出现问题。</p><p>举个例子，假设我们的模型在预测某个句子中的第一个单词时出现了错误。为了纠正这个错误，梯度将向网络的较早时间步传播，以更新与这个错误相关的权重。但是，由于梯度消失的问题，这些梯度可能会在网络中逐渐减小，导致较早时间步的权重几乎不会被更新，从而无法有效地学习到长期依赖关系。</p><p>另一方面，如果我们在训练过程中遇到了梯度爆炸的问题，梯度可能会变得非常大，导致权重的剧烈更新。这可能导致网络参数值的急剧变化，甚至可能导致数值溢出和数值不稳定性，使训练过程无法收敛。</p><p>为了解决这些问题，GRU模型通过引入门控机制，可以更好地控制信息的流动，并有效地缓解梯度消失和梯度爆炸问题。这使得网络能够更好地捕捉到长期依赖关系，提高模型的性能和泛化能力。</p><p>GRU（Gated Recurrent Unit）是一种循环神经网络（RNN）架构，专门用于处理序列数据。它在标准RNN的基础上引入了门控机制，帮助模型更有效地捕捉长程依赖关系。GRU有两个主要的门：</p><ol><li><strong>更新门</strong>（Update Gate）：控制当前状态的更新程度，它决定了多少先前的信息需要保留。</li><li><strong>重置门</strong>（Reset Gate）：控制将多少先前的状态丢弃，它影响如何结合过去的信息。</li></ol><p>通过这些门，GRU能够在处理序列数据时更好地管理信息流，减少梯度消失的问题，因此在许多任务（如自然语言处理、时间序列预测等）中表现良好。相比于LSTM，GRU的结构更简单，参数更少，通常训练更快。</p><p>对于给定的时间步$t$，假设输入一个mini-batch $X_{t}\in \mathbb{R}^{n\times d}$（其中$n$表示样本数，$d$表示输入数），前一个时间步的隐状态是$H_{t-1}\in \mathbb{R}^{n\times h}$（其中$h$表示隐藏单元数），则重置门$R_t\in \mathbb{R}^{n\times h}$和更新门$Z_t\in \mathbb{R}^{n\times h}$的计算如下所示：</p><p><img src="/2024/10/20/research%20lifev4/1729611492916.png" alt="1729611492916"></p><p>两者计算步骤相同。$\sigma$为sigmoid激活函数，充当遗忘门控信号。</p><p><img src="/2024/10/20/research%20lifev4/v2-3f9ce05492126d04892f815d9a029fcc_720w.webp" alt="img"></p><p>候选隐状态，我们得到重置门$R_t$和更新门$Z_t$的计算结果，但对于要更新的隐状态是如何计算的的呢？在GRU中，通常先通过$R_t$计算得到一个候选隐状态$\tilde{H}_{t}$，然后用$Z_t$来决定更新的程度。</p><p><img src="/2024/10/20/research%20lifev4/v2-84140f764dff6f7502f917876f25b46c_r.jpg" alt="img"></p><p>候选隐状态的详细计算过程如下：</p><p>将充值信号$R_t$与$H_{t-1}$做按元素乘积（哈达玛积），即$R_t\odot H_{t-1}$，然后与$W_{hh}\in \mathbb{R}^{h\times h}$做矩阵乘法，得到一个大小为$\mathbb{R}^{n\times h}$的矩阵，n是batch的大小；然后用$W_{xr}\in \mathbb{R}^{d\times h}$和输入$X_t$做矩阵乘法得到一个大小为$\mathbb{R}^{n\times h}$的矩阵。同时用$b_h\in \mathbb{R}^{d\times h}$与前面两个得到的矩阵相加（pytorch广播机制）。最后得到在时间步t的候选隐状态$\tilde{H}_{t}\in \mathbb{R}^{n\times h}$，如下所示：</p><p><img src="/2024/10/20/research%20lifev4/1729673988038.png" alt="1729673988038"></p><p>tanh用来确保候选隐状态矩阵$\tilde{H}_t$中element-wise值映射到区间（1，-1）内。</p><p>隐状态更新，有了重置门$R_t$，更新门$Z_t$的计算结果，以及一个候选隐状态$\tilde{H}<em>{t}$，接下来进行更新操作。通过上面运算得到一个候选隐状态$\tilde{H}</em>{t}$，现在要结合更新门$Z_t$来最终确定时间步t的隐状态在多大程度上由$H_{t-1}$和新得到的$\tilde{H}_{t}$决定。因为将$Z_t$的每个元素都限定在了（0，1）区间，因此可以做一个凸组合（如DDPG中的软更新），对于当前时间步的隐状态$H_t$：</p><p><img src="/2024/10/20/research%20lifev4/1729675014956.png" alt="1729675014956">从该式可以看出，当 $Z_t$ 趋于0时，新的隐状态 $H_t$ 几乎来自于 $\tilde{H}_t$ 。不过需要思考一下这样设计的理由。假如所有时间步的更新门 $Z_t$ 都接近1，则无论是多长的序列数据，距离当前时刻最久远的隐状态都能很好的保留到后续的计算中。To this end，我们得到一个完成GRU单元计算流，如下图所示（简化起见，省略了待学习参数矩阵乘法的标注）。</p><p><img src="/2024/10/20/research%20lifev4/v2-9621d01ebdf6b9b806fd96357cd9669d_r.jpg" alt="img"></p><p>与RNN相同的是，得到了当前时间步的隐状态，我们要将其映射到下游任务需要的特征维度，因此引入矩阵 $W_{hq}∈\mathbb{R}^{h×q}$ 和向量 $b_q$ 用于最后的线性层映射，其中输出的特征维度为 q 。以上计算可由（4）表示</p><p><img src="/2024/10/20/research%20lifev4/1729675460408.png" alt="1729675460408"></p><p>重置门有助于提取序列中的短期依赖关系，更新门有助于获取序列数据的长期依赖关系。</p><h2 id="day29-2024-10-24"><a href="#day29-2024-10-24" class="headerlink" title="day29 2024.10.24"></a>day29 2024.10.24</h2><h3 id="Trends-in-the-Control-of-Hexapod-Robots-A-Survey-2021"><a href="#Trends-in-the-Control-of-Hexapod-Robots-A-Survey-2021" class="headerlink" title="Trends in the Control of Hexapod Robots: A Survey 2021"></a>Trends in the Control of Hexapod Robots: A Survey 2021</h3><h4 id="Traditional-Controllers"><a href="#Traditional-Controllers" class="headerlink" title="Traditional Controllers"></a>Traditional Controllers</h4><p>传统控制器将六足机器人视为连接六个机械臂的刚体，并单独分析每个腿的驱动和控制[7]。运动通常通过运动学和动力学模型完全描述，机器人被视为开链机构（open-chained mechanisms）。</p><p>常见步态三足步态（tripod），波动步态（ripple）和异时步态（metachronal）[9]。</p><p>由于控制架构设计中采用的方法众多，收集的信息被归类为基于运动学的控制、基于动力学的控制以及实时路径和步态规划。</p><h5 id="基于运动学的控制（Kinematic-Based-Control）"><a href="#基于运动学的控制（Kinematic-Based-Control）" class="headerlink" title="基于运动学的控制（Kinematic-Based Control）"></a>基于运动学的控制（Kinematic-Based Control）</h5><p>该方法依赖于根据肢体所需运动计算关节所需的角度位置或其执行器的扭矩。</p><p>例如，尽管欧拉-拉格朗日动力学模型被用来研究机器人的相互作用，但六足机器人的驱动是通过逆运动学和关节当前角度位置作为反馈来控制的 [5]。</p><p>考虑到不同的应用，Zu 等人 [11] 使用机器人的运动学模型计算了其质心 (CM) 的位置，以调整其转向半径以避开障碍物。然而，环境会影响机器人的稳定性，为了产生自适应运动，需要获得对周围环境的感知。</p><p>生成自适应步态最常观察到的方法之一是评估足部的接触力，以检测地面的粗糙度，例如在 [11,12] 中。Irawan 和 Nonami [13] 也检测了足部的接触力，如果它们超过某个阈值，腿部的驱动则由线性弹簧模型控制，以增加支撑阶段的冲击吸收。</p><p>同样地，参考文献[1+] 使用接触检测来调整六足机器人的运动。这项研究的新颖之处在于其搜索算法，该算法旨在当肢体在摆动阶段无法检测到接触力时确定新的着力点。</p><p>考虑到运动学约束，脚首先施加向下的轨迹，如果传感器未检测到接触力，则执行多个搜索轨迹，直到关节极限被达到。此外，本研究的另一个区别是实施了静态稳定裕度 (SSM) 来评估步态的稳定性。该方法评估机器人质心 (CM) 地面投影到支撑多边形 (SP) 边缘的距离。如果 CM 在 SP 内，则机器人处于静态稳定状态。SSM 广泛用于六足机器人的稳定性控制，主要用于评估步态阶段转换期间躯干是否直立。在 [1] 中，SSM的使用旨在确保机器人在穿过台阶和坡道时产生稳定的步态，并且加工工具保持与身体居中。</p><p>同样地，参考文献 [16] 使用 SSM 来调整六足机器人在最大坡度为 30 度的斜坡上行走时的姿态。除了旨在降低 Noros-Il 执行器能耗外，考虑到对操纵对象的预期运动，Ding 和 Yang [17] 也使用 SSM 来评估为承载负荷而生成的四足步态的稳定性。</p><p>赵等 [4]关注克服障碍的能力，并在每个步态周期计算SSM，以确定用于克服物体的轨迹是否稳定。此外，刘等 [6]利用六足机器人的关节冗余，测试了当其中一条腿出现故障时生成稳定步态的能力，并使用SSM获得机器人最稳定的配置。</p><p>考虑到不同的方法，参考文献 [18] 采用了纵向稳定裕度 (LSM)，它类似于 SSM，但它计算了考虑运动方向的最小距离。该方法被用来在六足机器人跨越有几个禁区的崎岖地形行走时产生稳定的运动，并且需要调整其足部以避开这些禁区。</p><p>除了在非结构化环境中导航和克服障碍的能力，一些研究关注了攀爬表面的能力。Henrey 等人 [19] 在六足机器人 Abigaille-III 的每只脚上都包含了干粘合机制，并评估了攀爬垂直表面所需的粘附力，以确定关节所需的扭矩。</p><p>文献[2o]研究了在狭窄空间（例如烟囱）中攀爬的能力，该能力利用对机器人刚度和足部与墙壁之间变形程度的估计来控制关节的驱动。进行的实验还使用惯性测量单元（IMU）来检测和控制身体在上升运动过程中的倾斜。</p><p>采用不同的策略，[21] 的作者在 T-RHex 的腿上实现了受蟑螂微型毛发启发的微型脊椎，这使得它能够攀爬不同的表面，例如砖墙或斜度为 135 度的胶合板。</p><p><img src="/2024/10/20/research%20lifev4/1729742291268.png" alt="1729742291268"></p><p><img src="/2024/10/20/research%20lifev4/1729742307633.png" alt="1729742307633"></p><h5 id="基于动力学的控制（Dynamic-Based-Control）"><a href="#基于动力学的控制（Dynamic-Based-Control）" class="headerlink" title="基于动力学的控制（Dynamic-Based Control）"></a>基于动力学的控制（Dynamic-Based Control）</h5><p>尽管一些先前描述的论文采用力矩测量来检测站立阶段和腿部轨迹的变化 [11-1+]，但对足部位置偏差或关节角位置的控制仅依赖于运动学变量（例如，位置、速度和加速度）。相反，机器人的动态公式允许通过分析六足机器人上施加的外力和力矩引起的运动偏差，来全面了解其与环境的交互作用。表2总结了所有包含此类控制的研究。</p><p><img src="/2024/10/20/research%20lifev4/1729743495593.png" alt="1729743495593"></p><p>尽管 Khudher、Powell 和 Abbod [22] 使用这种公式为六足机器人生成基于所需足部加速度的扭矩控制器，该机器人旨在帮助人道主义排雷，但文献 [23] 则侧重于动态稳定步态的生成。</p><p>本研究提出了六足机器人的<strong>完整动力学模型</strong>，并测试了动态步态稳定裕度 (DGSM)，该裕度通过产生的角动量和 SP 的边缘来评估步态的稳定性。在这种情况下，如果总角动量与使六足机器人翻越 SP 边缘所需的最小角动量之间的平衡为正，则六足机器人是稳定的。然而，这些模型最重要的应用之一是评估脚与地面之间的接触力，如[2+]所示，这对于六足机器人在地形和粗糙度不同的环境中导航时调整其行为至关重要。</p><p>Soyguder 和 Alli [25] 模拟了一个弹簧加载倒立摆 (SLIP) 模型，用于六足动物运动的动态稳定性控制。与动物的行为类似，该系统估计了执行器的扭矩值，以确保每个肢体的虚拟动力学模型在支撑阶段表现为线性弹簧，在考虑摆动阶段刚度增加的同时，吸收了足部着地造成的冲击。</p><p>为了调整 HITCR-II 在户外环境中的姿态，刘等人 [26] 设计了一种基于足部力补偿模型的虚拟悬挂动力学模型，用于控制机体在轻微崎岖地形上的高度、俯仰角和偏航角，并设计了一种基于站立时非相邻肢体形成的多边形的调整方法，用于跨越更崎岖地面的行走。</p><p>在摆动阶段，通过比较获得的足部位置和Z轴上平坦地形上的预期立足点，识别了土壤的不规则性。参考文献[9]设计了一种顺应性控制器，根据测量接触力和预期接触力之间的误差来调整肢体的位置。</p><p>由于六足机器人必须产生稳定的步态，才能用两条腿偏心承载物体并在非结构化地面上行走，因此实施了卡尔曼滤波器来预测零力矩点 (ZMP) 的位置，并检查其是否在支撑多边形 (SP) 内。与SSM和LSM相比，该方法的优势在于，与假设准静态情况不同，它考虑了机器人运动对保持躯干直立能力的影响，这与DGSM类似。文献[27]也使用卡尔曼滤波器来估计六足机器人在地面垂直方向行走时的姿态。该算法通过 IMU 提供的数据、接触力和每个关节的位置来估计和校正身体姿态，即使在发生足部滑移的情况下，也获得了比机器人各自的动力学模型更好的结果。</p><p>与[25]中提出的方法不同，Bjelonic等人[28]在Weaver上实现了阻抗控制器，以提高运动的稳定性和能量效率。该文章还讨论了通过视觉惯性里程计检测障碍物以及通过IMU和每个关节的扭矩来计算地面粗糙度，从而从周围环境中收集数据。</p><p>此外，在[29]中，对Weaver进行了复杂环境导航的研究。该研究使用卡尔曼滤波器估计密闭空间的地板和天花板，以便Weaver能够跨越它们。六足机器人成功地调整了其身体的高度，以越过或穿过障碍物。</p><p>通过本体感觉信息分析周围环境的交互作用在 [30] 中也有讨论，该文献采用欧拉-拉格朗日方法，通过接触力和执行器产生的扭矩来确定关节期望位置和实际位置之间的偏差。</p><p>使用机器人的动力学模型而不是运动学模型的另一个优势是能够评估执行器的能耗。Jin、Chen 和 Li [31] 通过欧拉-拉格朗日公式和关节的扭矩分布，最小化了六足机器人不同负载下的能耗。因此，该模型可以根据其必须携带的额外质量调整其步态参数。</p><h5 id="实时路径和步态规划（Real-Time-Path-and-Gait-Planning-Methods）"><a href="#实时路径和步态规划（Real-Time-Path-and-Gait-Planning-Methods）" class="headerlink" title="实时路径和步态规划（Real-Time Path and Gait Planning Methods）"></a>实时路径和步态规划（Real-Time Path and Gait Planning Methods）</h5><p>之前各小节中提出的研究主要讨论了适应或生成特定行走模式的能力。然而，在复杂的环境中，使用复杂而精确的机械模型来定义六足机器人的适当行为可能非常耗时。本小节介绍了一些讨论此问题的文章，但没有提及整体控制架构是基于运动学的还是基于动力学的。</p><p>一些收集的文章讨论了使用计算机视觉算法来计算安全轨迹。例如，参考文献 [32] 使用快速扩展随机树 (RRT) 算法来估计未知户外环境中的安全路径。同样，参考文献 [33] 研究了 RRT 方法在具有不同类型障碍物的模拟环境中的路径规划。</p><p>Deepa 等人 [34] 还提出了一种大规模直接单目 SLAM 方法来映射六足机器人的周围环境并规划其路径，预测威胁区域，但这种方法没有在运动规划算法中进行测试。</p><p>除了生成安全路线外，六足机器人还需要知道如何适应其行为。[35]中提出的方法旨在通过人工神经网络 (ANN) 和模糊逻辑来减少六足机器人控制器规划其步态所需的时间。模糊逻辑负责在未知环境中确定肢体的正确驱动方式，而人工神经网络则决定六足机器人穿越已知环境时最合适的运动模式。这种方法的优势在于，机器人不需要评估地形，因为它从经验中知道最合适的步态。</p><p>考虑到另一个问题，Tennakoon [36] 引入了一个支持向量机来检测六足机器人行走于脆性表面时，其接触力是否会导致地形坍塌。利用这些信息，机器人调整了其质心和足部的位置，以避免不安全的区域。</p><h4 id="Bio-Inspired-Controllers"><a href="#Bio-Inspired-Controllers" class="headerlink" title="Bio-Inspired Controllers"></a>Bio-Inspired Controllers</h4><p>受生物启发的控制架构旨在模仿通过实现人工神经网络 [37] 生成运动的过程。其实现旨在为六足机器人的行为提供对环境的最佳适应 [6,38]。对于哺乳动物，兴奋性和抑制性运动指令从中脑运动区通过脊髓下降，以激活或抑制每个肢体的中央模式发生器 (CPG)，从而引起屈肌和伸肌运动神经元的节律性兴奋 [39]。利用相同的生物学原理，这些系统具有一个更高层的控制中心，类似于动物的大脑，它向负责腿部驱动（即 CPG 网络）的 ANN 发送运动指令。该 ANN 通常包含每个腿部一个神经振荡器（即两个相互抑制的神经元）来生物模拟肌肉的伸展和收缩 [+0]。</p><p><img src="/2024/10/20/research%20lifev4/1729750457586.png" alt="1729750457586"></p><p><img src="/2024/10/20/research%20lifev4/1729750470204.png" alt="1729750470204"></p><p>为了生成对称步态，仿生架构设计中最常见的方法是使用一个包含六个耦合振荡器的CPG网络，以有节奏地激活肢体的摆动和支撑阶段，以及一个较低的控制层，将耦合振荡器的输出转换为关节的角位置，如[42]中所述。</p><p>通过调整神经振荡器的参数，可以获得所需的步态模式。然而，并非所有分析的出版物都使用相同类型的振荡器。[42,47] 的作者由于其输出信号的稳定性（不受外部干扰影响）而使用了非线性修正的 Van der Pool (VDP) 振荡器，而参考文献 [1,++] 则建议使用 Hopf 振荡器，因为它们稳定且简单，考虑到不需要仿生模拟昆虫肢体的驱动，因为机器人腿已经是这些生物系统的简化版本。</p><p>Grzelczyk, Stanczyk 和 Awrejcewicz [49] 研究了 VDP、Hopf、Rayleigh 和粘滑振荡器在 CPG 模型中的实现，并得出结论，由于其简单性和低能耗，后者是最合适的。另一方面，参考文献 [55,56] 使用脉冲神经元而不是振荡器来设计 CPG 架构，以提高计算效率，并且由于可以使用时间事件作为激活函数。</p><p>尽管采用不同的方法，所有这些出版物在生成三足、波浪和异时步态以及安全地在它们之间转换方面都提供了类似的结果。</p><p>使用仿生架构的主要兴趣是通过在系统发生扰动时调整振荡器或尖峰神经元的参数和输出信号来实现感觉反馈，从而产生自适应运动 [55,58]。最简单的方法是将IMU与具有三个Matsuoka振荡器的CPG模型结合起来，以控制六足机器人的姿态 [40]。在本研究中，使用安装在机器人质心 (CM) 上的 IMU 提供的数据来调整振荡器的输出，调整足部高度并确保身体保持水平。</p><p>然而，正如其他研究中观察到的，地形感知和分类对于自适应运动的生成至关重要。Liu 等人 [4] 使用来自 IMU、足部力传感器、超声波传感器和扫描激光测距仪的数据作为 CPG 网络的输入，来分析地形的崎岖程度。然而，这种方法很大程度上受六足机器人的姿态影响；</p><p>[52] 中提出了一种不同的策略，使用径向基函数人工神经网络对每个关节产生的扭矩进行地面在线分类。获得的值调整了 CPG 层的六个 VDP 振荡器的参数。</p><p>六足机器人分别在细砾石、粗砾石和光滑表面上进行了测试；Yu、Gao 和 Deng [3s] 利用反射神经元和敏感神经元来模仿动物的反射行为。这些神经元的输入是每个肢体的接触力。如果系统检测到由于与障碍物碰撞而导致的早期地面接触，则启动支撑阶段。相反，当反射神经元在摆动阶段后未检测到任何接触力时，肢体执行多个摆动轨迹以寻找新的立足点；</p><p>反射神经元的实现也在 [5i] 中进行了讨论，其中它们被用于基于 Hopf 的 CPG 网络来生成自适应的蟹类运动。在这项研究中，六足机器人每个足尖都配备了红外传感器，用于检测地面并执行与 [38] 中类似的行为；</p><p>尽管[46]中也讨论了反射机制的生成，但本研究在每条腿中使用了基于储层的循环神经网络 (RNN) 来实现这些行为。RNN 通过处理来自关节控制的感官反馈和来自力接触传感器的數據来预测肢体的状态，并调整由 CPG 模型发送的运动指令。该控制架构已在 AMOS-II 中实现，并在复杂环境中进行了测试，例如跨越间隙、在具有可变拓扑的地面上行走以及攀爬表面，在适应性方面取得了良好的效果。</p><p>利用陀螺仪评估六足机器人的姿态，王等人 [57] 研究了肢体运动适应性以调整身体姿态并执行平地和斜坡之间的过渡运动。该机器人能够爬上倾斜度高达 16 度的斜坡；</p><p>与其他研究不同，AmphiHex-II 利用其可变刚度的肢体来调整其在爬坡和楼梯、游泳和在非结构化地面上行走时的姿势 [51]。与评估对称步态产生的研究类似，对基于 Hopf 的 CPG 模型的分析包括调整其参数以生成三足模式，这突出了这种解决方案在复杂环境中导航的简单性。</p><p>这些控制架构还测试了调整生成步态以克服障碍的能力。尽管实验是在室内规则地面上进行的，但参考文献[48]讨论了在CPG控制器中集成在线路径规划。本研究使用了一个小脑步态运动检测神经网络来处理视觉传感器提供的数据，并确定机器人轨迹的适当转弯半径。然而，由于视觉传感器放置在六足机器人的前方，并且该神经网络仅处理在每个时间点获取的数据，因此系统没有关于先前避开的障碍物的信息，导致机器人当转弯半径未完美调整时与它们发生碰撞。</p><p>Zhong 等人 [53] 也提出了一种利用计算机视觉的避障方法，但这种情况下，CPG 的输出被调整以越过或跨越物体。虽然其目标是空间探索，但在 [54], 一个 SNN 被用来仿生 LAURON V 的肢体驱动，并研究了其轨迹适应以克服障碍和越过障碍。</p><h4 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h4><p>Lele 等人[7] 在 SNN 中实现了 RL 以生成稳定的三足步态。通过这种方法，六足机器人学会了协调其腿部，无需预先编程的步态序列。使用陀螺仪和摄像头作为 SNN 的输入数据，系统的奖励和网络权重的调整基于机器人的平衡。由于目标是向前行走，因此相机提供了对所生成运动的视觉确认。与其他学习方法相比，这种方法在训练期间花费的时间更少，其中 70% 的情况收敛到三足步态。然而，该领域最近的兴趣带来了允许六足机器人自学如何应对周围环境的可能性 [38]：</p><p>障碍物规避：参考文献 [60] 将模糊逻辑与 Q 学习相结合，生成用于障碍物规避的实时控制。模糊逻辑用于将放置在六足上的声纳提供的数据组织和分组为一组有限状态，从而简化了算法的学习过程。该方法收敛速度快，具有最优策略，能够改变六足的方向以避开不同的障碍物；</p><p>自适应运动：在[61]中，蒙特卡罗方法被用来通过放置在每个脚尖上的力传感器检测步态阶段之间的转变。这些数据与SSM一起被用来确定需要驱动哪条腿以确保机器人的稳定性。在这种情况下，算法在每个回合结束时评估其结果，并且不能保证智能体访问所有状态，这可以提供一个贪婪策略。自适应步态的生成也在[62]中讨论。该方法包含一个具有两层的 CPG 模型。其中一层负责肢体间的协调，以产生三足步态、波浪步态或交替步态，而另一层则必须通过正确驱动膝关节和踝关节来调整每个肢体的行为。因此，为了避免手动调整第二层振荡器，我们实施了深度确定性策略梯度算法。该算法使用机器人的位置和速度以及关节的扭矩、角度位置和速度作为观测值，以获得振荡器的正确参数（例如，振幅和相位）。该算法的奖励函数惩罚高能耗，但奖励高航向速度值。该方法在 1400 个回合后收敛到一个解，并且机器人能够成功地调整其运动以适应不同摩擦系数的不同表面。</p><p>损伤恢复：Verma 等人 [63] 提出了一种基于近端策略优化的方法，使用监督学习神经网络进行损伤自诊断来实现损伤恢复。该算法可以在六足机器人失去一到两条腿的情况下找到一种步态策略。相反，Chatzilygeroudis 和 Mouret [64] 认为基于模型的策略搜索算法在机器人控制方面更有效，并设计了一种无重置的试错算法来恢复内部损伤。在这项研究中，六足机器人可以在不到一分钟的时间内自主学习最佳行走策略，即使一个或两个肢体出现故障，尽管存在计算问题。两种方法的优点都是不需要智能体在训练期间每次情节结束后都返回到初始位置。在 [65] 中，还关注了损伤恢复问题，作者提出了一种基于地图的多策略算法。该方法存储并映射所有可能的策略，以选择提供最大预期奖励的策略。尽管讨论了自恢复能力，这项研究仅测试了在爬楼梯环境中的运动生成，其中模型的一些特征，例如某些脚趾的尺寸，被改变以诱发一些损伤。</p><p><img src="/2024/10/20/research%20lifev4/1729757138731.png" alt="1729757138731"></p><h4 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h4><p><img src="/2024/10/20/research%20lifev4/1729758228038.png" alt="1729758228038"></p><p><img src="/2024/10/20/research%20lifev4/1729758412237.png" alt="1729758412237"></p><p><img src="/2024/10/20/research%20lifev4/1729758523766.png" alt="1729758523766"></p><p><img src="/2024/10/20/research%20lifev4/1729758552230.png" alt="1729758552230"></p><p><img src="/2024/10/20/research%20lifev4/1729758571002.png" alt="1729758571002"></p><p><img src="/2024/10/20/research%20lifev4/1729758642106.png" alt="1729758642106"></p><p><img src="/2024/10/20/research%20lifev4/1729758656188.png" alt="1729758656188"></p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p><img src="/2024/10/20/research%20lifev4/1729758727063.png" alt="1729758727063"></p><p><img src="/2024/10/20/research%20lifev4/1729758767539.png" alt="1729758767539"></p><p><img src="/2024/10/20/research%20lifev4/1729758790796.png" alt="1729758790796"></p><p><img src="/2024/10/20/research%20lifev4/1729758812544.png" alt="1729758812544"></p><p><img src="/2024/10/20/research%20lifev4/1729758828760.png" alt="1729758828760"></p><h2 id="day30-2024-10-25"><a href="#day30-2024-10-25" class="headerlink" title="day30 2024.10.25"></a>day30 2024.10.25</h2><h3 id="Using-Parameterized-Black-Box-Priors-to-Scale-Up-Model-Based-Policy-Search-for-Robotics-2018ICRA"><a href="#Using-Parameterized-Black-Box-Priors-to-Scale-Up-Model-Based-Policy-Search-for-Robotics-2018ICRA" class="headerlink" title="Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics 2018ICRA"></a>Using Parameterized Black-Box Priors to Scale Up Model-Based Policy Search for Robotics 2018ICRA</h3><p>Black-DROPS，利用参数化的黑盒先验来扩展到高维系统以及对先验信息的较大误差具有鲁棒性。</p><p>Black-DROPS 为 PILCO 带来了两项主要优势：(1) 可以使用任何奖励函数或策略参数化（包括非可微策略，例如有限状态机），以及 (2) 它是一种高度并行的算法，可以利用多核计算机的优势。</p><p>使用先验知识加速策略搜索。通过在模型上使用先验 [10]，[11]，[12]，[13]，[33] 可以减少基于模型的策略搜索中的交互时间；即，从对动力学的初始猜测开始，然后学习残差模型。具有先验 [10] 的 PILCO 和 PI-REM [12] 紧密相关，因为它们都使用 PILCO 的策略搜索过程。带有先验的 PILCO 使用模拟数据来创建高斯过程先验，而 PI-REM 使用解析方程来构建先验模型。带有先验的 PILCO 的主要限制是，它隐式地要求任务在 PILCO 的先验模型中得到解决（为了获得原始论文 [10] 中显示的加速效果）。GP-ILQG [11] 也像 PI-REM 一样学习残差模型，然后使用 ILQG [34] 的修改版本来找到给定模型不确定性的策略。然而，GP-ILQG 需要先验模型可微分。</p><p>我们希望得到一个模型$\hat{F}$，它能够尽可能准确地近似我们系统未知的动力学$F$，前提是给定一个初始猜测$M$。我们依靠高斯过程 (GP) 来实现这一点，因为它们已成功应用于许多基于模型的强化学习方法[7]、[8]、[41]、[42]、[5]、[40]、[6]。高斯过程 (GP) 是多元高斯分布到无限维随机过程的扩展，其中任意有限维组合都将服从高斯分布 [43]。</p><p><strong>模型学习方法</strong></p><p>我们的模型学习方法，我们称之为 GP-MI（高斯过程模型识别），它结合了非参数模型学习和参数模型识别，与 [16] 中的方法有关，但两者之间存在一些关键差异。首先，[16] 中的模型学习过程依赖于机械手方程，并且不能轻松地用于不直接符合该方程的机器人（例如，我们实验中的六足机器人或具有复杂动力学的软体机器人），而 GP-MI 对先验模型没有施加任何结构，除了提供一些可调参数（连续或离散）。此外，[16] 中的方法与逆动力学模型相关联，在一般情况下不能与正向模型一起使用（对于长期正向预测是必要的）；相反，GP-MI 可以与逆或正向动力学模型一起使用，并且通常可以与任何黑盒可调先验模型一起使用。</p><h3 id="Deep-Reinforcement-Learning-for-Multi-contact-Motion-Planning-of-Hexapod-Robots"><a href="#Deep-Reinforcement-Learning-for-Multi-contact-Motion-Planning-of-Hexapod-Robots" class="headerlink" title="Deep Reinforcement Learning for Multi-contact Motion Planning of Hexapod Robots"></a>Deep Reinforcement Learning for Multi-contact Motion Planning of Hexapod Robots</h3><p>文章将RL与Multi-contact motion planning与Transition feasibility结合起来，实现六足机器人走梅花桩的任务。</p><p>可以看一下Multi-contact motion planning和Transition feasibility的相关概念和文献。</p><h3 id="sim-to-real-research"><a href="#sim-to-real-research" class="headerlink" title="sim-to-real research"></a>sim-to-real research</h3><h2 id="day31-2024-10-26"><a href="#day31-2024-10-26" class="headerlink" title="day31 2024.10.26"></a>day31 2024.10.26</h2><h3 id="Adaptive-Gait-Generation-for-Hexapod-Robots-Based-on-Reinforcement-Learning-and-Hierarchical-Framework"><a href="#Adaptive-Gait-Generation-for-Hexapod-Robots-Based-on-Reinforcement-Learning-and-Hierarchical-Framework" class="headerlink" title="Adaptive Gait Generation for Hexapod Robots Based on Reinforcement Learning and Hierarchical Framework"></a>Adaptive Gait Generation for Hexapod Robots Based on Reinforcement Learning and Hierarchical Framework</h3><h3 id="Diffusion-Models-for-Reinforcement-Learning-A-Survey"><a href="#Diffusion-Models-for-Reinforcement-Learning-A-Survey" class="headerlink" title="Diffusion Models for Reinforcement Learning: A Survey"></a>Diffusion Models for Reinforcement Learning: A Survey</h3><p>planner</p><p>强化学习中的规划是指利用动态模型进行想象决策，并选择最优行动以最大化累积奖励。这个过程通常会探索各种行动和状态序列，从而在更长的时间范围内改进决策。规划通常在使用学习到的动态模型的模型-基于模型强化学习 (MBRL) 框架中使用。然而，规划序列通常是自回归地模拟的，这可能会导致严重的累积误差，尤其是在离线设置中，由于数据支持有限。扩散模型提供了一种很有前景的替代方案，因为它们可以同时生成多步规划序列。</p><p>policy</p><p>根据强化学习算法在决策过程中是否依赖动态模型，可以将其分为基于模型的强化学习 (MBRL) 和无模型强化学习。根据这种分类标准，使用扩散模型作为规划器更类似于 MBRL，因为生成的轨迹包含了动力学信息。另一个观点是，扩散规划器可以被视为策略和动态模型的结合 [Janner 等人，2022]。相比之下，将扩散模型用作策略侧重于改进现有的无模型强化学习解决方案。第 2.1 节阐述了当前离线策略学习方法的主要缺点：过度保守和缺乏表达能力。许多工作使用扩散模型作为无模型强化学习中的策略类来解决这些问题。</p><p>data synthesizer</p><p>除了拟合多模态分布外，扩散模型的一个简单且常见的用途是创建合成数据，这在计算机视觉领域得到了广泛应用。因此，扩散模型是强化学习数据集的自然数据合成器，因为数据稀缺是一个实际问题。为了确保合成数据与环境动态的一致性，强化学习中之前的数据增强方法通常会对状态和动作添加微小的扰动 [Sinha et al., 2021]。</p><p>online RL</p><p>最近，一些研究表明扩散模型也可以提升在线强化学习训练。在线强化学习中的价值估计存在噪声且随着当前策略的变化而变化，这给多步扩散模型的训练带来了额外的挑战。DIPO [杨等人，2023a] 提出了一种动作重新标记策略，通过数据层面的策略改进，绕过潜在的不稳定价值引导训练。在线推出数据集中的动作通过梯度上升进行更新，扩散训练目标只是对重新标记数据集的监督学习。Chen 等人 [2023d] 进行实验以验证具有单步采样的一致性模型可以自然地用作在线 RL 策略，并在探索和利用之间取得平衡。与将扩散模型用作策略不同，Rigter 等人 [2023] 建立了一个扩散动力学模型，以生成与在线 RL 策略一致的合成轨迹。扩散模型在在线 RL 中的应用探索较少 表 1：RL 扩散模型论文摘要。</p><p>Trajectory generation</p><p>轨迹生成旨在生成满足一组约束的动态可行路径。我们专注于使用扩散模型生成人类姿态和机器人交互序列，这些序列与决策场景更为相关。许多工作 [Zhang et al., 2022; Jiang et al., 2023; Tevet et al., 2022; Zhang et al., 2023b; Chen et al., 2022; Dabral et al., 2022] 指出，条件扩散模型比使用 GAN 或 Transformer 的传统方法表现更好。他们采用基于去噪扩散的框架，通过各种条件上下文实现了多样化和细粒度的运动生成 [Chen et al., 2023b; Carvalho et al., 2023]。最近的研究 [Du et al., 2023b; Ko et al., 2023; Du et al., 2023a] 利用扩散模型来合成一组未来帧，描绘其在未来的计划动作，之后从生成的视频中提取控制动作。这种方法使得仅使用 RGB 视频训练策略，并将学习到的策略部署到具有不同动力学的各种机器人任务成为可能 [Black 等人，2023；Gao 等人，2023]。UniSim [Yang 等人，2023b] 使用扩散模型通过学习组合的各种数据集来构建一个真实世界交互的通用模拟器。它可以用来训练高级视觉语言规划器和低级 RL 策略，展示了强大的仿真能力。</p><h2 id="day32-2024-10-27"><a href="#day32-2024-10-27" class="headerlink" title="day32 2024.10.27"></a>day32 2024.10.27</h2><h3 id="Human-Motion-Diffusion-Model"><a href="#Human-Motion-Diffusion-Model" class="headerlink" title="Human Motion Diffusion Model"></a>Human Motion Diffusion Model</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;day25-2024-10-20&quot;&gt;&lt;a href=&quot;#day25-2024-10-20&quot; class=&quot;headerlink&quot; title=&quot;day25 2024.10.20&quot;&gt;&lt;/a&gt;day25 2024.10.20&lt;/h2&gt;&lt;h3 id=&quot;World-Mod</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>从零开始的科研生活003</title>
    <link href="http://example.com/2024/10/12/research%20lifev3/"/>
    <id>http://example.com/2024/10/12/research%20lifev3/</id>
    <published>2024-10-12T07:10:29.108Z</published>
    <updated>2024-10-20T08:19:30.375Z</updated>
    
    <content type="html"><![CDATA[<h2 id="day17"><a href="#day17" class="headerlink" title="day17"></a>day17</h2><p>回学校了，做了会ppt</p><h2 id="day18-2024-10-13"><a href="#day18-2024-10-13" class="headerlink" title="day18 2024.10.13"></a>day18 2024.10.13</h2><p>做ppt，RoboCup开会</p><h2 id="day19-2024-10-14"><a href="#day19-2024-10-14" class="headerlink" title="day19 2024.10.14"></a>day19 2024.10.14</h2><h3 id="Learning-quadrupedal-locomotion-over-challenging-terrainScience-Robotics-2020SR"><a href="#Learning-quadrupedal-locomotion-over-challenging-terrainScience-Robotics-2020SR" class="headerlink" title="Learning quadrupedal locomotion over challenging terrainScience Robotics 2020SR"></a>Learning quadrupedal locomotion over challenging terrainScience Robotics 2020SR</h3><p>对物理系统进行逼真建模，包括执行器。</p><p>域随机化</p><p>teacher-student，teacher根据特权信息编码成隐式向量进行训练</p><p>时间卷积网络TCN根据本体感觉状态的历史信息推断隐式向量</p><p>课程学习，地形难度随训练过程逐渐提升</p><p><img src="/2024/10/12/research%20lifev3/1728886299186.png" alt="1728886299186"></p><h3 id="Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR"><a href="#Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR" class="headerlink" title="Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR"></a>Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR</h3><p>关节分析模型建模+数据驱动学习执行器网络</p><p><img src="/2024/10/12/research%20lifev3/1728891298239.png" alt="1728891298239"></p><p>1.适用于机器人系统-&gt;建模困难、自由度高的方法   硬件系统特性   无人机sim-to-real</p><p>对象区别，构建仿真系统的方法</p><p>2.系统可以拿到，仿真系统；哪个可实现 哪个不可实现</p><p>3.核心关注算法点，方法；实现手段</p><p>4.仿真嵌入AIGC，生成。认知孪生系统，虚实交互，交互反馈提升仿真系统</p><p>5.六足区别于四足，被动式</p><p>细节、细节差别、细微差别</p><p>揣测工作的源头，溯源而上</p><p>虚实交互</p><p>单机智能直接感知</p><p>虚拟场景发现问题</p><h2 id="day20-2024-10-15"><a href="#day20-2024-10-15" class="headerlink" title="day20 2024.10.15"></a>day20 2024.10.15</h2><p>摸了</p><h2 id="day21-2024-10-16"><a href="#day21-2024-10-16" class="headerlink" title="day21 2024.10.16"></a>day21 2024.10.16</h2><h3 id="Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR-1"><a href="#Learning-Agile-and-Dynamic-Motor-Skills-for-Legged-Robots-2019SR-1" class="headerlink" title="Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR"></a>Learning Agile and Dynamic Motor Skills for Legged Robots 2019SR</h3><p><img src="/2024/10/12/research%20lifev3/1729064820128.png" alt="1729064820128"></p><p>求解器[41]</p><p>12个关节的执行器模型各自训练，输入关节角度差和关节角速度，端到端输出关节位置，然后通过一个PD控制器实现位置控制。</p><p>TRPO</p><p>关节执行器网络建模激活函数选的是softsign。</p><p>策略网络训练选的是tanh。</p><p>没有用力传感器作为observation的一部分，用joint state history代替。</p><p>课程学习、课程参数调节reward各项系数值。</p><p>训练过程中对observation的velocity做randomization。</p><p><img src="/2024/10/12/research%20lifev3/1729069222649.png" alt="1729069222649"></p><h2 id="day22-2024-10-17"><a href="#day22-2024-10-17" class="headerlink" title="day22 2024.10.17"></a>day22 2024.10.17</h2><p>Model-Based RL -&gt; Dreamer相关论文</p><h3 id="DayDreamer-World-Models-for-Physical-Robot-Learning"><a href="#DayDreamer-World-Models-for-Physical-Robot-Learning" class="headerlink" title="DayDreamer: World Models for Physical Robot Learning"></a>DayDreamer: World Models for Physical Robot Learning</h3><p>文章用Dreamer在物理机器人上在线学习，在实验中，仅用一小时四足机器人就能学会翻到转身和行走，而无需使用仿真。</p><p><img src="/2024/10/12/research%20lifev3/1729234733820.png" alt="1729234733820"></p><p>Dreamer是一个MBRL算法，学习世界模型（即dynamics model）。世界模型试通过RSSM（Recurrent State-space Model）实现的。</p><p>RSSM包含四个部分：<br><img src="/2024/10/12/research%20lifev3/1729235516657.png" alt="1729235516657"></p><p><img src="/2024/10/12/research%20lifev3/1729235125653.png" alt="1729235125653"></p><h2 id="day23-2024-10-18"><a href="#day23-2024-10-18" class="headerlink" title="day23 2024.10.18"></a>day23 2024.10.18</h2><h3 id="2022-A-Survey-on-Model-based-Reinforcement-Learning"><a href="#2022-A-Survey-on-Model-based-Reinforcement-Learning" class="headerlink" title="2022 A Survey on Model-based Reinforcement Learning"></a>2022 A Survey on Model-based Reinforcement Learning</h3><p>MBRL（Model-based Reinforcement Learning）能够显著提高采样效率，在有世界模型后，智能体的训练可能具有一定的想象力，即可以预测不同action产生的结果，因此在训练过程中可以选用恰当的action减小试错成本。</p><p>MBRL的主要任务是学习状态转移函数M（或者叫state transition dynamics）。理论上来说要实现好的训练效果，MFRL最好是直接与真实环境交互采样数据（经验数据）。</p><p>(a)为on-policy强化学习，(b)为off-policy强化学习，(c)为actor-critic，(d)为model-based方法，(e)为off-policy actor-critic方法。</p><p>与actor-critic相比，model-based方法重构了状态转移动力学，尽管critic计算设计状态转移动力学，但MBRL中的学习模型与策略解耦，因此该模型可用于评估其他策略，而critic是与actor绑定的。</p><p>off-policy、actor-critic和MBRL是并行的，可以组合使用（如图e）</p><p><img src="/2024/10/12/research%20lifev3/1729238916444.png" alt="1729238916444"></p><p>对于一个给定的MBRL问题，MDP中&lt;S,A,M,R,$\gamma$&gt;要学习的是状态转移动力学(state transition dynamics)M和奖励函数R。历史轨迹数据以state-action-reward序列出现呈现（如下图）。很显然序列包含了M和R的输入和输出数据。</p><p><img src="/2024/10/12/research%20lifev3/1729240130483.png" alt="1729240130483"></p><p><img src="/2024/10/12/research%20lifev3/1729240141468.png" alt="1729240141468"></p><h4 id="表格法（状态、动作空间小且有限）-R-MAX"><a href="#表格法（状态、动作空间小且有限）-R-MAX" class="headerlink" title="表格法（状态、动作空间小且有限）-&gt; R-MAX"></a>表格法（状态、动作空间小且有限）-&gt; R-MAX</h4><h4 id="Prediction-Loss"><a href="#Prediction-Loss" class="headerlink" title="Prediction Loss"></a>Prediction Loss</h4><p>通过神经网络实现近似函数代替表格。transition model命名为$M_{\theta}$，其中$\theta$是网络权重，真实transition model命名为$M^{*}$。</p><h5 id="单步转移模型（fit-one-step-transition）"><a href="#单步转移模型（fit-one-step-transition）" class="headerlink" title="单步转移模型（fit one-step transition）"></a>单步转移模型（fit one-step transition）</h5><p>当$M_{\theta}$是确定性(deterministic)的时，模型学习目标就是$M_{\theta}$对下一个状态的均方预测误差。为了应对不确定性，$M_{\theta}$可以利用概率转移模型建模（通常被实例化为高斯分布），此时模型的学习目标可以是$M_{\theta}$和$M^{*}$之间的KL散度最小化（有点像监督学习任务）。</p><p>价值评估误差：使用预测模型损失获得模型后，模型能够提供多少帮助？特别是策略$\pi$在模型和真实环境中的表现差异。</p><p>模拟引理（Simulation Lemma）表明与模型误差相比，奖励误差并不严重。</p><p><img src="/2024/10/12/research%20lifev3/1729252209575.png" alt="1729252209575"></p><p><img src="/2024/10/12/research%20lifev3/1729248793657.png" alt="1729248793657"></p><p>模拟引理2（Simulation Lemma II）表明策略价值评估误差是有界的。</p><p><img src="/2024/10/12/research%20lifev3/1729249030491.png" alt="1729249030491"></p><h5 id="多步预测"><a href="#多步预测" class="headerlink" title="多步预测"></a>多步预测</h5><p>由于累积误差是由于使用单步转移模型递归生成state-action而产生的，因此缓解该问题的一种方法是同时预测多个步骤。多步模型 [Asadi 等人，2019] 以当前状态 $s_t$ 和长度为 $h$ 的一系列动作 $(a_t, a_{t+1}, . . . , a_{t+h}) $作为输入，并预测未来的 $h$ 状态。（确定性）多步模型表示如下，同样可以通过监督学习训练</p><p><img src="/2024/10/12/research%20lifev3/1729249665915.png" alt="1729249665915"></p><p>以上方法得到的transtion models均为前向模型。MBRL的反向模型研究并不多，主要研究是以未来状态作为输入来预测当前状态，通常用于生成反向数据。</p><h4 id="Reduced-Error"><a href="#Reduced-Error" class="headerlink" title="Reduced Error"></a>Reduced Error</h4><p>上述基于predicition loss的方法的主要问题是horizon-squared compounding error，这主要是由于学习无约束模型导致的。</p><h5 id="Lipschitz-Continuity-Constraint"><a href="#Lipschitz-Continuity-Constraint" class="headerlink" title="Lipschitz Continuity Constraint"></a>Lipschitz Continuity Constraint</h5><p>为了减少累计误差，一种方法是约束模型。Lipschitz连续性约束。使用Wasserstein距离来衡量两个transition distributions间的相似性</p><p><img src="/2024/10/12/research%20lifev3/1729251606385.png" alt="1729251606385"></p><p>考虑状态空间S上的状态分布$\rho$而不是单个状态s时，可以定义广义转移模型（generalized transition model）如下</p><p><img src="/2024/10/12/research%20lifev3/1729251788059.png" alt="1729251788059"></p><p>n步误差</p><p>将Lipschitz连续性引入概率转移模型，一个概率转移模型M是K-Lipschitz的当且仅当</p><p><img src="/2024/10/12/research%20lifev3/1729252014058.png" alt="1729252014058"></p><p>在Lipschitz连续约束模型下，可以对n步误差进行界定。</p><p><img src="/2024/10/12/research%20lifev3/1729252185814.png" alt="1729252185814"></p><p><img src="/2024/10/12/research%20lifev3/1729252302302.png" alt="1729252302302"></p><p>the compounding error可以得到控制。</p><h5 id="Distribution-Matching"><a href="#Distribution-Matching" class="headerlink" title="Distribution Matching"></a>Distribution Matching</h5><p>学习transition的长期影响，考虑去匹配真实轨迹和学习模型展开的轨迹间的分布。在之前的一个工作中（GAIL）通过对抗学习和模仿学习，采用distribution matching的思想，以对抗的方式模仿专家策略。（最小化$\rho_{\pi_{E}}$(专家策略)和$\rho_{\pi}$）之间的JS散度。</p><p>在模仿学习角度，转移模型$M_{\theta}$以当前state-action输入并预测下一个状态的分布，被视为一种policy（对抗学习）。discriminator用于区分专家state和预测的state。（？）</p><p><img src="/2024/10/12/research%20lifev3/1729255229504.png" alt="1729255229504"></p><p>Simulation Lemma III解决了compounding error的问题。</p><h5 id="Robust-Model-Learning"><a href="#Robust-Model-Learning" class="headerlink" title="Robust Model Learning"></a>Robust Model Learning</h5><p>虽然compounding error减小了，但是策略差异仍然可能很大。策略差异（policy divergence）是指数据收集策略和目标策略之间的差异。为了减小差异，一个方向是使用具有广泛分布的数据收集策略。</p><h5 id="Complex-Environments-Dynamics"><a href="#Complex-Environments-Dynamics" class="headerlink" title="Complex Environments Dynamics"></a>Complex Environments Dynamics</h5><p>environment dynamics实现的主流思路为通过神经网络构建均值和协方差矩阵的高斯分布。该种架构在MuJuCo机器人运动环境中表现不错。</p><p>部分可观测性。对于部分可观测的环境所导致的POMDP，可能会有观测不足以作为推到的充分统计量。POMDP的一个经典解决方案是belief state estimation，观测模型$p(o_t|s_t)$和潜在转移模型latent transition model$p(s_{t+1}|s_t,a_t)$通过最大后验学习，并且可以推断后验分布$p(s_t|o_1,…,o_t)$。latent state distribution$p(s_t|o_1,…,o_t)$可以通过循环神经网络获得。</p><p>表示学习（Representation Learning）。对于诸如图像的高纬状态空间，学习信息丰富的latent state or action将极大地有利于环境模型的构建。</p><p><img src="/2024/10/12/research%20lifev3/1729262995387.png" alt="1729262995387"></p><p>DreamerV2用离散潜在变量替换了PlaNet中提出的高斯潜在变量，提高了性能。</p><p>state-action pair分布在模型训练和rollout阶段不匹配的问题，Shen等人将域自适应（domain adaptation）纳入模型学习任务，鼓励模型学习真实数据和rollout数据间state-action pair的不变表示。</p><h2 id="day24-2024-10-19"><a href="#day24-2024-10-19" class="headerlink" title="day24 2024.10.19"></a>day24 2024.10.19</h2><h3 id="TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer"><a href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer" class="headerlink" title="TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer"></a>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer</h3><p>训练teacher model的时候会生成一个经过DR的轨迹数据集D，以用来训练student model。</p><p><img src="/2024/10/12/research%20lifev3/1729350289161.png" alt="1729350289161"></p><p>所有上述modules的实现通过神经网络实现，网络参数为$\phi$，state通过deterministic componenet $h_t$和遵循categorical distribution的stochastic component联合表征。在每一步，RSSM用$h_t$计算两个stochastic state $z_t$和$\hat{z}$的分布。其中随机后验状态$z_t$是对当前输入观测$x_t$的编码，先验状态$\hat{z}$是对后验状态的预测，无需访问当前输入观测。因此，通过学习预测$z_t$，模型可以学习预测环境的动态。给定后验状态，训练解码器和奖励预测器分别重建当前输入观测值$x_t$和奖励$r_t$。</p><p><img src="/2024/10/12/research%20lifev3/1729350986669.png" alt="1729350986669"></p><p>一旦模型经过训练，就可以通过使用先验$\hat{z}$代替后验$z$，在不获取任何输入观测值的情况下进行推广。这使得模型能够生成形式为${(h_t，\hat{z_t}，a_t，r_t)_{t&#x3D;0}^{t&#x3D;T}}$的无限合成（或想象的）轨迹，其中t是想象的时间范围。</p><p>在知识蒸馏部分student model训练去学习对于teacher model一个从D中采样长度为L的轨迹trajectory $\tau$d的先验分布$p(\tilde{z}_t^{teacher}|\tilde{h}_t^{teacher})$，后验分布$q(\tilde{z}_t^{teacher}|\tilde{h}_t^{teacher},s_t)$和deterministic representations $h_t^{teacher}$</p><p><img src="/2024/10/12/research%20lifev3/1729350451865.png" alt="1729350451865"></p><p><img src="/2024/10/12/research%20lifev3/1729350542280.png" alt="1729350542280"></p><h3 id="PlaNet-2019ICML-Learning-Latent-Dynamics-for-Planning-from-Pixels"><a href="#PlaNet-2019ICML-Learning-Latent-Dynamics-for-Planning-from-Pixels" class="headerlink" title="PlaNet 2019ICML Learning Latent Dynamics for Planning from Pixels."></a>PlaNet 2019ICML Learning Latent Dynamics for Planning from Pixels.</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;day17&quot;&gt;&lt;a href=&quot;#day17&quot; class=&quot;headerlink&quot; title=&quot;day17&quot;&gt;&lt;/a&gt;day17&lt;/h2&gt;&lt;p&gt;回学校了，做了会ppt&lt;/p&gt;
&lt;h2 id=&quot;day18-2024-10-13&quot;&gt;&lt;a href=&quot;#day18-</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>从零开始的科研生活002</title>
    <link href="http://example.com/2024/10/04/research%20lifev2/"/>
    <id>http://example.com/2024/10/04/research%20lifev2/</id>
    <published>2024-10-04T02:11:19.138Z</published>
    <updated>2024-10-12T15:42:24.230Z</updated>
    
    <content type="html"><![CDATA[<h2 id="day8"><a href="#day8" class="headerlink" title="day8"></a>day8</h2><p>回家了</p><h2 id="day9"><a href="#day9" class="headerlink" title="day9"></a>day9</h2><p>看了一点ANYMAL的文章</p><h2 id="day10"><a href="#day10" class="headerlink" title="day10"></a>day10</h2><h3 id="ANYmal-Parkour-Learning-Agile-Navigation-for-Quadrupedal-Robots"><a href="#ANYmal-Parkour-Learning-Agile-Navigation-for-Quadrupedal-Robots" class="headerlink" title="ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots"></a>ANYmal Parkour: Learning Agile Navigation for Quadrupedal Robots</h3><p>locomotion policy是固定情景下的运动策略，在固定障碍物环境下训练。perception module和navigaion module在不同障碍物布置的情境下训练。</p><p>训练算法用的是PPO，不过对PPO做了一定的改进。hydird actor输出，low-level command用高斯分布，skill selection用分类分布。训练在一个hierarchical set-up中进行的。</p><p><img src="/2024/10/04/research%20lifev2/1728098611335.png" alt="1728098611335"></p><p>文章并没有具体提及sim-to-real的办法，不过确实有对障碍物(场景)的随机化。</p><p>这篇文章值得多看几遍。</p><h3 id="Sim-to-Real-Learnging-Agile-Locomotion-For-Quadruped-Robots"><a href="#Sim-to-Real-Learnging-Agile-Locomotion-For-Quadruped-Robots" class="headerlink" title="Sim-to-Real: Learnging Agile Locomotion For Quadruped Robots"></a>Sim-to-Real: Learnging Agile Locomotion For Quadruped Robots</h3><p>仿真用的是PyBullet，观测空间舍弃了观测值容易漂和变化剧烈的观测值，如关节速度项，这是因为越紧致的观测空间越容易减小gap。</p><p>减小sim-to-real gap的方法主要有两个，第一个是提高仿真的保真度。该方法首先建立了一个精确的URDF模型(对于难以获得精确值的机器人，通过SysID也可以实现这部分工作)。此外还建立了一个更精准的执行器(关节)模型(PD控制增益项不能太大，否在在reality中容易振荡)根据理想直流电机动力学模型为基础对执行器进行建模，同时修改了线性转矩-电流的关系。此外是延迟的解决。对延迟进行建模，对最相邻两个time step的observation进行线性插值(对时间插值)，同时测量实际电机执行的延迟并将其建模到仿真中。</p><p>第二个是学习鲁棒的控制器。有三个方向，第一个是domain randomization；第二个是训练时增加随机扰动，具体为在机器人基体上施加随即方向、随机大小的力；第三个是使用紧致的观测空间，因为高纬度的观测空间可能导致机器人对训练场景(仿真)过拟合。</p><p><img src="/2024/10/04/research%20lifev2/1728120665560.png" alt="1728120665560"></p><h2 id="day11"><a href="#day11" class="headerlink" title="day11"></a>day11</h2><h3 id="RMA-Rapid-Motor-Adaptation-for-Legged-Robots"><a href="#RMA-Rapid-Motor-Adaptation-for-Legged-Robots" class="headerlink" title="RMA: Rapid Motor Adaptation for Legged Robots"></a>RMA: Rapid Motor Adaptation for Legged Robots</h3><p>Adaptation Module通过状态历史state和action来估计隐式向量z，z是privileged knowledge编码后的结果，在reality中因为无法获取privileged knowledge，z通过机器人的历史状态和action来获得。Base Policy通过当前状态state、上一步action和隐式向量z来输出机器狗的action。</p><p>仿真中可以直接拿到priviledged knowledge，所以Adapation Module的训练可以采用监督学习。训练过程中on-policy，即同步和base policy训练。</p><p><img src="/2024/10/04/research%20lifev2/1728192253044.png" alt="1728192253044"></p><h3 id="Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation"><a href="#Sim2Real2Sim-Bridging-the-Gap-Between-Simulation-and-Real-World-in-Flexible-Object-Manipulation" class="headerlink" title="Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation"></a>Sim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation</h3><p>首先在一个相对rough的仿真中训练一个模型，然后在实物上部署并收集数据，最后根据收集到的数据去仿真中更新模型和方法。</p><p>对于较软的物体，仿真建模通常不够精准，Sim2Real2Sim方法就是为了解决这个问题而提出的。</p><p>仿真用的是Gazebo</p><p><img src="/2024/10/04/research%20lifev2/1728216401553.png" alt="1728216401553"></p><h3 id="Real2sim2real-Self-supervised-learning-of-physical-single-step-dynamic-actions-for-planar-robot-casting"><a href="#Real2sim2real-Self-supervised-learning-of-physical-single-step-dynamic-actions-for-planar-robot-casting" class="headerlink" title="Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting"></a>Real2sim2real: Self-supervised learning of physical single-step dynamic actions for planar robot casting</h3><p>Parameter estimation using Differential Evolution [53,55] with associated code and datasets from 64,350 simulated experiments and 2,076 physical experiments with 3 distinct cables.</p><p><img src="/2024/10/04/research%20lifev2/1728223180124.png" alt="1728223180124"></p><p>首先从reality中收集physical dataset，并选取子集来进行仿真的SysID。</p><p>然后文章从PyBullet、两个版本的NVIDIA Isaac Gym三个仿真做了对比。</p><h2 id="day12"><a href="#day12" class="headerlink" title="day12"></a>day12</h2><h3 id="Robot-Parkour-Learning"><a href="#Robot-Parkour-Learning" class="headerlink" title="Robot Parkour Learning"></a>Robot Parkour Learning</h3><p>文章提出了一个端到端的机器狗跑酷策略网络，每个具体技能的训练分为两阶段。第一阶段为soft dynamics预训练，障碍物可穿越，训练过程采用了课程学习，障碍物难度逐渐增大。同时该阶段的训练用到了privilege visual information。</p><p>第二阶段为fine-tune阶段，同样使用PPO，在预训练后在Hard Dynamics Constraints上再对每个运动技能进行训练。该阶段的训练能够实现sim2real的转换。五个skill的训练过程用到了privilege visual information</p><p>训练完五个技能的policy网络后，通过一个DAgger蒸馏一个vision-based的parkour策略网络。策略参数化为GRU，输入包括recurrent latent state、本体感知、上一步的aciton和经过CNN处理得到的深度图的latent embedding。</p><p>经过蒸馏后，策略具有了一定sim-to-real transfer的能力(terrain friction and mass properties)，不过还需要对visual appearence进行sim-to-real transfer。</p><p><img src="/2024/10/04/research%20lifev2/1728292261234.png" alt="1728292261234"></p><p>毕设申题需要翻译一篇文献，翻译了一下RMA那篇文章。</p><h2 id="day13"><a href="#day13" class="headerlink" title="day13"></a>day13</h2><h3 id="Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real"><a href="#Learning-Locomotion-Skills-for-Cassie-Iterative-Design-and-Sim-to-Real" class="headerlink" title="Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real"></a>Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real</h3><p>文章研究的是双足机器人，文章的主要思想就是蒸馏。策略蒸馏部分可以在看一下。</p><p><img src="/2024/10/04/research%20lifev2/1728378609266.png" alt="1728378609266"></p><p>系统辨识提升建模精度，文章发现膝关节的reflected inertia of motors十分重要，仿真器用的是Mujoco。文章发现没有使用dynamics rondamization策略网络也可以实现sim-to-real transfer。</p><p>文章中提到没有使用data-driven的方法来对执行机构进行建模。</p><p>文章中使用了状态估计的方法，状态估计用来模拟传感器。</p><h2 id="day14"><a href="#day14" class="headerlink" title="day14"></a>day14</h2><h3 id="Sim-to-Real-Transfer-in-Deep-Reinforcement-Learning-for-Robotics-a-Survey"><a href="#Sim-to-Real-Transfer-in-Deep-Reinforcement-Learning-for-Robotics-a-Survey" class="headerlink" title="Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey"></a>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey</h3><p>主要的sim-to-real方法一览</p><p><img src="/2024/10/04/research%20lifev2/1728453552003.png" alt="1728453552003"></p><ol><li>系统辨识：建立物理系统的精确模型，使得仿真更加精准</li><li>域随机化：与其精确建立现实世界的所有参数，不如对仿真进行高度随机化，以覆盖现实世界数据的真是分布</li><li>域自适应：利用源数据来提高学习模型在数据量较少的目标域上的性能。在源域和目标域不同的特征空间寻找统一的特征集。</li><li>带干扰的学习：奖励函数加入随机噪声。</li><li>仿真环境：Gazebo PyBullet MuJoCo IsaacGym</li><li>元强化学习</li><li>模仿学习</li><li>知识蒸馏</li></ol><p><img src="/2024/10/04/research%20lifev2/1728557708842.png" alt="1728557708842"></p><h2 id="day15"><a href="#day15" class="headerlink" title="day15"></a>day15</h2><h3 id="Sim-to-real-Six-legged-robot-control-with-deep-reinforcement-learning-and-curriculum-learning"><a href="#Sim-to-real-Six-legged-robot-control-with-deep-reinforcement-learning-and-curriculum-learning" class="headerlink" title="Sim-to-real: Six-legged robot control  with deep reinforcement learning and curriculum learning."></a>Sim-to-real: Six-legged robot control  with deep reinforcement learning and curriculum learning.</h3><p>文章的主体思路和谢广明的TRO差不多，粗仿真+精确仿真+课程学习</p><p>仿真用的是V-rep，sim-to-real主要是靠精准的逆运动学实现的，所以这并不是一个端到端的方法。</p><p><img src="/2024/10/04/research%20lifev2/1728742522363.png" alt="1728742522363"></p><h2 id="day16"><a href="#day16" class="headerlink" title="day16"></a>day16</h2><h3 id="Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer-2019ICML"><a href="#Continual-Reinforcement-Learning-deployed-in-Real-life-using-Policy-Distillation-and-Sim2Real-Transfer-2019ICML" class="headerlink" title="Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer 2019ICML"></a>Continual Reinforcement Learning deployed in Real-life using Policy Distillation and Sim2Real Transfer 2019ICML</h3><p>SRL CL RL Distillation</p><p><img src="/2024/10/04/research%20lifev2/1728636890456.png" alt="1728636890456"></p><p><img src="/2024/10/04/research%20lifev2/1728637438867.png" alt="1728637438867"></p><h3 id="TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer-2024ICRA"><a href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer-2024ICRA" class="headerlink" title="TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer 2024ICRA"></a>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer 2024ICRA</h3><p>该篇文章用知识蒸馏的方法去训练了类似于状态估计器的模型，用privileged states去训练teacher world model，再去蒸馏训练student world model。（world model即dynamics model）</p><p>该文章针对vision-based的方法，在训练teacher world model的时候对输入图片进行了domain randomization。student world model同理。</p><p>teacher world model的训练是用的Dreamer算法，一种model-based强化学习算法。</p><p><img src="/2024/10/04/research%20lifev2/1728623533715.png" alt="1728623533715"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;day8&quot;&gt;&lt;a href=&quot;#day8&quot; class=&quot;headerlink&quot; title=&quot;day8&quot;&gt;&lt;/a&gt;day8&lt;/h2&gt;&lt;p&gt;回家了&lt;/p&gt;
&lt;h2 id=&quot;day9&quot;&gt;&lt;a href=&quot;#day9&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>从零开始的科研生活001</title>
    <link href="http://example.com/2024/09/26/research%20life/"/>
    <id>http://example.com/2024/09/26/research%20life/</id>
    <published>2024-09-26T10:04:56.889Z</published>
    <updated>2024-10-12T07:11:21.350Z</updated>
    
    <content type="html"><![CDATA[<p>这是一个从零开始记录的研究生科研生活，不过是从研0开始的(悲)</p><p>昨天老板找我聊毕设的事，忽觉自己在复试完后已经放飞自我。老板给了两个方向，仿生(六足)和双臂机械臂操作。本人对六足更感兴趣但觉得没什么前途，做机械臂操作又感觉没什么意思，遂仍在犹豫中。</p><p>同时老板还给了做一个sim2real调研的任务，ddl定在了10月7号，这真不是让我帮他写毕设申题吗(bushi)。 </p><p>总之研0生活现在就算是开始了，昨晚找了许多篇文章准备开啃。</p><h2 id="day1"><a href="#day1" class="headerlink" title="day1"></a>day1</h2><h3 id="From-Simulation-to-Reality-A-Learning-Framework"><a href="#From-Simulation-to-Reality-A-Learning-Framework" class="headerlink" title="From Simulation to Reality A Learning Framework"></a>From Simulation to Reality A Learning Framework</h3><p>今天首先仔细过了一遍谢广明在2022年发在TRO上的《From Simulation to Reality A Learning Framework》，文章针对仿生鱼提出了Tri-S仿真+A2C强化学习训练的方法，减小了reality gap，实现了sim-to-real跨越。</p><p>文章提出了Tri-S System(Self-Switching-Simulator)是一个结合Surrogate env和CFD env的两阶段方法，Surrogate是data driven的环境仿真方法，在实验中通过给定输入，根据观测到的机器鱼的位置和姿态来做从输入到机器鱼状态s的映射fs。CFD env是基于数值计算的环境仿真方法，其计精度更高但同时计算效率也更低。</p><p><img src="/2024/09/26/research%20life/1727346561621.png" alt="1727346561621"></p><p>训练主体也可以看作两阶段训练，即Surrogate-&gt;CFD，Surrogate用于快速训练，为后续在CFD上的训练提供一个基本成型的策略网络结果。然后通过CFD高精度的仿真性能，再进行训练将显著减少reality gap，实现sim-to-real。</p><p><img src="/2024/09/26/research%20life/1727346792628.png" alt="1727346792628"></p><p>文章在此基础上实现了两个目标任务的训练，第一个是目标轨迹的跟随，第二个是到固定位姿。第一个的奖励函数设置是紧密的，训练过程并不难。但第二个任务由于任务的多目标性(要求距离和角度差同时为0)和机器鱼的非完整约束性质(nonholonomic constraints)，距离差和角度差不能同时保证减小，因此设置紧密的奖励函数是比较复杂的，为此文章采取了稀疏的奖励函数设置，同时为了保证训练过程能够收敛，采用了三阶段课程学习(curriculum learning)的方法。</p><p>该文章解决sim-to-real问题的主体思路是采用更精准的环境仿真器，同时采用一个data-driven的方法进行训练加速。</p><h3 id="TRANSIC-Sim-to-Real-Policy-Transfer-by-Learning-from-Online-Correction"><a href="#TRANSIC-Sim-to-Real-Policy-Transfer-by-Learning-from-Online-Correction" class="headerlink" title="TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction"></a>TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction</h3><p>另一篇文章是李飞飞在今年发在CoRL的《TRANSIC: Sim-to-Real Policy Transfer by Learning from Online Correction》，该文章提出了一个基于干预的学习方案来消除机械臂操作的sim-to-real问题。具体来说TRANSIC首先在仿真中采用teacher-student的方法，通过PPO训练teacher policy，然后通过GMMs(Gaussian Mixture Models)方法来训练student网络。</p><p>teacher可以利用特权信息，而student只能用点云输出。</p><p>随后，这些策略被部署到真实机器人上，由人类操作员监控执行情况。当机器人遇到错误或困难时，操作员会进行必要的干预，通过遥操作提供在线纠正。这些干预和纠正的数据被收集起来，用于训练一个残差策略，该策略能够学习预测并纠正机器人的状态差异。最终，基础策略和残差策略在测试时通过一个集成部署框架结合起来，通过门控机制决定何时应用残差策略，以实现更自然、更流畅的动作执行。TRANSIC方法的一个显著优势是它能够显著提高sim-to-real转移的性能，同时相比于其他仅依赖真实机器人轨迹的方法，它需要的真实世界数据更少。</p><p><img src="/2024/09/26/research%20life/1727351017885.png" alt="1727351017885"></p><p><img src="/2024/09/26/research%20life/1727353328109.png" alt="1727353328109"></p><p>李飞飞在这篇文章中对sim-to-real出现的原因，以及各种解决方案做了比较全面地介绍，对了解sim-to-real地相关问题大有裨益。</p><h4 id="sim-to-real已有的成熟解决方案的领域"><a href="#sim-to-real已有的成熟解决方案的领域" class="headerlink" title="sim-to-real已有的成熟解决方案的领域"></a>sim-to-real已有的成熟解决方案的领域</h4><p>a) locomotion</p><p>b) dexterous in-hand manipulation</p><p>c) simple non-prehensile manipulation</p><h4 id="sim-to-real-gaps"><a href="#sim-to-real-gaps" class="headerlink" title="sim-to-real gaps:"></a>sim-to-real gaps:</h4><p>a) perception gap：传感器观测差异</p><p>b) embodiment mismatch：机器人仿真模型与现实模型不匹配</p><p>c) controller inaccuracy：控制误差</p><p>d) poor physical realism：仿真精度问题，如对接触和碰撞不能做出精准的仿真计算</p><h4 id="traditional-methods"><a href="#traditional-methods" class="headerlink" title="traditional methods:"></a>traditional methods:</h4><p>a) system identification</p><p>b) domain randomization</p><p>c) real-world adaptation</p><p>d) simulator augmentation</p><h3 id="TODO-LIST"><a href="#TODO-LIST" class="headerlink" title="TODO LIST"></a>TODO LIST</h3><ol><li><p>TRANSIC中提到的state-of-the-art simulation</p></li><li><p>sim-to-real已有解决方案领域的文章，尤其是locomotion</p></li><li><p>sim-to-real gap相关文章</p></li><li><p>sim-to-real gap解决方案相关文章</p></li><li><p>残差网络、模仿学习、off line RL</p></li></ol><h2 id="day2"><a href="#day2" class="headerlink" title="day2"></a>day2</h2><h3 id="state-of-the-art-simulation"><a href="#state-of-the-art-simulation" class="headerlink" title="state-of-the-art simulation"></a>state-of-the-art simulation</h3><p>Mujoco：物理引擎，精度计算高于Pybullet，适合机械臂控制任务。DeepMind基于MuJoCo开发了强化学习环境<a href="https://link.zhihu.com/?target=https://github.com/deepmind/dm_control">Control Suite</a> 。OpenAI维护了一个mujoco-py版本专门给gym用。精度最高</p><p>Pybullet：基于Bullet Physics物理引擎，Bullet Physics使用离散元素方法(DEM)的算法来处理碰撞和接触问题。该方法将物体划分为小的离散单元，计算单元间的相互作用。</p><p>robosuite：基于Mujoco，专门用于manipulation。</p><p>Isaac Sim&#x2F;Lab：集成物理引擎PhysX，GPU加速训练。速度最快</p><p>Gazebo：与ROS集成，有多种物理引擎，仿真速度较慢。</p><p>Webots：速度慢、物理引擎精度低。</p><p>虚幻引擎：高保真度。</p><h3 id="domain-adaption"><a href="#domain-adaption" class="headerlink" title="domain adaption"></a>domain adaption</h3><p>domain adaption：source domain -&gt; target domain。核心观点是不同domain拥有某些共同特征，需要来自target domain的数据。</p><p>《Learning Invariant Feature Spaces To Transfer Skills With Reinforcement Learning》，该文章重点在不同morphology机械臂的policy网络的迁移。并非传统意义上的sim-to-real问题。</p><p><img src="/2024/09/26/research%20life/1727426172631.png" alt="1727426172631"></p><p>该文章的主体思路是寻找两个机械臂共通的隐式状态空间，然后用共通的隐式状态空间表征这两个机械臂。</p><p>举个例子来说就是我们知道robot1(2关节机械臂)在做task b时会先伸直机械臂，那么我们也鼓励robot2(3关节机械臂)也去伸直机械臂。更加广义的来说，相当于robot1给robot2提供了一个expert policy供robot2来进行imitation learning，只不过robot1和robot2结构不同，这里用一个mapping把他们对应起来</p><p>对于两个不同的domain a和b，我们有了domain a的知识，然后通过建立a和b的映射，把domain a的知识迁移到domain b上。 具体到这个robot的例子，我们想利用robot1的policy对robot2来做imitation learning，但是他们的状态无法对应，所以要做的就是找一个mapping来把他们的状态对应起来，然后在对应的状态上鼓励相似。</p><h3 id="domain-randomization"><a href="#domain-randomization" class="headerlink" title="domain randomization"></a>domain randomization</h3><p>source domain和target domain间的difference被建模为source domain中的可变量。domain randomization已在vision-based控制器中成功应用，即在低保真渲染仿真环境中的训练，通过光照、纹理即相机位置等来弥补视觉外观差异。</p><p>xuebin peng2018年发表在ICRA上的文章《Sim-to-Real Transfer of Robotic Control with Dynamics Randomization》。文章中提到的概念是dynamics randomization。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DeepRL policies are prone to exploiting idiosyncrasies of the simulator to realize behaviours that are infeasible in the real world.</span><br></pre></td></tr></table></figure><p>文章domain randomization的参数包括机械臂连杆质量，关节阻尼系，物体质量、摩擦系数和阻尼系数，桌子的高度，位置控制器的增益，动作的时间步长，观测噪声共95维。</p><p>推动任务依赖于actuators的物理属性，如质量、摩擦力等，而这些属性在真实世界中是难以获得的，因此文章提出了通过历史信息推断这些物理属性的方法。</p><p>HER(Hindsight Experience Replay)用于解决稀疏奖励的问题。依赖于off-policy方法。</p><p>采用RDPG( a method to train recurrent policies with off-policy data)，dynamics randomization参数输入到值函数中。</p><p>网络由feedword和recurrent两个branch组成，internal memory通过LSTM实现。</p><p><img src="/2024/09/26/research%20life/1727427687767.png" alt="1727427687767"></p><h3 id="System-Identification-SysID"><a href="#System-Identification-SysID" class="headerlink" title="System Identification(SysID)"></a>System Identification(SysID)</h3><p>刘正芸在2017年发表在RSS上的《Preparing for the Unknown: Learning a Universal Policy with Online System Identification》</p><p><img src="/2024/09/26/research%20life/1727439635214.png" alt="1727439635214"></p><p>Once trained, the combined algorithm, UP-OSI, can be executed in an unknown dynamic environment. At each time instance, the dynamic model parameters is first predicted by the learned system identification model. The universal control policy then takes the predicted model parameters along with the current state to compute the the optimal action (Figure 1).</p><p>UP同时接受机械臂的状态和dynamic model parameters，OSI的训练采用监督学习。UP训练过程选用TRPO方法。</p><p>学习过程分两个stage: universial policy learning和online system identification. 在第一个stage，我们对系统参数mu进行采样，训练出一个统一的控制策略pi，使其可以handle这个范围内所有的mu. 这个universial control policy以当前agent的state以及系统参数mu作为输入，输出action. 经过这个stage的训练，控制策略可以处理一定范围内的系统参数：比如如果这里的系统参数指地面摩擦系数，那么学出的行走控制器可以处理不同摩擦系数的地面.</p><p>在第二个stage，我们要做的是online地去识别出系统参数. 作者的想法也很符合直觉，我们给定agent过去一段时间的state action 序列，然后用一个简单的神经网络去拟合对应的系统参数. 当然这里的这些训练数据要尽可能的diverse,防止overfitting的发生.</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>现在比较好的仿真无非三个，Isaac Sim(速度最快)，MuJoCo(精度最高)，Pybullet(上手快，性能也不错)</p><p>domain randomization就是对物理变量进行随机化，system identification则是对物理系统的dynamics进行预测。domain adaption则是通过共同的隐式空间进行策略迁移。</p><p>xuebin peng的ICRA有点集成了domain randomization和system identification的意思。</p><h3 id="TODO-LIST-1"><a href="#TODO-LIST-1" class="headerlink" title="TODO LIST"></a>TODO LIST</h3><ol><li><p>real-word adaptation</p></li><li><p>simulator augmentation</p></li><li><p>off lien RL与imitation learning</p></li><li><p>locomotion sim2real &amp; ETH SR</p></li></ol><h2 id="day3"><a href="#day3" class="headerlink" title="day3"></a>day3</h2><h3 id="real-world-adaptation"><a href="#real-world-adaptation" class="headerlink" title="real-world adaptation"></a>real-world adaptation</h3><p>2020年发表在IROS上的《Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks》，采用了PEARL(Probabilistic Embeddings for Actor-Critic RL)方法来进行元强化学习训练。</p><p><img src="/2024/09/26/research%20life/1727500886489.png" alt="1727500886489"></p><p>仿真器采用MuJoCo。所谓的Simulated Environment Design就是随机化机械臂插入任务的相关参数，比如每次reset机械臂的末端位置、方孔尺寸和物块尺寸、机械臂位置控制器步长等参数。</p><p>sim-to-real transfer via meta reinforcement learning，PEARL对于off-policy训练采样效率更高，PEARL学习一些列任务的隐式表达。这这些特性得PEARL具有很快的适应性，在真实环境采样数据代价比较高的请跨下十分有效。然后，在仿真中训练的元RL策略能够在少量试验中适应从训练分布中采样的任务。对于PEARL来说，真实环境只是另一个需要去适应的task。</p><p>文章只用元强化学习做了insertion部分的工作，抓取和控制部分是自己设计的固定算法。</p><h3 id="simulator-augmentation"><a href="#simulator-augmentation" class="headerlink" title="simulator augmentation"></a>simulator augmentation</h3><p>2019年发表在ICRA上的《Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience》，文章中提到了domain randomization + system identification的方法的相关文章。</p><p>simulation randomization方法采用的主要思想是domain randomization，即通过一个参数来对仿真进行参数分布化，然后在该参数的策略下最小化仿真迹和真实迹的差异。问题在于真实迹的获取需要大量的实物试验，而这是不现实的。SimOpt的pipeline如下，在一个仿真分布上训练得到策略网络后，针对该分布参数和策略在仿真和实物上进行采样，计算得到仿真迹的真实迹的差异后对分布参数进行更新。</p><p>policy训练选用PPO算法，分布采用的是高斯分布形式</p><p><img src="/2024/09/26/research%20life/1727509923665.png" alt="1727509923665"></p><p>另一篇是来自于google2020年发表的《Augmenting differentiable simulators with neural networks to close the sim2real gap》，该文章通过一个残差模型来增强仿真。</p><p><img src="/2024/09/26/research%20life/1727513232044.png" alt="1727513232044"></p><h3 id="DeXtreme-Transfer-of-Agile-In-hand-Manipulation-from-Simulation-to-Reality"><a href="#DeXtreme-Transfer-of-Agile-In-hand-Manipulation-from-Simulation-to-Reality" class="headerlink" title="DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality"></a>DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality</h3><p>2023年发表在ICRA上。vision-based policy，仿真采用Isaac Gym。离散时间、部分观测马可夫决策过程，算法采用PPO。ACTOR和CRITIC的观测空间并不同：</p><p><img src="/2024/09/26/research%20life/1727517004236.png" alt="1727517004236"></p><p>domain randomization策略如下图。文章中采用了ADR(Automatic Domain Randomization)，每个参数的randomization范围被建模为均匀分布，所有randomization由ADR完成。randomization的参数包括physics randomization、non-physics randomization。</p><p><img src="/2024/09/26/research%20life/1727517252458.png" alt="1727517252458"></p><p>除此之外，在pose estimation部分也使用了randomization，该部分的randomization设置如下。同时还用到了augmentation</p><p><img src="/2024/09/26/research%20life/1727518399793.png" alt="1727518399793"></p><p><img src="/2024/09/26/research%20life/1727518713303.png" alt="1727518713303"></p><h3 id="Reinforcement-Learning-for-Pivoting-Task"><a href="#Reinforcement-Learning-for-Pivoting-Task" class="headerlink" title="Reinforcement Learning for Pivoting Task"></a>Reinforcement Learning for Pivoting Task</h3><p>domain randomization</p><h3 id="TODO-LIST-2"><a href="#TODO-LIST-2" class="headerlink" title="TODO LIST"></a>TODO LIST</h3><p>locomotion sim-to-real</p><h2 id="day4"><a href="#day4" class="headerlink" title="day4"></a>day4</h2><p>摸了</p><h2 id="day5"><a href="#day5" class="headerlink" title="day5"></a>day5</h2><p>摸了</p><h2 id="day6"><a href="#day6" class="headerlink" title="day6"></a>day6</h2><p>今天和老板又讨论了一下，决定毕设去做六足了。方向大概是一个从粗到精的sim-to-real。先通过强化学习实现在仿真中的步态实现，然后实现sim-to-real，然后部署在实物上。</p><p>毕业设计页数大于等于65页，要有一定的深度。至少20篇相关论文。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">NOTICE</span><br><span class="line"></span><br><span class="line">步态实现 locomotion运动学模型正常交互 dynamics介入</span><br><span class="line">无模型data-driven</span><br><span class="line">CPG参数最小化仿真中学习sim-to-real</span><br><span class="line">从粗到精</span><br><span class="line">强化学习步态仿真-&gt;sim-to-real-&gt;实物</span><br><span class="line">谢广明transformer</span><br><span class="line">强化学习behavior  环境动态交互  强化学习在机器人上</span><br><span class="line">基于强化学习的六足</span><br><span class="line">zwx wyr</span><br><span class="line">强化学习算法相关研究综述：算法、机器人背景创新性&gt;=65页</span><br><span class="line">准备ppt</span><br><span class="line">做一篇期刊英语pratice口语听力</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="day7"><a href="#day7" class="headerlink" title="day7"></a>day7</h2><p>摸了</p><h2 id="day8"><a href="#day8" class="headerlink" title="day8"></a>day8</h2><p>回家了</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这是一个从零开始记录的研究生科研生活，不过是从研0开始的(悲)&lt;/p&gt;
&lt;p&gt;昨天老板找我聊毕设的事，忽觉自己在复试完后已经放飞自我。老板给了两个方向，仿生(六足)和双臂机械臂操作。本人对六足更感兴趣但觉得没什么前途，做机械臂操作又感觉没什么意思，遂仍在犹豫中。&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Welcome ！！！</title>
    <link href="http://example.com/2024/09/26/hello-world/"/>
    <id>http://example.com/2024/09/26/hello-world/</id>
    <published>2024-09-26T09:56:41.957Z</published>
    <updated>2024-09-27T04:21:03.542Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome!!! 我是杨哲，就读于浙江大学机器人工程（竺可桢学院）专业，目前大四，已保研到浙江大学控制科学与工程学院控制科学与工程专业。我的研究兴趣为仿生智能（多足机器人），目前在研究机械臂操作方向。</p><p>在本科期间我参加了ZJUNlict（浙江大学RoboCup SSL战队），在2024年取得了RoboCup中国赛小型组冠军、RoboCup世界赛小型组亚军。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome!!! 我是杨哲，就读于浙江大学机器人工程（竺可桢学院）专业，目前大四，已保研到浙江大学控制科学与工程学院控制科学与工程专业。我的研究兴趣为仿生智能（多足机器人），目前在研究机械臂操作方向。&lt;/p&gt;
&lt;p&gt;在本科期间我参加了ZJUNlict（浙江大学RoboC</summary>
      
    
    
    
    
  </entry>
  
</feed>
